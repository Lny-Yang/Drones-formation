"""
ç»§ç»­è®­ç»ƒè„šæœ¬ - ç¬¬ä¸€é˜¶æ®µé¢†èˆªè€…å•æœºå¯¼èˆªè®­ç»ƒ

è®­ç»ƒç›®æ ‡:
- ç»§ç»­è®­ç»ƒé¢†èˆªè€…æ— äººæœºè¿›è¡Œå•æœºå¯¼èˆª
- åŸºäºä¹‹å‰è®­ç»ƒçš„æ¨¡å‹ç»§ç»­å­¦ä¹ 
- å‡‘é½100000å›åˆè®­ç»ƒ
- ä½¿ç”¨PPOç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
"""
import sys
import os
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import datetime
from collections import deque
import time
import pybullet as p

# å¯¼å…¥stable-baselines3
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
import torch
import random
from drone_envs.envs.drone_env_multi import DroneNavigationMulti

class PathConfig:
    """è·¯å¾„é…ç½®ç±» - é›†ä¸­ç®¡ç†æ‰€æœ‰ä¿å­˜è·¯å¾„ (SB3ä¸“ç”¨)"""
    
    # åŸºç¡€ç›®å½• - æ”¾åœ¨agentç›®å½•ä¸‹
    BASE_DIR = Path(__file__).parent
    AGENT_DIR = BASE_DIR / "agent"
    LOG_SB3_DIR = AGENT_DIR / "log_SB3"
    MODEL_SB3_DIR = AGENT_DIR / "model_SB3"
    
    # ç®€åŒ–å¼•ç”¨
    LOG_DIR = LOG_SB3_DIR
    MODEL_DIR = MODEL_SB3_DIR
    
    # è®­ç»ƒè¿›åº¦ç›¸å…³è·¯å¾„
    TRAINING_PROGRESS_PLOT = LOG_DIR / "training_progress.png"
    TRAINING_DATA_JSON = LOG_DIR / "training_data.json"
    TRAJECTORIES_JSON = LOG_DIR / "trajectories.json"
    
    # æœ€ç»ˆç»“æœè·¯å¾„
    FINAL_MODEL = MODEL_DIR / "leader_phase1_final"
    FINAL_PROGRESS_PLOT = LOG_DIR / "leader_phase1_final_progress.png"
    FINAL_DATA_JSON = LOG_DIR / "leader_phase1_final_data.json"
    FINAL_TRAJECTORIES_JSON = LOG_DIR / "leader_phase1_final_trajectories.json"
    
    @classmethod
    def ensure_directories(cls):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        cls.AGENT_DIR.mkdir(parents=True, exist_ok=True)
        cls.LOG_SB3_DIR.mkdir(parents=True, exist_ok=True)
        cls.MODEL_SB3_DIR.mkdir(parents=True, exist_ok=True)
    
    @classmethod
    def get_episode_model_path(cls, episode_num):
        """è·å–æŒ‡å®šå›åˆçš„æ¨¡å‹ä¿å­˜è·¯å¾„"""
        return cls.MODEL_DIR / f"leader_phase1_episode_{episode_num}"
    
    @classmethod
    def get_timestamped_path(cls, base_name, extension="json"):
        """è·å–å¸¦æ—¶é—´æˆ³çš„è·¯å¾„"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return cls.LOG_DIR / f"{base_name}_{timestamp}.{extension}"

class RewardTracker:
    """å¥–åŠ±è·Ÿè¸ªå’Œå¯è§†åŒ–ç±»"""
    def __init__(self, window_size=100):
        self.episode_rewards = []
        self.episode_lengths = []
        self.moving_avg_rewards = []
        self.success_rate = []
        self.collision_rate = []
        self.window_size = window_size
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # è®¾ç½®å›¾è¡¨å†…å­˜è­¦å‘Šé˜ˆå€¼ï¼Œé¿å…è­¦å‘Šå¹²æ‰°
        plt.rcParams['figure.max_open_warning'] = 50
        
    def add_episode(self, episode_reward, episode_length, success, collision):
        """æ·»åŠ æ–°å›åˆæ•°æ®"""
        self.episode_rewards.append(episode_reward)
        self.episode_lengths.append(episode_length)
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        if len(self.episode_rewards) >= self.window_size:
            moving_avg = np.mean(self.episode_rewards[-self.window_size:])
        else:
            moving_avg = np.mean(self.episode_rewards)
        self.moving_avg_rewards.append(moving_avg)
        
        # å­˜å‚¨æˆåŠŸæ ‡å¿—
        if not hasattr(self, 'success_flags'):
            self.success_flags = []
        self.success_flags.append(success)
        
        # è®¡ç®—æˆåŠŸç‡ - ä½¿ç”¨å­˜å‚¨çš„successæ ‡å¿—
        recent_episodes = min(len(self.success_flags), self.window_size)
        recent_successes = sum(self.success_flags[-recent_episodes:])
        self.success_rate.append(recent_successes / recent_episodes)
        
        # ç¢°æ’ç‡
        self.collision_rate.append(1.0 if collision else 0.0)
        
    def plot_training_progress(self, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾"""
        if save_path is None:
            save_path = PathConfig.TRAINING_PROGRESS_PLOT
        
        # å…³é—­ä¹‹å‰çš„å›¾è¡¨ï¼Œé¿å…å†…å­˜æ³„æ¼
        plt.close('all')
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        episodes = range(1, len(self.episode_rewards) + 1)
        
        # 1. å¥–åŠ±æ›²çº¿
        ax1.plot(episodes, self.episode_rewards, alpha=0.3, color='blue', label='åŸå§‹å¥–åŠ±')
        ax1.plot(episodes, self.moving_avg_rewards, color='red', linewidth=2, label=f'{self.window_size}å›åˆç§»åŠ¨å¹³å‡')
        ax1.set_xlabel('å›åˆæ•°')
        ax1.set_ylabel('å¥–åŠ±')
        ax1.set_title('ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå¥–åŠ±æ›²çº¿')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å›åˆé•¿åº¦
        ax2.plot(episodes, self.episode_lengths, color='green', alpha=0.7)
        ax2.set_xlabel('å›åˆæ•°')
        ax2.set_ylabel('å›åˆé•¿åº¦ï¼ˆæ­¥æ•°ï¼‰')
        ax2.set_title('å›åˆé•¿åº¦å˜åŒ–')
        ax2.grid(True, alpha=0.3)
        
        # 3. ç¢°æ’ç‡
        ax3.plot(episodes, self.collision_rate, color='red', alpha=0.7)
        ax3.set_xlabel('å›åˆæ•°')
        ax3.set_ylabel('ç¢°æ’ç‡')
        ax3.set_title('ç¢°æ’ç‡å˜åŒ–')
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1)
        
        # 4. æˆåŠŸç‡
        ax4.plot(episodes, self.success_rate, color='purple', alpha=0.7)
        ax4.set_xlabel('å›åˆæ•°')
        ax4.set_ylabel('æˆåŠŸç‡')
        ax4.set_title(f'å¯¼èˆªæˆåŠŸç‡ ({self.window_size}å›åˆæ»‘åŠ¨çª—å£)')
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… è®­ç»ƒè¿›åº¦å›¾å·²ä¿å­˜: {save_path}")
        
        return fig
    
    def save_data(self, save_path=None):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        if save_path is None:
            save_path = PathConfig.TRAINING_DATA_JSON
        # å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ä»¥æ”¯æŒJSONåºåˆ—åŒ–
        data = {
            'episode_rewards': [float(x) for x in self.episode_rewards],
            'episode_lengths': [int(x) for x in self.episode_lengths],
            'moving_avg_rewards': [float(x) for x in self.moving_avg_rewards],
            'success_rate': [float(x) for x in self.success_rate],
            'success_flags': [bool(x) for x in getattr(self, 'success_flags', [])],
            'collision_rate': [float(x) for x in self.collision_rate],
            'total_episodes': len(self.episode_rewards),
            'final_avg_reward': float(self.moving_avg_rewards[-1] if self.moving_avg_rewards else 0),
            'final_success_rate': float(self.success_rate[-1] if self.success_rate else 0),
            'algorithm': 'Stable-Baselines3 PPO',
            'timestamp': str(datetime.now())
        }
        
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: {save_path}")

class TrainingCallback(BaseCallback):
    """è‡ªå®šä¹‰å›è°ƒå‡½æ•° - ç”¨äºè·Ÿè¸ªè®­ç»ƒè¿›åº¦å’Œå®šæœŸä¿å­˜"""
    
    def __init__(self, reward_tracker, max_episodes, previous_episode_count=0, plot_interval=500, save_interval=500, verbose=1):
        super(TrainingCallback, self).__init__(verbose)
        self.reward_tracker = reward_tracker
        self.max_episodes = max_episodes  # æœ€å¤§å›åˆæ•°é™åˆ¶
        self.previous_episode_count = previous_episode_count  # ä¹‹å‰çš„å›åˆæ•°
        self.plot_interval = plot_interval
        self.save_interval = save_interval
        self.episode_count = previous_episode_count  # ä»ä¹‹å‰çš„å›åˆæ•°å¼€å§‹
        self.episode_reward = 0
        self.episode_length = 0
        self.start_time = time.time()
        
        
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        # ç´¯ç§¯å¥–åŠ±
        self.episode_reward += self.locals['rewards'][0]
        self.episode_length += 1
        
        # æ£€æŸ¥æ˜¯å¦å›åˆç»“æŸ
        if self.locals['dones'][0]:
            self.episode_count += 1
            
            # è·å–infoä¿¡æ¯
            info = self.locals['infos'][0]
            success = info.get('success', False)
            
            # ä»reward_infoä¸­åˆ¤æ–­æ˜¯å¦ç¢°æ’
            reward_info = info.get('reward_info', {})
            crash_reward = reward_info.get('crash', 0)
            collision = crash_reward < 0  # å¦‚æœæœ‰ç¢°æ’æƒ©ç½šï¼Œè¯´æ˜å‘ç”Ÿäº†ç¢°æ’
            
            # è®°å½•åˆ°tracker
            self.reward_tracker.add_episode(
                self.episode_reward,
                self.episode_length,
                success,
                collision
            )
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            current_avg_reward = np.mean(self.reward_tracker.episode_rewards[-10:]) if len(self.reward_tracker.episode_rewards) >= 10 else self.episode_reward
            current_success_rate = self.reward_tracker.success_rate[-1] if self.reward_tracker.success_rate else 0
            current_collision_rate = np.mean(self.reward_tracker.collision_rate[-10:]) if len(self.reward_tracker.collision_rate) >= 10 else (1.0 if collision else 0.0)
            
            # è®¡ç®—ETA
            elapsed_time = time.time() - self.start_time
            # ä½¿ç”¨max_episodesä½œä¸ºæ€»å›åˆæ•°
            total_episodes = self.max_episodes
            eta = (elapsed_time / (self.episode_count - self.previous_episode_count)) * (total_episodes - self.episode_count) if (self.episode_count - self.previous_episode_count) > 0 else 0
            
            # è·å–ç»ˆæ­¢ç±»å‹
            if success:
                termination_type = "æˆåŠŸ"
            elif collision:
                # å°è¯•ä»reward_infoè·å–æ›´è¯¦ç»†çš„ç¢°æ’ç±»å‹
                contact_points = reward_info.get('contact_points', 0)
                if contact_points > 0:
                    termination_type = f"ç‰©ç†ç¢°æ’({contact_points}ç‚¹)"
                else:
                    termination_type = "ç¢°æ’"
            else:
                termination_type = "è¶…æ—¶"
            
            # æ‰“å°è¿›åº¦
            print(f"å›åˆ {self.episode_count:5d}/{total_episodes} | "
                  f"å¥–åŠ±: {self.episode_reward:8.2f} | "
                  f"å¹³å‡: {current_avg_reward:6.2f} | "
                  f"æ­¥æ•°: {self.episode_length:4d} | "
                  f"æˆåŠŸ: {'âœ“' if success else 'âœ—'} | "
                  f"æˆåŠŸç‡: {current_success_rate:.1%} | "
                  f"ç»ˆæ­¢: {termination_type} | "
                  f"ç¢°æ’ç‡: {current_collision_rate:.1%} | "
                  f"ETA: {eta/60:.1f}åˆ†é’Ÿ")
            
            
            # å®šæœŸç»˜åˆ¶å’Œä¿å­˜
            if self.episode_count % self.plot_interval == 0:
                self.reward_tracker.plot_training_progress()
                self.reward_tracker.save_data()
                
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if self.episode_count % self.save_interval == 0:
                model_path = PathConfig.get_episode_model_path(self.episode_count)
                self.model.save(model_path)
                print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
                print("-" * 80)
            
            # é‡ç½®å›åˆç»Ÿè®¡
            self.episode_reward = 0
            self.episode_length = 0
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å›åˆæ•°ï¼Œå¦‚æœæ˜¯åˆ™åœæ­¢è®­ç»ƒ
            if self.episode_count >= self.max_episodes:
                print("="*80)
                print(f"âœ… å·²å®Œæˆ {self.max_episodes} å›åˆè®­ç»ƒï¼Œåœæ­¢è®­ç»ƒ")
                print("="*80)
                return False  # è¿”å›Falseåœæ­¢è®­ç»ƒ
        
        return True
    

def make_env(max_steps=1000):
    """åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°
    
    Args:
        max_steps: æ¯ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•° (é»˜è®¤1000)
    """
    # å›ºå®šå…¨å±€éšæœºç§å­
    random.seed(42)
    np.random.seed(42)
    
    def _init():
        env = DroneNavigationMulti(
            num_drones=1,
            use_depth_camera=True,
            depth_camera_range=10.0,
            depth_resolution=16,
            enable_formation_force=False,
            training_stage=1,
            max_steps=max_steps
        )
        return env
    return _init


def continue_train_leader_phase1_sb3(target_total_episodes=100000, total_timesteps=None, plot_interval=100, 
                                    load_model_path=None):
    """ç»§ç»­ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ - é¢†èˆªè€…å•æœºå¯¼èˆªè®­ç»ƒï¼ˆä½¿ç”¨PPOç®—æ³•ï¼‰
    
    Args:
        target_total_episodes: ç›®æ ‡æ€»è®­ç»ƒå›åˆæ•° (é»˜è®¤100000)
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™æ ¹æ®å‰©ä½™å›åˆæ•°ä¼°ç®—ï¼‰
        plot_interval: ç»˜å›¾å’Œä¿å­˜é—´éš”
        load_model_path: è¦åŠ è½½çš„æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹ï¼‰
    """
    print("="*80)
    print("ç»§ç»­ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ - é¢†èˆªè€…å¯¼èˆªè®­ç»ƒ")
    print(f"ç›®æ ‡æ€»å›åˆæ•°: {target_total_episodes}")
    print("="*80)
    
    # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
    PathConfig.ensure_directories()
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    print("æ­£åœ¨åˆ›å»ºç¯å¢ƒ...")
    env = DummyVecEnv([make_env(max_steps=1000)])
    
    # è·å–ç¯å¢ƒé…ç½®ä¿¡æ¯
    test_env = env.envs[0]
    print(f"ç¯å¢ƒé…ç½®:")
    print(f"  - æ— äººæœºæ•°é‡: {test_env.num_drones}")
    print(f"  - è§‚æµ‹ç©ºé—´: {test_env.observation_space.shape}")
    print(f"  - åŠ¨ä½œç©ºé—´: {test_env.action_space.shape}")
    print(f"  - æ·±åº¦ç‰¹å¾ç»´åº¦: {test_env.depth_feature_dim}")
    print(f"  - è®­ç»ƒé˜¶æ®µ: {test_env.training_stage}")
    print(f"  - ç¼–é˜ŸåŠ›çŠ¶æ€: {'ç¦ç”¨' if not test_env.enable_formation_force else 'å¯ç”¨'}")
    print(f"  - å¹³é¢æ¨¡å¼: {'å¯ç”¨' if test_env.enforce_planar else 'ç¦ç”¨'}")
    print(f"  - æœ€å¤§æ­¥æ•°: {test_env.max_steps}")
    
    # æ£€æµ‹å¹¶å¼ºåˆ¶ä½¿ç”¨GPU
    if torch.cuda.is_available():
        device_name = 'cuda'
        print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device_name = 'cpu'
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    
    # ç¡®å®šè¦åŠ è½½çš„æ¨¡å‹è·¯å¾„
    if load_model_path is None:
        # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
        if PathConfig.FINAL_MODEL.with_suffix('.zip').exists():
            load_model_path = PathConfig.FINAL_MODEL
            print(f"ğŸ”„ æ‰¾åˆ°æœ€ç»ˆæ¨¡å‹ï¼ŒåŠ è½½: {load_model_path}")
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„episodeæ¨¡å‹
            model_files = list(PathConfig.MODEL_DIR.glob("leader_phase1_episode_*.zip"))
            if model_files:
                # æŒ‰episodeæ•°æ’åºï¼Œå–æœ€å¤§çš„
                model_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
                load_model_path = model_files[-1].with_suffix('')
                print(f"ğŸ”„ æ‰¾åˆ°æœ€æ–°episodeæ¨¡å‹ï¼ŒåŠ è½½: {load_model_path}")
            else:
                raise FileNotFoundError("âŒ æœªæ‰¾åˆ°ä»»ä½•å·²ä¿å­˜æ¨¡å‹ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
    else:
        print(f"ğŸ”„ åŠ è½½æŒ‡å®šæ¨¡å‹: {load_model_path}")
    
    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = PPO.load(load_model_path, env=env, device=device_name)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    # å°è¯•åŠ è½½è®­ç»ƒå†å²æ•°æ®
    history_data = None
    if PathConfig.TRAINING_DATA_JSON.exists():
        print("æ­£åœ¨åŠ è½½è®­ç»ƒå†å²...")
        with open(PathConfig.TRAINING_DATA_JSON, 'r', encoding='utf-8') as f:
            history_data = json.load(f)
        previous_total_episodes = history_data.get('total_episodes', 0)
        print(f"âœ… å·²åŠ è½½ {previous_total_episodes} å›åˆçš„å†å²æ•°æ®")
        print(f"   ä¸Šæ¬¡æœ€ç»ˆæˆåŠŸç‡: {history_data.get('final_success_rate', 0):.1%}")
        print(f"   ä¸Šæ¬¡å¹³å‡å¥–åŠ±: {history_data.get('final_avg_reward', 0):.2f}")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå†å²æ•°æ®ï¼Œå°†ä»0å¼€å§‹ç»Ÿè®¡")
        previous_total_episodes = 0
    
    # è®¡ç®—å‰©ä½™å›åˆæ•°
    remaining_episodes = target_total_episodes - previous_total_episodes
    if remaining_episodes <= 0:
        print(f"âš ï¸  å·²ç»è¾¾åˆ°æˆ–è¶…è¿‡ç›®æ ‡å›åˆæ•° {target_total_episodes}ï¼Œå½“å‰ {previous_total_episodes} å›åˆ")
        return None, None, None
    
    print(f"ğŸ“Š ç»§ç»­è®­ç»ƒè®¡åˆ’:")
    print(f"   ä¹‹å‰å›åˆæ•°: {previous_total_episodes}")
    print(f"   ç›®æ ‡æ€»å›åˆæ•°: {target_total_episodes}")
    print(f"   å‰©ä½™å›åˆæ•°: {remaining_episodes}")
    
    # ä¼°ç®—æ€»æ­¥æ•°ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if total_timesteps is None:
        # ä½¿ç”¨æ›´ä¿å®ˆçš„ä¼°è®¡ï¼Œç¡®ä¿ä¸ä¼šå› ä¸ºæ­¥æ•°é™åˆ¶è€Œè¿‡æ—©åœæ­¢
        # å‡è®¾å¹³å‡æ¯å›åˆ300æ­¥ï¼ˆè€ƒè™‘åˆ°æ¢ç´¢å’Œå¤±è´¥çš„æƒ…å†µï¼‰
        avg_steps_per_episode = 300
        total_timesteps = remaining_episodes * avg_steps_per_episode
        print(f"   ä¼°ç®—æ€»æ­¥æ•°: {total_timesteps:,} ({remaining_episodes}å›åˆ Ã— {avg_steps_per_episode}æ­¥)")
        print(f"   âš ï¸  æ³¨æ„: å®é™…è®­ç»ƒå°†åœ¨è¾¾åˆ° {target_total_episodes} æ€»å›åˆæ—¶åœæ­¢ï¼ˆç”±å›è°ƒå‡½æ•°æ§åˆ¶ï¼‰")
    
    print("="*80)
    
    # åˆ›å»ºå¥–åŠ±è·Ÿè¸ªå™¨
    reward_tracker = RewardTracker(window_size=50)
    
    # æ¢å¤å†å²æ•°æ®
    if history_data:
        print("æ­£åœ¨æ¢å¤è®­ç»ƒå†å²...")
        reward_tracker.episode_rewards = history_data.get('episode_rewards', [])
        reward_tracker.episode_lengths = history_data.get('episode_lengths', [])
        reward_tracker.moving_avg_rewards = history_data.get('moving_avg_rewards', [])
        reward_tracker.success_rate = history_data.get('success_rate', [])
        reward_tracker.success_flags = history_data.get('success_flags', [])
        reward_tracker.collision_rate = history_data.get('collision_rate', [])
        print(f"âœ… å·²æ¢å¤ {len(reward_tracker.episode_rewards)} å›åˆçš„è®­ç»ƒå†å²")
    
    # åˆ›å»ºå›è°ƒå‡½æ•°
    callback = TrainingCallback(
        reward_tracker=reward_tracker,
        max_episodes=target_total_episodes,  # ç›®æ ‡æ€»å›åˆæ•°
        previous_episode_count=previous_total_episodes,  # ä¹‹å‰çš„å›åˆæ•°
        plot_interval=plot_interval,
        save_interval=plot_interval
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹ç»§ç»­è®­ç»ƒ...")
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            log_interval=10,  # æ¯10ä¸ªepisodeæ‰“å°ä¸€æ¬¡æ—¥å¿—
            progress_bar=False  # æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ˜¾ç¤º
        )
    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼")
    
    # è®­ç»ƒå®Œæˆ
    training_time = time.time() - start_time
    print("="*80)
    print("ç»§ç»­è®­ç»ƒå®Œæˆï¼")
    
    # æœ€ç»ˆç»Ÿè®¡
    final_avg_reward = np.mean(reward_tracker.episode_rewards[-50:]) if len(reward_tracker.episode_rewards) >= 50 else np.mean(reward_tracker.episode_rewards)
    final_success_rate = reward_tracker.success_rate[-1] if reward_tracker.success_rate else 0
    
    print(f"æœ€ç»ˆç»Ÿè®¡:")
    print(f"  - æ€»å›åˆæ•°: {len(reward_tracker.episode_rewards)}")
    print(f"  - æ€»æ­¥æ•°: {callback.num_timesteps}")
    print(f"  - æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_reward:.2f}")
    print(f"  - æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.2%}")
    print(f"  - è®­ç»ƒæ—¶é•¿: {training_time/60:.1f}åˆ†é’Ÿ")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œæ•°æ®
    final_model_path = PathConfig.FINAL_MODEL
    model.save(final_model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    reward_tracker.plot_training_progress(PathConfig.FINAL_PROGRESS_PLOT)
    reward_tracker.save_data(PathConfig.FINAL_DATA_JSON)
    
    
    print("="*80)
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜:")
    print(f"  ğŸ“Š æ—¥å¿—å’Œå›¾è¡¨: agent/log_SB3/")
    print(f"  ğŸ“ æ¨¡å‹æ–‡ä»¶: agent/model_SB3/")
    print("="*80)
    
    env.close()
    return final_model_path, reward_tracker, model


if __name__ == '__main__':
    # è¿è¡Œç»§ç»­è®­ç»ƒ
    model_path, reward_tracker, model = continue_train_leader_phase1_sb3(
        target_total_episodes=100000,  # ç›®æ ‡æ€»å›åˆæ•°
        total_timesteps=None,          # è‡ªåŠ¨æ ¹æ®å‰©ä½™å›åˆæ•°ä¼°ç®—
        plot_interval=100,            # æ¯100å›åˆç»˜åˆ¶ä¸€æ¬¡å›¾
        load_model_path=None          # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
    )
    
    print(f"\nç»§ç»­è®­ç»ƒå®Œæˆï¼æ£€æŸ¥ {PathConfig.LOG_DIR} ç›®å½•æŸ¥çœ‹ç»“æœå›¾è¡¨å’Œæ•°æ®ã€‚")