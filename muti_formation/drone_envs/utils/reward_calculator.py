"""
ğŸ¯ æç®€ç‰ˆå¥–åŠ±è®¡ç®—æ¨¡å— - æ— å†—ä½™è®¾è®¡
"""
import numpy as np
import pybullet as p
from typing import Dict, Any, Tuple, Optional
from .depth_obstacle_processor import DepthObstacleProcessor

class RewardCalculator:
    """ğŸ¯ æç®€ç‰ˆå¥–åŠ±è®¡ç®—å™¨ - å¹³è¡¡ç¨³å®šç‰ˆ v5ï¼ˆå–æ¶ˆå¯†é›†å¥–åŠ±ä¸Šé™ï¼‰
    
    æ ¸å¿ƒç†å¿µï¼š
    1. ä¸€ä¸ªè¡Œä¸ºï¼Œä¸€ä¸ªå¥–åŠ± - æ¶ˆé™¤åŠŸèƒ½é‡å 
    2. ğŸ”¥ æˆåŠŸå¥–åŠ±å æ¯”è¦è¶³å¤Ÿå¤§ - å¼•å¯¼æ­£ç¡®çš„å­¦ä¹ æ–¹å‘
    3. ğŸ”¥ ç¢°æ’æƒ©ç½šæ˜ç¡® - ç¡®ä¿é”™è¯¯è¡Œä¸ºæœ‰ä»£ä»·
    4. ğŸ”¥ æ— å¯†é›†å¥–åŠ±ä¸Šé™ - ä¸é˜»ç¢æ™ºèƒ½ä½“å­¦ä¹ å¤æ‚è·¯å¾„
    5. åœºæ™¯åŒ–å¥–åŠ± - æ ¹æ®ç¯å¢ƒåŠ¨æ€è°ƒæ•´
    
    å¥–åŠ±æ¶æ„ï¼ˆ4ç»„ä»¶ï¼‰ï¼š
    
    ğŸ“ ç¨€ç–å¥–åŠ±å±‚ (æ–¹å‘å¼•å¯¼)
    1. success: +10000 - æˆåŠŸåˆ°è¾¾ç›®æ ‡ï¼ˆå æ¯”å¤§ï¼Œå¼•å¯¼å­¦ä¹ ï¼‰
    2. crash: -2000 - ç¢°æ’å¤±è´¥ï¼ˆç¡®ä¿è´Ÿå€¼ï¼‰
    
    ğŸ“Š å¯†é›†å¥–åŠ±å±‚ (æ¢¯åº¦æä¾›) - æ— ä¸Šé™é™åˆ¶
    3. navigation: ~3.0/step - å¯¼èˆªä¸»ä¿¡å·
       â”” åˆå¹¶: è·ç¦»å˜åŒ– + æœå‘å¯¹é½
       â”” æ¥æº: navigation + forward_movement
    
    4. safe_navigation: ~1.0/step - å®‰å…¨å¯¼èˆª
       â”” èåˆ: é¿éšœ + è½¬å‘ + é€Ÿåº¦è°ƒèŠ‚
       â”” æ¥æº: obstacle + rotation + adaptive_speed
    
    å®é™…æ•°æ®åˆ†æï¼š
    - 3000æ­¥è¶…æ—¶å®é™…å¯†é›†å¥–åŠ±ï¼šâ‰ˆ5500åˆ†
    - æ­£å¸¸å¯¼èˆªå¯†é›†å¥–åŠ±ï¼šâ‰ˆ2000-3000åˆ†
    - ç†è®ºæœ€å¤§å¯†é›†å¥–åŠ±ï¼šâ‰ˆ21000åˆ†ï¼ˆæç«¯æƒ…å†µï¼‰
    
    å¥–åŠ±åˆ†å¸ƒç¤ºä¾‹ï¼ˆv5 - ç®€åŒ–ç‰ˆï¼‰ï¼š
    - å¿«é€ŸæˆåŠŸ(60æ­¥): +10000 +420 = +10420 (æˆåŠŸå 96%âœ“)
    - æ…¢é€ŸæˆåŠŸ(200æ­¥): +10000 +1400 = +11400 (æˆåŠŸå 88%âœ“)
    - ç¢°æ’å¤±è´¥(150æ­¥): -2000 +1000 = -1000 (è´Ÿå€¼âœ“ï¼Œé¿å…ç¢°æ’)
    - è¶…æ—¶å¤±è´¥(3000æ­¥): 0 +5500 = +5500 (æ˜æ˜¾ä½äºæˆåŠŸâœ“ï¼Œé¼“åŠ±æ•ˆç‡)
    
    è®¾è®¡ç†å¿µï¼š
    âœ… æˆåŠŸå æ¯”å¤§ï¼ˆ88-96%ï¼‰ï¼Œå¼•å¯¼æ­£ç¡®æ–¹å‘
    âœ… ç¢°æ’å¿…ä¸ºè´Ÿå€¼ï¼Œæ˜ç¡®é”™è¯¯ä»£ä»·
    âœ… æ— ä¸Šé™é™åˆ¶ï¼Œå…è®¸æ™ºèƒ½ä½“å……åˆ†æ¢ç´¢å’Œå­¦ä¹ 
    âœ… ç®€å•ç›´æ¥ï¼Œæ˜“äºè°ƒè¯•å’Œç†è§£
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨
        
        Args:
            config: å¥–åŠ±é…ç½®å‚æ•°
        """
        # ç¨€ç–å¥–åŠ±é…ç½®
        self.success_bonus = config.get('success_bonus', 10000.0)
        self.crash_penalty = config.get('crash_penalty', -2000.0)
        
        # å…¶ä»–å‚æ•°
        self.collision_distance = config.get('collision_distance', 0.6)
        
        # çŠ¶æ€è®°å½•
        self.previous_distances = {}  # è®°å½•ä¸Šä¸€æ­¥è·ç¦»
        
        # âœ… åˆå§‹åŒ–æ·±åº¦å¤„ç†å™¨ï¼ˆç”¨äºä¸“ä¸šçš„éšœç¢ç‰©åˆ†æï¼‰
        self.depth_processor = DepthObstacleProcessor(
            depth_image_size=(128, 160),
            collision_threshold=config.get('collision_distance', 0.6),
            depth_scale=config.get('depth_scale', 4.0),
            max_depth=config.get('max_depth', 2.0),
            cnn_feature_dim=config.get('cnn_feature_dim', 128)
        )

        # å¯†é›†å¥–åŠ±ç¼©æ”¾ç³»æ•°ï¼ˆå»ºè®®0.2~0.3ï¼‰
        self.dense_scale = config.get('dense_scale', 0.2)
        
    def compute_total_reward(self,
                           drone_id: str,
                           position: np.ndarray,
                           target_position: np.ndarray,
                           velocity: np.ndarray,
                           depth_info: Dict[str, float],
                           orientation: Optional[np.ndarray] = None,
                           formation_info: Optional[Dict[str, Any]] = None,
                           done: bool = False,
                           success: bool = False,
                           current_step: int = 0) -> Tuple[float, Dict[str, float]]:
        """
        ğŸ¯ æç®€ç‰ˆå¥–åŠ±è®¡ç®— - å¹³è¡¡ç¨³å®šç‰ˆ v5ï¼ˆå–æ¶ˆå¯†é›†å¥–åŠ±ä¸Šé™ï¼‰
        
        å¥–åŠ±ç»„æˆï¼š
        1. success (ç¨€ç–): +10000 - æˆåŠŸå æ¯”å¤§ï¼Œå¼•å¯¼å­¦ä¹ 
        2. crash (ç¨€ç–): -2000 - ç¢°æ’æƒ©ç½šï¼Œç¡®ä¿è´Ÿå€¼
        3. navigation (å¯†é›†): ~3.0/step - è·ç¦»+æœå‘ï¼ˆæ— ä¸Šé™ï¼‰
        4. safe_navigation (å¯†é›†): ~1.0/step - é¿éšœ+è½¬å‘+é€Ÿåº¦ï¼ˆæ— ä¸Šé™ï¼‰
        
        é¢„æœŸæ€»å¥–åŠ±ï¼ˆv5 - ç®€åŒ–ç‰ˆï¼‰ï¼š
        - å¿«é€ŸæˆåŠŸ(60æ­¥): +10000 +420 = +10420 (æˆåŠŸå 96%âœ“)
        - æ…¢é€ŸæˆåŠŸ(200æ­¥): +10000 +1400 = +11400 (æˆåŠŸå 88%âœ“)
        - ç¢°æ’å¤±è´¥(150æ­¥): -2000 +1000 = -1000 (è´Ÿå€¼âœ“)
        - è¶…æ—¶å¤±è´¥(3000æ­¥): 0 +5500 = +5500 (æ˜æ˜¾ä½äºæˆåŠŸâœ“)
        
        è®¾è®¡ç†å¿µï¼š
        âœ… æˆåŠŸå æ¯”å¤§ï¼ˆ88-96%ï¼‰ï¼Œå¼•å¯¼æ­£ç¡®æ–¹å‘
        âœ… ç¢°æ’å¿…ä¸ºè´Ÿå€¼ï¼Œæ˜ç¡®é”™è¯¯ä»£ä»·
        âœ… æ— ä¸Šé™é™åˆ¶ï¼Œå…è®¸æ™ºèƒ½ä½“å……åˆ†æ¢ç´¢å’Œå­¦ä¹ 
        âœ… ç®€å•ç›´æ¥ï¼Œæ˜“äºè°ƒè¯•
        """
        reward_details = {}

        # 1. æˆåŠŸå¥–åŠ± - ç¨€ç–ï¼Œæœ€é«˜ä¼˜å…ˆçº§
        reward_details['success'] = self.success_bonus if success else 0.0

        # 2. ç¢°æ’æƒ©ç½š - ç¨€ç–ï¼Œå¼ºè´Ÿåé¦ˆ
        collision_occurred = depth_info.get('collision', False)
        reward_details['crash'] = self.crash_penalty if collision_occurred else 0.0
        
        # ğŸ”¥ 3. è¶…æ—¶æƒ©ç½š - ç¨€ç–ï¼Œé˜²æ­¢"æ‹–æ—¶é—´"ç­–ç•¥
        # å¦‚æœå›åˆç»“æŸä½†æ—¢æ²¡æˆåŠŸä¹Ÿæ²¡ç¢°æ’ï¼Œè¯´æ˜æ˜¯è¶…æ—¶
        # -8000ç¡®ä¿ï¼šæˆåŠŸ(+5900) >> ç¢°æ’(-950) > è¶…æ—¶(-1000)ï¼ˆmax_steps=1000æ—¶ï¼‰
        # è¶…æ—¶æ€»å¥–åŠ± = -8000 + 7000(å¯†é›†) = -1000ï¼ˆå¼ºè´Ÿå€¼ï¼Œå¿…é¡»é¿å…ï¼ï¼‰
        timeout_occurred = done and not success and not collision_occurred
        reward_details['timeout'] = -8000.0 if timeout_occurred else 0.0

        # 4. å¯¼èˆªå¥–åŠ± - å¯†é›†ï¼Œåˆå¹¶ç‰ˆï¼ˆè·ç¦»+æœå‘ï¼‰
        navigation_reward = self._compute_navigation_reward_merged(
            drone_id, position, target_position, velocity, orientation
        )
        # 5. å®‰å…¨å¯¼èˆªå¥–åŠ± - å¯†é›†ï¼Œèåˆç‰ˆï¼ˆé¿éšœ+è½¬å‘+é€Ÿåº¦ï¼‰
        safe_nav_reward = self._compute_safe_navigation_reward(
            depth_info, velocity, orientation, 
            np.linalg.norm(position - target_position)
        )
        # ç»Ÿä¸€ç¼©æ”¾å¯†é›†å¥–åŠ±
        reward_details['navigation'] = navigation_reward * self.dense_scale
        reward_details['safe_navigation'] = safe_nav_reward * self.dense_scale

        # è®¡ç®—æ€»å¥–åŠ±
        total_reward = sum(reward_details.values())

        return total_reward, reward_details
    
    def _compute_navigation_reward_merged(self, drone_id: str, position: np.ndarray, 
                                         target_position: np.ndarray,
                                         velocity: np.ndarray,
                                         orientation: Optional[np.ndarray]) -> float:
        """ğŸ¯ åˆå¹¶ç‰ˆå¯¼èˆªå¥–åŠ± - æ¶ˆé™¤å†—ä½™ï¼ˆv2: 2å€å¢å¼ºï¼‰
        
        åˆå¹¶åŠŸèƒ½ï¼š
        1. è·ç¦»å˜åŒ–å¥–åŠ±ï¼ˆæ¥è‡ªæ—§navigationï¼‰
        2. æœå‘å¯¹é½å¥–åŠ±ï¼ˆæ¥è‡ªæ—§forward_movementï¼‰
        
        è®¾è®¡åŸç†ï¼š
        - Part A: è·ç¦»å‡å°‘ = ä¸»è¦ä¿¡å·ï¼ˆå¼•å¯¼é è¿‘ï¼‰
        - Part B: æœå‘å¯¹é½ = è¾…åŠ©ä¿¡å·ï¼ˆé˜²æ­¢ä¾§æ»‘ã€åé€€ï¼‰
        
        é¢„æœŸè¾“å‡ºï¼ˆv2 - 2å€å¢å¼ºï¼‰ï¼š
        - æ­£å¸¸é£è¡Œï¼š+3.0/stepï¼ˆåŸ+1.5ï¼‰
        - åé€€/ä¾§æ»‘ï¼š-1.0/stepï¼ˆåŸ-0.5ï¼‰
        """
        current_distance = np.linalg.norm(position - target_position)
        
        # åˆå§‹åŒ–è·ç¦»è®°å½•
        if drone_id not in self.previous_distances:
            self.previous_distances[drone_id] = current_distance
            # ç¬¬ä¸€æ­¥åªç»™åŸºç¡€å€’æ•°å¥–åŠ±
            return 1.5 * max(0, (1.0 - current_distance / 40.0))
        
        prev_distance = self.previous_distances[drone_id]
        distance_change = prev_distance - current_distance  # æ­£=é è¿‘ï¼Œè´Ÿ=è¿œç¦»
        
        # æ›´æ–°è·ç¦»è®°å½•
        self.previous_distances[drone_id] = current_distance
        
        # ===== Part A: è·ç¦»å˜åŒ–å¥–åŠ±ï¼ˆä¸»è¦ä¿¡å·ï¼‰=====
        # ğŸ”¥ å¢å¼º2å€ï¼šæ¯0.1ç±³é è¿‘ = +1.6åˆ†ï¼ˆåŸæ¥0.8ï¼‰
        if distance_change > 0.01:  # é è¿‘ç›®æ ‡
            reward_distance = distance_change * 16.0  # 8.0 â†’ 16.0
            reward_distance = min(reward_distance, 4.0)  # å•æ­¥æœ€å¤š+4ï¼ˆåŸ+2ï¼‰
        elif distance_change < -0.01:  # è¿œç¦»ç›®æ ‡
            reward_distance = distance_change * 16.0  # 8.0 â†’ 16.0
            reward_distance = max(reward_distance, -2.0)  # å•æ­¥æœ€å¤š-2ï¼ˆåŸ-1ï¼‰
        else:
            reward_distance = 0.0
        
        # ===== Part B: æœå‘å¯¹é½å¥–åŠ±ï¼ˆè¾…åŠ©ä¿¡å·ï¼‰=====
        reward_alignment = 0.0
        
        if orientation is not None and np.linalg.norm(velocity[:2]) > 0.1:
            # è®¡ç®—æœå‘å‘é‡
            euler = p.getEulerFromQuaternion(orientation)
            yaw = euler[2]
            heading = np.array([np.cos(yaw), np.sin(yaw)])
            
            # è®¡ç®—åˆ°ç›®æ ‡çš„æ–¹å‘
            to_target = target_position[:2] - position[:2]
            distance_2d = np.linalg.norm(to_target)
            
            if distance_2d > 0.1:
                to_target_normalized = to_target / distance_2d
                
                # æœå‘ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦ï¼ˆ-1åˆ°1ï¼‰
                alignment = np.dot(heading, to_target_normalized)
                
                # ğŸ”¥ å¢å¼º2å€æœå‘å¥–åŠ±
                if alignment > 0.7:  # æœå‘ç›®æ ‡ï¼ˆcos(45Â°)â‰ˆ0.7ï¼‰
                    reward_alignment = 1.0 * (alignment - 0.7) / 0.3  # 0åˆ°+1.0ï¼ˆåŸ+0.5ï¼‰
                elif alignment < 0:  # èƒŒå¯¹ç›®æ ‡
                    reward_alignment = -0.6 * abs(alignment)  # 0åˆ°-0.6ï¼ˆåŸ-0.3ï¼‰
                # ä¾§å‘ä¸ç»™å¥–åŠ±ä¹Ÿä¸æƒ©ç½šï¼ˆå…è®¸ç»•è·¯é¿éšœï¼‰
        
        # åˆå¹¶å¥–åŠ±
        total_reward = reward_distance + reward_alignment
        
        return total_reward
    
    def _compute_safe_navigation_reward(self, depth_info: Dict[str, float],
                                       velocity: np.ndarray,
                                       orientation: Optional[np.ndarray],
                                       distance_to_target: float) -> float:
        """ğŸ¯ èåˆç‰ˆå®‰å…¨å¯¼èˆªå¥–åŠ± - å›ºå®šç¿¼ç‰¹åŒ–è®¾è®¡
        
        ğŸ›« å›ºå®šç¿¼ç‰¹æ€§ï¼š
        1. ä¸èƒ½æ‚¬åœæˆ–æ€¥åˆ¹è½¦
        2. é¿éšœä¸»è¦é è½¬å¼¯
        3. æ¨åŠ›å‡å°åªèƒ½ç¼“æ…¢é™é€Ÿ
        4. å¿…é¡»ä¿æŒæœ€å°å‰è¿›é€Ÿåº¦
        
        é¿éšœç­–ç•¥ï¼š
        - å¼€é˜”ç©ºé—´ï¼šå…¨é€Ÿç›´è¡Œ
        - å‘ç°éšœç¢ï¼šæå‰è½¬å‘
        - å±é™©åŒºåŸŸï¼šå¤§è§’åº¦è½¬å¼¯+é€‚åº¦é™é€Ÿ
        - ç´§æ€¥æƒ…å†µï¼šæ€¥è½¬å¼¯é€ƒç¦»
        
        é¢„æœŸè¾“å‡ºï¼š
        - å¼€é˜”å…¨é€Ÿï¼š+2.0/step
        - æå‰è½¬å‘ï¼š+1.5/step
        - æ­£ç¡®é¿éšœï¼š+1.0/step
        - é”™è¯¯è¡Œä¸ºï¼š-1.6/step
        """
        depth_map = depth_info.get('depth_map', None)
        
        if depth_map is None:
            return 0.0
        
        # ===== Step 1: ä½¿ç”¨æ·±åº¦å¤„ç†å™¨è¿›è¡Œä¸“ä¸šåˆ†æ =====
        obstacle_analysis = self.depth_processor.get_obstacle_analysis(depth_map)
        
        # è·å–ç»“æ„åŒ–çš„éšœç¢ç‰©ä¿¡æ¯
        danger_level = obstacle_analysis['danger_level']  # 0-1ï¼Œ1æœ€å±é™©
        forward_openness = obstacle_analysis['forward_openness']  # 0-1ï¼Œ1æœ€å¼€é˜”
        physical_min_depth = obstacle_analysis['physical_min_depth']  # ç‰©ç†è·ç¦»ï¼ˆç±³ï¼‰
        
        # ä» depth_info è·å–å·²è®¡ç®—çš„å·¦å³åŒºåŸŸæ·±åº¦ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        left_depth = depth_info.get('left_min', 0.5)  # æ ‡å‡†åŒ–æ·±åº¦
        right_depth = depth_info.get('right_min', 0.5)
        
        # è®¡ç®—é€Ÿåº¦å’Œè§’é€Ÿåº¦
        speed_2d = np.linalg.norm(velocity[:2])
        angular_vel = depth_info.get('angular_velocity', 0.0)
        
        # ===== Step 2: å›ºå®šç¿¼é¿éšœç­–ç•¥ =====
        
        # ï¿½ å›ºå®šç¿¼æ ¸å¿ƒï¼šé¿éšœé è½¬å¼¯ï¼Œä¸èƒ½å‡é€Ÿæ‚¬åœ
        
        # åœºæ™¯A: éå¸¸å¼€é˜”ï¼ˆdanger_level < 0.2ï¼Œå‰æ–¹>4mï¼‰â†’ å…¨é€Ÿç›´è¡Œ
        if danger_level < 0.2:
            # å‰æ–¹å®‰å…¨ï¼Œé¼“åŠ±ä¿æŒé€Ÿåº¦
            if speed_2d > 1.5:  # ä¿æŒè¾ƒé«˜é€Ÿåº¦
                return +2.0  # ä¼˜ç§€ï¼å…¨é€Ÿå‰è¿›
            elif speed_2d > 1.0:  # ä¸­ç­‰é€Ÿåº¦
                return +1.2  # ä¸é”™ï¼Œä½†å¯ä»¥æ›´å¿«
            else:  # é€Ÿåº¦å¤ªæ…¢
                return +0.3  # å‰æ–¹å¼€é˜”ï¼Œåº”è¯¥åŠ é€Ÿ
        
        # åœºæ™¯B: è¾ƒå¼€é˜”ï¼ˆdanger_level < 0.4ï¼Œå‰æ–¹2-4mï¼‰â†’ ä¿æŒé€Ÿåº¦ï¼Œå‡†å¤‡è½¬å‘
        elif danger_level < 0.4:
            # å‰æ–¹è¿˜ç®—å®‰å…¨ï¼Œä¿æŒé€Ÿåº¦ï¼Œå¦‚æœæœ‰éšœç¢ç‰©åˆ™å‡†å¤‡è½¬å‘
            left_right_diff = abs(left_depth - right_depth) * self.depth_processor.depth_scale
            
            if left_right_diff > 0.5:  # å·¦å³æœ‰å·®å¼‚ï¼Œåº”è¯¥è½¬å‘å¼€é˜”æ–¹å‘
                should_turn_left = left_depth > right_depth
                is_turning = (should_turn_left and angular_vel < -0.02) or \
                            (not should_turn_left and angular_vel > 0.02)
                
                if is_turning:
                    # æ­£åœ¨æå‰è½¬å‘ï¼Œå¾ˆå¥½ï¼
                    if speed_2d > 1.0:  # ä¿æŒé€Ÿåº¦çš„åŒæ—¶è½¬å‘
                        return +1.5  # ä¼˜ç§€ï¼šæå‰è§„é¿+ä¿æŒé€Ÿåº¦
                    else:
                        return +1.0  # ä¸é”™ï¼šåœ¨è½¬å‘
                else:
                    # åº”è¯¥è½¬å‘ä½†æ²¡è½¬
                    if speed_2d > 1.0:
                        return +0.8  # è¿˜è¡Œï¼Œè‡³å°‘ä¿æŒé€Ÿåº¦
                    else:
                        return +0.3  # ä¸€èˆ¬
            else:
                # å·¦å³å·®ä¸å¤šï¼Œç›´è¡Œå³å¯
                if speed_2d > 1.0:
                    return +1.2  # å¥½ï¼Œä¿æŒé€Ÿåº¦
                else:
                    return +0.5  # ä¸€èˆ¬
        
        # åœºæ™¯C: æ¥è¿‘éšœç¢ï¼ˆdanger_level < 0.7ï¼Œå‰æ–¹1-2mï¼‰â†’ å¿…é¡»è½¬å‘ï¼
        elif danger_level < 0.7:
            # ğŸ›« å›ºå®šç¿¼å…³é”®ï¼šè¿™é‡Œå¿…é¡»è½¬å¼¯ï¼Œä¸èƒ½é å‡é€Ÿ
            left_right_diff = abs(left_depth - right_depth) * self.depth_processor.depth_scale
            
            if left_right_diff > 0.3:  # æœ‰ä»»ä½•å·¦å³å·®å¼‚å°±åº”è¯¥è½¬å‘
                should_turn_left = left_depth > right_depth
                is_turning_correctly = (should_turn_left and angular_vel < -0.03) or \
                                      (not should_turn_left and angular_vel > 0.03)
                
                if is_turning_correctly:
                    # æ­£åœ¨è½¬å‘å¼€é˜”æ–¹å‘ï¼Œå›ºå®šç¿¼çš„æ­£ç¡®é¿éšœï¼
                    if abs(angular_vel) > 0.08:  # å¤§è§’åº¦è½¬å¼¯
                        return +1.5  # ä¼˜ç§€ï¼šå¤§è§’åº¦é¿éšœ
                    elif abs(angular_vel) > 0.04:  # ä¸­ç­‰è½¬å¼¯
                        return +1.0  # å¥½ï¼šæ­£åœ¨è½¬å‘
                    else:  # å°è§’åº¦è½¬å¼¯
                        return +0.6  # è¿˜è¡Œï¼Œä½†è½¬å¾—ä¸å¤Ÿ
                elif abs(angular_vel) > 0.03:
                    # è½¬é”™æ–¹å‘äº†ï¼
                    return -0.8  # å±é™©ï¼šè½¬å‘é”™è¯¯æ–¹å‘
                else:
                    # å±é™©ï¼šæ²¡æœ‰è½¬å‘ï¼
                    return -1.2  # ä¸¥é‡é”™è¯¯ï¼šéšœç¢ç‰©è¿‘äº†è¿˜ä¸è½¬
            else:
                # ä¸¤è¾¹å·®ä¸å¤šï¼Œéšä¾¿é€‰ä¸ªæ–¹å‘è½¬
                if abs(angular_vel) > 0.05:  # åœ¨è½¬å¼¯
                    return +0.8  # å¥½ï¼Œè‡³å°‘åœ¨é¿éšœ
                else:
                    return -1.0  # å±é™©ï¼šä¸è½¬å¼¯
        
        # åœºæ™¯D: éå¸¸å±é™©ï¼ˆdanger_level >= 0.7ï¼Œå‰æ–¹<1mï¼‰â†’ ç´§æ€¥è½¬å‘ï¼
        else:
            # ï¿½ ç´§æ€¥æƒ…å†µï¼šå¿…é¡»å¤§è§’åº¦æ€¥è½¬ï¼
            if abs(angular_vel) > 0.1:  # å¤§è§’åº¦æ€¥è½¬
                return +1.2  # å¥½ï¼ç´§æ€¥é¿éšœ
            elif abs(angular_vel) > 0.05:  # ä¸­ç­‰è½¬å¼¯
                return +0.6  # è¿˜è¡Œï¼Œä½†åº”è¯¥è½¬æ›´æ€¥
            else:
                # éå¸¸å±é™©è¿˜ä¸è½¬å¼¯ï¼
                return -2.0  # ä¸¥é‡é”™è¯¯ï¼šå³å°†ç¢°æ’è¿˜ä¸è½¬
        
        return 0.0
    
    def reset_state(self):
        """é‡ç½®çŠ¶æ€ï¼ˆç”¨äºæ–°å›åˆï¼‰"""
        self.previous_distances.clear()


def create_default_reward_config() -> Dict[str, Any]:
    """ğŸ¯ æç®€ç‰ˆå¥–åŠ±é…ç½® - å¹³è¡¡ç¨³å®šç‰ˆ v6ï¼ˆæ·»åŠ è¶…æ—¶æƒ©ç½šï¼‰
    
    æ ¸å¿ƒè®¾è®¡ï¼š
    1. ğŸ”¥ æˆåŠŸå¥–åŠ±å æ¯”è¦è¶³å¤Ÿå¤§ï¼Œå¼•å¯¼æ­£ç¡®å­¦ä¹ æ–¹å‘
    2. ğŸ”¥ ç¢°æ’æƒ©ç½šç¡®ä¿é”™è¯¯è¡Œä¸ºä»£ä»·æ˜ç¡®
    3. ğŸ”¥ è¶…æ—¶æƒ©ç½šé˜²æ­¢"æ‹–æ—¶é—´"ç­–ç•¥
    4. ğŸ”¥ å¯†é›†å¥–åŠ±å¼ºåº¦ä¸º1.0ï¼Œæä¾›å……åˆ†å¯¼èˆªæŒ‡å¯¼
    
    å®é™…æ•°æ®åˆ†æï¼ˆmax_steps=500ï¼ŒåŠ é€Ÿè®­ç»ƒï¼‰ï¼š
    - 500æ­¥è¶…æ—¶çš„å®é™…å¯†é›†å¥–åŠ±ï¼šâ‰ˆ3500åˆ†ï¼ˆ7åˆ†/æ­¥ï¼‰
    - ç†è®ºæœ€å¤§å¯†é›†å¥–åŠ±ï¼š7åˆ†/æ­¥ Ã— 500æ­¥ = 3500åˆ†
    - æ­£å¸¸å¯¼èˆªå¯†é›†å¥–åŠ±ï¼šâ‰ˆ420-1400åˆ†ï¼ˆ60-200æ­¥ï¼‰
    
    å¥–åŠ±æµ‹ç®—ï¼ˆv8 - max_steps=1000ï¼Œå¼ºè´Ÿå€¼è¶…æ—¶ï¼‰ï¼š
    - å¿«é€ŸæˆåŠŸ(60æ­¥):  +4500 +420 = +4920åˆ†  âœ“ æ•ˆç‡æœ€ä¼˜
    - æ…¢é€ŸæˆåŠŸ(200æ­¥): +4500 +1400 = +5900åˆ†  âœ“ æœ€é«˜å¥–åŠ±  
    - ç¢°æ’å¤±è´¥(150æ­¥): -2000 +1050 = -950åˆ†  âœ— æ˜ç¡®è´Ÿå€¼
    - è¶…æ—¶å¤±è´¥(1000æ­¥): -8000 +7000 = -1000åˆ† âœ—âœ— å¼ºè´Ÿå€¼ï¼ˆæ¯”ç¢°æ’è¿˜å·®ï¼ï¼‰
    
    å…³é”®æ”¹è¿›ï¼ˆv7â†’v8ï¼‰ï¼š
    1. è¶…æ—¶æƒ©ç½š: -500 â†’ -8000ï¼ˆå½»åº•è§£å†³"æ‹–å»¶ç­–ç•¥"ï¼‰
    2. ç¢°æ’æƒ©ç½š: -1000 â†’ -2000ï¼ˆç¡®ä¿æ˜ç¡®è´Ÿå€¼ï¼‰
    3. å¥–åŠ±æ’åºï¼šæˆåŠŸ(+5900) >>> ç¢°æ’(-950) > è¶…æ—¶(-1000)
    4. Agentå­¦åˆ°ï¼š"å¿…é¡»æˆåŠŸï¼æ‹–å»¶æ˜¯æœ€å·®ç­–ç•¥ï¼"
    
    è®¾è®¡ç†å¿µï¼š
    âœ… å¿«é€ŸæˆåŠŸæœ€ä¼˜ â†’ é¼“åŠ±é«˜æ•ˆå¯¼èˆª
    âœ… æ…¢é€ŸæˆåŠŸæ¬¡ä¼˜ â†’ å…è®¸è°¨æ…æ¢ç´¢
    âœ… è¶…æ—¶å¤±ä¸ºæ­£å€¼ï¼Œä½†æ˜æ˜¾ä½äºæˆåŠŸ â†’ ä¸é¼“åŠ±æ‹–æ—¶é—´
    âœ… ç¢°æ’å¿…ä¸ºè´Ÿå€¼ â†’ æ˜ç¡®é”™è¯¯ä»£ä»·
    """
    return {
        # ç¨€ç–å¥–åŠ± - ğŸ”¥ å¼ºåŒ–ç‰ˆè®¾è®¡ï¼ˆè§£å†³log_stdå¢é•¿+è¶…æ—¶é—®é¢˜ï¼‰
        'success_bonus': 4500.0,        # æˆåŠŸå¥–åŠ±
        'crash_penalty': -2000.0,       # ğŸ”¥ ç¢°æ’å¼ºæƒ©ç½šï¼Œç¡®ä¿ç¢°æ’å¿…ä¸ºè´Ÿå€¼
        
        # é¿éšœå‚æ•°
        'collision_distance': 0.6,       # ç¢°æ’é˜ˆå€¼ï¼š0.6ç±³
        'dense_scale': 1.0,              # ğŸ”¥ å¯†é›†å¥–åŠ±ä¸ç¼©æ”¾ï¼ˆä¿æŒ1.0ï¼‰
                                         # ç†ç”±ï¼šæä¾›å……åˆ†çš„å¯¼èˆªæŒ‡å¯¼èƒ½åŠ›ï¼Œ
                                         #      è¶…æ—¶æƒ©ç½š-8000è¶³å¤Ÿå¤§ï¼Œç¡®ä¿è¶…æ—¶ä¸ºå¼ºè´Ÿå€¼
        
        # æ·±åº¦å¤„ç†å™¨å‚æ•°
        'depth_scale': 4.0,              # æ·±åº¦ç¼©æ”¾å› å­
        'max_depth': 2.0,                # æœ€å¤§æ·±åº¦å€¼
        'cnn_feature_dim': 128,          # CNNç‰¹å¾ç»´åº¦
    }
