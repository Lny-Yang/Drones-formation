"""
ğŸ¯ æç®€ç‰ˆå¥–åŠ±è®¡ç®—æ¨¡å— - æ— å†—ä½™è®¾è®¡
"""
import numpy as np
import pybullet as p
from typing import Dict, Any, Tuple, Optional


class RewardCalculator:
    """ğŸ¯ æç®€ç‰ˆå¥–åŠ±è®¡ç®—å™¨ - 3ç»„ä»¶æ— å†—ä½™è®¾è®¡
    
    æ ¸å¿ƒç†å¿µï¼š
    1. ä¸€ä¸ªè¡Œä¸ºï¼Œä¸€ä¸ªå¥–åŠ± - æ¶ˆé™¤åŠŸèƒ½é‡å 
    2. ç¨€ç–ä¸»å¯¼æ–¹å‘ - æˆåŠŸå æ¯” > 90%
    3. å¯†é›†æä¾›æ¢¯åº¦ - åªä¿ç•™å¿…è¦ä¿¡å·
    4. åœºæ™¯åŒ–å¥–åŠ± - æ ¹æ®ç¯å¢ƒåŠ¨æ€è°ƒæ•´
    
    å¥–åŠ±æ¶æ„ï¼ˆ4ç»„ä»¶ï¼‰ï¼š
    
    ğŸ“ ç¨€ç–å¥–åŠ±å±‚ (æ–¹å‘å¼•å¯¼)
    1. success: +2000 - æˆåŠŸåˆ°è¾¾ç›®æ ‡
    2. crash: -1500 - ç¢°æ’å¤±è´¥
    
    ğŸ“Š å¯†é›†å¥–åŠ±å±‚ (æ¢¯åº¦æä¾›)  
    3. navigation: ~1.5/step - å¯¼èˆªä¸»ä¿¡å·
       â”” åˆå¹¶: è·ç¦»å˜åŒ– + æœå‘å¯¹é½
       â”” æ¥æº: navigation + forward_movement
    
    4. safe_navigation: ~0.5/step - å®‰å…¨å¯¼èˆª
       â”” èåˆ: é¿éšœ + è½¬å‘ + é€Ÿåº¦è°ƒèŠ‚
       â”” æ¥æº: obstacle + rotation + adaptive_speed
    
    å¥–åŠ±åˆ†å¸ƒç¤ºä¾‹ï¼š
    - å¿«é€ŸæˆåŠŸ(60æ­¥): +2000 +90 +30 = +2120 (æˆåŠŸå 94.3%)
    - æ…¢é€ŸæˆåŠŸ(200æ­¥): +2000 +300 +100 = +2400 (æˆåŠŸå 83.3%)
    - ç¢°æ’å¤±è´¥(150æ­¥): -1500 +150 +75 = -1275 (è´Ÿå€¼âœ“)
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨
        
        Args:
            config: å¥–åŠ±é…ç½®å‚æ•°
        """
        # ç¨€ç–å¥–åŠ±é…ç½®
        self.success_bonus = config.get('success_bonus', 2000.0)
        self.crash_penalty = config.get('crash_penalty', -1500.0)
        
        # å…¶ä»–å‚æ•°
        self.collision_distance = config.get('collision_distance', 0.6)
        
        # çŠ¶æ€è®°å½•
        self.previous_distances = {}  # è®°å½•ä¸Šä¸€æ­¥è·ç¦»
        
    def compute_total_reward(self,
                           drone_id: str,
                           position: np.ndarray,
                           target_position: np.ndarray,
                           velocity: np.ndarray,
                           depth_info: Dict[str, float],
                           orientation: Optional[np.ndarray] = None,
                           formation_info: Optional[Dict[str, Any]] = None,
                           done: bool = False,
                           success: bool = False,
                           current_step: int = 0) -> Tuple[float, Dict[str, float]]:
        """
        ğŸ¯ æç®€ç‰ˆå¥–åŠ±è®¡ç®— - 4ç»„ä»¶æ— å†—ä½™è®¾è®¡
        
        å¥–åŠ±ç»„æˆï¼š
        1. success (ç¨€ç–): +2000 - æˆåŠŸå 94%
        2. crash (ç¨€ç–): -1500 - ç¡®ä¿ç¢°æ’è´Ÿå€¼
        3. navigation (å¯†é›†): ~1.5/step - è·ç¦»+æœå‘
        4. safe_navigation (å¯†é›†): ~0.5/step - é¿éšœ+è½¬å‘+é€Ÿåº¦
        
        é¢„æœŸæ€»å¥–åŠ±ï¼š
        - å¿«é€ŸæˆåŠŸ(60æ­¥): +2120 (æˆåŠŸå 94.3%)
        - æ…¢é€ŸæˆåŠŸ(200æ­¥): +2400 (æˆåŠŸå 83.3%)
        - ç¢°æ’å¤±è´¥(150æ­¥): -1275 (è´Ÿå€¼âœ“)
        """
        reward_details = {}

        # 1. æˆåŠŸå¥–åŠ± - ç¨€ç–ï¼Œæœ€é«˜ä¼˜å…ˆçº§
        reward_details['success'] = self.success_bonus if success else 0.0

        # 2. ç¢°æ’æƒ©ç½š - ç¨€ç–ï¼Œå¼ºè´Ÿåé¦ˆ
        reward_details['crash'] = self.crash_penalty if (done and not success) else 0.0

        # 3. å¯¼èˆªå¥–åŠ± - å¯†é›†ï¼Œåˆå¹¶ç‰ˆï¼ˆè·ç¦»+æœå‘ï¼‰
        navigation_reward = self._compute_navigation_reward_merged(
            drone_id, position, target_position, velocity, orientation
        )
        reward_details['navigation'] = navigation_reward

        # 4. å®‰å…¨å¯¼èˆªå¥–åŠ± - å¯†é›†ï¼Œèåˆç‰ˆï¼ˆé¿éšœ+è½¬å‘+é€Ÿåº¦ï¼‰
        safe_nav_reward = self._compute_safe_navigation_reward(
            depth_info, velocity, orientation, 
            np.linalg.norm(position - target_position)
        )
        reward_details['safe_navigation'] = safe_nav_reward

        # è®¡ç®—æ€»å¥–åŠ±
        total_reward = sum(reward_details.values())

        return total_reward, reward_details
    
    def _compute_navigation_reward_merged(self, drone_id: str, position: np.ndarray, 
                                         target_position: np.ndarray,
                                         velocity: np.ndarray,
                                         orientation: Optional[np.ndarray]) -> float:
        """ğŸ¯ åˆå¹¶ç‰ˆå¯¼èˆªå¥–åŠ± - æ¶ˆé™¤å†—ä½™
        
        åˆå¹¶åŠŸèƒ½ï¼š
        1. è·ç¦»å˜åŒ–å¥–åŠ±ï¼ˆæ¥è‡ªæ—§navigationï¼‰
        2. æœå‘å¯¹é½å¥–åŠ±ï¼ˆæ¥è‡ªæ—§forward_movementï¼‰
        
        è®¾è®¡åŸç†ï¼š
        - Part A: è·ç¦»å‡å°‘ = ä¸»è¦ä¿¡å·ï¼ˆå¼•å¯¼é è¿‘ï¼‰
        - Part B: æœå‘å¯¹é½ = è¾…åŠ©ä¿¡å·ï¼ˆé˜²æ­¢ä¾§æ»‘ã€åé€€ï¼‰
        
        é¢„æœŸè¾“å‡ºï¼š
        - æ­£å¸¸é£è¡Œï¼š+1.5/step
        - åé€€/ä¾§æ»‘ï¼š-0.5/step
        """
        current_distance = np.linalg.norm(position - target_position)
        
        # åˆå§‹åŒ–è·ç¦»è®°å½•
        if drone_id not in self.previous_distances:
            self.previous_distances[drone_id] = current_distance
            # ç¬¬ä¸€æ­¥åªç»™åŸºç¡€å€’æ•°å¥–åŠ±
            return 1.5 * max(0, (1.0 - current_distance / 40.0))
        
        prev_distance = self.previous_distances[drone_id]
        distance_change = prev_distance - current_distance  # æ­£=é è¿‘ï¼Œè´Ÿ=è¿œç¦»
        
        # æ›´æ–°è·ç¦»è®°å½•
        self.previous_distances[drone_id] = current_distance
        
        # ===== Part A: è·ç¦»å˜åŒ–å¥–åŠ±ï¼ˆä¸»è¦ä¿¡å·ï¼‰=====
        # æ¯0.1ç±³é è¿‘ = +0.8åˆ†
        if distance_change > 0.01:  # é è¿‘ç›®æ ‡
            reward_distance = distance_change * 8.0
            reward_distance = min(reward_distance, 2.0)  # å•æ­¥æœ€å¤š+2
        elif distance_change < -0.01:  # è¿œç¦»ç›®æ ‡
            reward_distance = distance_change * 8.0
            reward_distance = max(reward_distance, -1.0)  # å•æ­¥æœ€å¤š-1
        else:
            reward_distance = 0.0
        
        # ===== Part B: æœå‘å¯¹é½å¥–åŠ±ï¼ˆè¾…åŠ©ä¿¡å·ï¼‰=====
        reward_alignment = 0.0
        
        if orientation is not None and np.linalg.norm(velocity[:2]) > 0.1:
            # è®¡ç®—æœå‘å‘é‡
            euler = p.getEulerFromQuaternion(orientation)
            yaw = euler[2]
            heading = np.array([np.cos(yaw), np.sin(yaw)])
            
            # è®¡ç®—åˆ°ç›®æ ‡çš„æ–¹å‘
            to_target = target_position[:2] - position[:2]
            distance_2d = np.linalg.norm(to_target)
            
            if distance_2d > 0.1:
                to_target_normalized = to_target / distance_2d
                
                # æœå‘ä¸ç›®æ ‡æ–¹å‘çš„å¯¹é½åº¦ï¼ˆ-1åˆ°1ï¼‰
                alignment = np.dot(heading, to_target_normalized)
                
                if alignment > 0.7:  # æœå‘ç›®æ ‡ï¼ˆcos(45Â°)â‰ˆ0.7ï¼‰
                    reward_alignment = 0.5 * (alignment - 0.7) / 0.3  # 0åˆ°+0.5
                elif alignment < 0:  # èƒŒå¯¹ç›®æ ‡
                    reward_alignment = -0.3 * abs(alignment)  # 0åˆ°-0.3
                # ä¾§å‘ä¸ç»™å¥–åŠ±ä¹Ÿä¸æƒ©ç½šï¼ˆå…è®¸ç»•è·¯é¿éšœï¼‰
        
        # åˆå¹¶å¥–åŠ±
        total_reward = reward_distance + reward_alignment
        
        return total_reward
    
    def _compute_safe_navigation_reward(self, depth_info: Dict[str, float],
                                       velocity: np.ndarray,
                                       orientation: Optional[np.ndarray],
                                       distance_to_target: float) -> float:
        """ğŸ¯ èåˆç‰ˆå®‰å…¨å¯¼èˆªå¥–åŠ± - åœºæ™¯åŒ–è®¾è®¡
        
        èåˆåŠŸèƒ½ï¼š
        1. é¿éšœåˆ¤æ–­ï¼ˆæ¥è‡ªæ—§obstacleï¼‰
        2. è½¬å‘å¼•å¯¼ï¼ˆæ¥è‡ªæ—§rotation_guidanceï¼‰
        3. é€Ÿåº¦è°ƒèŠ‚ï¼ˆæ¥è‡ªæ—§adaptive_speedï¼‰
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        æ ¹æ®å‰æ–¹æ·±åº¦åœºæ™¯ï¼Œç»™å‡º"åº”è¯¥æ€ä¹ˆåš"çš„å»ºè®®ï¼š
        - å¼€é˜”: é¼“åŠ±é«˜é€Ÿç›´è¡Œ
        - ç‹­çª„: é¼“åŠ±è½¬å‘å¼€é˜”æ–¹å‘
        - å±é™©: é¼“åŠ±å‡é€Ÿæˆ–é¿éšœ
        
        é¢„æœŸè¾“å‡ºï¼š
        - å¼€é˜”é«˜é€Ÿï¼š+1.0/step
        - æ­£å¸¸é£è¡Œï¼š+0.5/step  
        - æ­£ç¡®é¿éšœï¼š+0.3/step
        - é”™è¯¯è¡Œä¸ºï¼š-0.5/step
        """
        depth_map = depth_info.get('depth_map', None)
        
        if depth_map is None:
            return 0.0
        
        # ===== Step 1: åˆ†ææ·±åº¦ä¿¡æ¯ =====
        h, w = depth_map.shape
        
        # å‰æ–¹ä¸­å¤®åŒºåŸŸ
        center = depth_map[h//3:2*h//3, w//3:2*w//3]
        center_valid = center[center > 0.1]
        center_depth = float(center_valid.mean()) if len(center_valid) > 0 else 0.5
        
        # å·¦ä¾§åŒºåŸŸ
        left = depth_map[h//4:3*h//4, :w//3]
        left_valid = left[left > 0.1]
        left_depth = float(left_valid.mean()) if len(left_valid) > 0 else 0.5
        
        # å³ä¾§åŒºåŸŸ
        right = depth_map[h//4:3*h//4, 2*w//3:]
        right_valid = right[right > 0.1]
        right_depth = float(right_valid.mean()) if len(right_valid) > 0 else 0.5
        
        # è®¡ç®—é€Ÿåº¦å’Œè§’é€Ÿåº¦
        speed_2d = np.linalg.norm(velocity[:2])
        angular_vel = depth_info.get('angular_velocity', 0.0)
        
        # ===== Step 2: åœºæ™¯åˆ¤æ–­ä¸å¥–åŠ± =====
        
        # åœºæ™¯A: éå¸¸å¼€é˜”ï¼ˆå‰æ–¹>6mï¼‰â†’ åº”è¯¥é«˜é€Ÿç›´è¡Œ
        if center_depth > 1.5:
            if speed_2d > 3.0:
                return +1.0  # ä¼˜ç§€ï¼é«˜é€Ÿé€šè¿‡
            elif speed_2d > 2.0:
                return +0.7  # ä¸é”™
            elif speed_2d > 1.0:
                return +0.3  # è¿˜è¡Œ
            else:
                return -0.2  # å¤ªæ…¢äº†
        
        # åœºæ™¯B: è¾ƒå¼€é˜”ï¼ˆå‰æ–¹3-6mï¼‰â†’ åº”è¯¥ä¸­é€Ÿå‰è¿›
        elif center_depth > 0.75:
            if speed_2d > 1.5:
                return +0.5  # å¥½
            elif speed_2d > 0.8:
                return +0.3  # è¿˜è¡Œ
            else:
                return 0.0  # ä¸€èˆ¬
        
        # åœºæ™¯C: æ¥è¿‘éšœç¢ï¼ˆå‰æ–¹2-3mï¼‰â†’ åº”è¯¥è½¬å‘æˆ–å‡é€Ÿ
        elif center_depth > 0.5:
            # æ£€æŸ¥åº”è¯¥è½¬å‘å“ªè¾¹
            openness_diff = abs(left_depth - right_depth)
            
            if openness_diff > 0.3:  # æœ‰æ˜æ˜¾çš„å¼€é˜”æ–¹å‘
                should_turn_left = left_depth > right_depth
                is_turning_correctly = (should_turn_left and angular_vel < -0.05) or \
                                      (not should_turn_left and angular_vel > 0.05)
                
                if is_turning_correctly:
                    return +0.4  # å¥½ï¼æ­£åœ¨è½¬å‘å¼€é˜”æ–¹å‘
                elif abs(angular_vel) > 0.05:
                    return -0.2  # è½¬é”™æ–¹å‘äº†
                elif speed_2d < 1.0:
                    return +0.2  # è‡³å°‘åœ¨å‡é€Ÿ
                else:
                    return -0.3  # åº”è¯¥è½¬å‘æˆ–å‡é€Ÿ
            else:
                # ä¸¤è¾¹å·®ä¸å¤šï¼Œå‡é€Ÿå³å¯
                if speed_2d < 1.0:
                    return +0.3
                else:
                    return -0.2
        
        # åœºæ™¯D: éå¸¸å±é™©ï¼ˆå‰æ–¹<2mï¼‰â†’ åº”è¯¥ç´§æ€¥é¿éšœ
        else:
            if abs(angular_vel) > 0.1:  # åœ¨æ—‹è½¬é¿éšœ
                return +0.3
            elif speed_2d < 0.5:  # åœ¨å‡é€Ÿ
                return +0.2
            else:
                return -0.8  # å±é™©ï¼åº”è¯¥é¿éšœ
        
        return 0.0
    
    def reset_state(self):
        """é‡ç½®çŠ¶æ€ï¼ˆç”¨äºæ–°å›åˆï¼‰"""
        self.previous_distances.clear()


def create_default_reward_config() -> Dict[str, Any]:
    """ğŸ¯ æç®€ç‰ˆå¥–åŠ±é…ç½®
    
    æ ¸å¿ƒè®¾è®¡ï¼š
    1. ç¨€ç–å¥–åŠ±ä¸»å¯¼ï¼ˆæˆåŠŸå æ¯”>90%ï¼‰
    2. å¯†é›†å¥–åŠ±ç²¾ç®€ï¼ˆæ— å†—ä½™ï¼‰
    3. ç¢°æ’å¿…ä¸ºè´Ÿå€¼
    
    å¥–åŠ±æµ‹ç®—ï¼š
    - å¿«é€ŸæˆåŠŸ(60æ­¥): +2000 +90 +30 = +2120 (æˆåŠŸå 94.3%)
    - æ…¢é€ŸæˆåŠŸ(200æ­¥): +2000 +300 +100 = +2400 (æˆåŠŸå 83.3%)
    - ç¢°æ’å¤±è´¥(150æ­¥): -1500 +150 +75 = -1275 (è´Ÿå€¼âœ“)
    """
    return {
        # ç¨€ç–å¥–åŠ±
        'success_bonus': 2000.0,         # æˆåŠŸå¥–åŠ±ï¼ˆæé«˜åˆ°2000ï¼‰
        'crash_penalty': -1500.0,        # ç¢°æ’æƒ©ç½šï¼ˆæé«˜åˆ°-1500ï¼‰
        
        # é¿éšœå‚æ•°
        'collision_distance': 0.6,       # ç¢°æ’é˜ˆå€¼ï¼š0.6ç±³
        
        # æ·±åº¦å¤„ç†å™¨å‚æ•°
        'depth_scale': 4.0,              # æ·±åº¦ç¼©æ”¾å› å­
        'max_depth': 2.0,                # æœ€å¤§æ·±åº¦å€¼
        'cnn_feature_dim': 128,          # CNNç‰¹å¾ç»´åº¦
    }
