"""
æ‰¹é‡æµ‹è¯•å¤šä¸ªcheckpointæ¨¡å‹
ç”¨äºæ‰¾å‡ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³æ¨¡å‹
"""
import subprocess
import sys
from pathlib import Path
import json
import pandas as pd

def test_checkpoint(model_path, episodes=50):
    """æµ‹è¯•å•ä¸ªcheckpoint"""
    print(f"\n{'='*80}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_path}")
    print(f"{'='*80}")
    
    cmd = [
        sys.executable,
        "muti_formation/test_phase1_model.py",  
        "--model", str(model_path),
        "--episodes", str(episodes),
        "--no_render"  # æ‰¹é‡æµ‹è¯•æ—¶ä¸æ¸²æŸ“
    ]
    
    try:
        subprocess.run(cmd, check=True)
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def collect_results():
    """æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ"""
    # ğŸ”§ ä»ä¸“é—¨çš„æµ‹è¯•ç»“æœæ–‡ä»¶å¤¹è¯»å–
    log_dir = Path("muti_formation/agent/log/test_results")
    results = []
    
    if not log_dir.exists():
        print(f"âš ï¸ æµ‹è¯•ç»“æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {log_dir}")
        return results
    
    for result_file in log_dir.glob("phase1_test_results_*.json"):
        with open(result_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        model_name = result_file.stem.replace("phase1_test_results_", "")
        
        # æå–å…³é”®æŒ‡æ ‡
        num_episodes = len(data['episode_rewards'])
        success_rate = data['success_count'] / num_episodes if num_episodes > 0 else 0
        collision_rate = data['collision_count'] / num_episodes if num_episodes > 0 else 0
        avg_reward = sum(data['episode_rewards']) / num_episodes if num_episodes > 0 else 0
        avg_length = sum(data['episode_lengths']) / num_episodes if num_episodes > 0 else 0
        
        # è®¡ç®—æˆåŠŸå¥–åŠ±å æ¯”
        success_ratio = 0
        if data.get('success_episode_rewards'):
            avg_success_reward = sum(data['success_episode_rewards']) / len(data['success_episode_rewards'])
            success_component = sum([r for r in data['reward_components']['success'] if r > 0])
            if success_component > 0:
                success_ratio = (success_component / len([r for r in data['reward_components']['success'] if r > 0])) / avg_success_reward
        
        # è®¡ç®—æˆåŠŸ/ç¢°æ’æ¯”
        success_fail_ratio = 0
        if data.get('success_episode_rewards') and data.get('collision_episode_rewards'):
            avg_success = sum(data['success_episode_rewards']) / len(data['success_episode_rewards'])
            avg_collision = sum(data['collision_episode_rewards']) / len(data['collision_episode_rewards'])
            if avg_collision > 0:
                success_fail_ratio = avg_success / avg_collision
        
        results.append({
            'model': model_name,
            'success_rate': success_rate,
            'collision_rate': collision_rate,
            'avg_reward': avg_reward,
            'avg_length': avg_length,
            'success_ratio': success_ratio,
            'success_fail_ratio': success_fail_ratio
        })
    
    return results

def generate_comparison_report(results):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    df = pd.DataFrame(results)
    df = df.sort_values('success_rate', ascending=False)
    
    print("\n" + "="*120)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š")
    print("="*120)
    
    print("\næŒ‰æˆåŠŸç‡æ’åº:")
    print("-"*120)
    print(f"{'æ¨¡å‹åç§°':<40} | {'æˆåŠŸç‡':>8} | {'ç¢°æ’ç‡':>8} | {'å¹³å‡å¥–åŠ±':>10} | {'å¹³å‡æ­¥æ•°':>8} | {'æˆåŠŸå æ¯”':>8} | {'æˆåŠŸ/ç¢°æ’æ¯”':>10}")
    print("-"*120)
    
    for _, row in df.iterrows():
        print(f"{row['model']:<40} | "
              f"{row['success_rate']:>7.1%} | "
              f"{row['collision_rate']:>7.1%} | "
              f"{row['avg_reward']:>10.2f} | "
              f"{row['avg_length']:>8.1f} | "
              f"{row['success_ratio']:>7.1%} | "
              f"{row['success_fail_ratio']:>10.2f}")
    
    print("-"*120)
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model = df.iloc[0]
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['model']}")
    print(f"   æˆåŠŸç‡: {best_model['success_rate']:.1%}")
    print(f"   å¹³å‡å¥–åŠ±: {best_model['avg_reward']:.2f}")
    print(f"   å¹³å‡æ­¥æ•°: {best_model['avg_length']:.1f}")
    print(f"   æˆåŠŸå¥–åŠ±å æ¯”: {best_model['success_ratio']:.1%}")
    print(f"   æˆåŠŸ/ç¢°æ’æ¯”: {best_model['success_fail_ratio']:.2f}:1")
    
    # ğŸ”§ ä¿å­˜æŠ¥å‘Šåˆ°æµ‹è¯•ç»“æœæ–‡ä»¶å¤¹
    test_results_dir = Path("muti_formation/agent/log/test_results")
    test_results_dir.mkdir(parents=True, exist_ok=True)
    report_path = test_results_dir / "model_comparison_report.csv"
    df.to_csv(report_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    print("="*120)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•æ¨¡å‹")
    
    # å®šä¹‰è¦æµ‹è¯•çš„checkpoint
    model_dir = Path("muti_formation/agent/model")
    
    # æ–¹å¼1: æµ‹è¯•æ‰€æœ‰checkpoint
    # checkpoints = sorted(model_dir.glob("leader_phase1_episode_*.pth"))
    
    # æ–¹å¼2: æµ‹è¯•ç‰¹å®šcheckpointï¼ˆæ¨èï¼‰
    episodes_to_test = [9000, 11000]  # ğŸ”§ æ–°å¢ï¼šæµ‹è¯•14000å›åˆæ¨¡å‹
    checkpoints = []
    for ep in episodes_to_test:
        if ep == 'final':
            checkpoint_path = model_dir / "leader_phase1_final.pth"
        else:
            checkpoint_path = model_dir / f"leader_phase1_episode_{ep}.pth"
        
        if checkpoint_path.exists():
            checkpoints.append(checkpoint_path)
        else:
            print(f"âš ï¸ æ¨¡å‹ä¸å­˜åœ¨: {checkpoint_path}")
    
    print(f"æ‰¾åˆ° {len(checkpoints)} ä¸ªæ¨¡å‹å¾…æµ‹è¯•")
    
    # æµ‹è¯•æ¯ä¸ªcheckpoint
    for i, checkpoint in enumerate(checkpoints, 1):
        print(f"\nè¿›åº¦: [{i}/{len(checkpoints)}]")
        test_checkpoint(checkpoint, episodes=50)
    
    # æ”¶é›†å¹¶åˆ†æç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š æ”¶é›†æµ‹è¯•ç»“æœ...")
    print("="*80)
    results = collect_results()
    
    if results:
        generate_comparison_report(results)
    else:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœ")

if __name__ == '__main__':
    main()
