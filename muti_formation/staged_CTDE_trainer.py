
"""
åˆ†é˜¶æ®µCTDE PPOè®­ç»ƒè„šæœ¬ - é¢†èˆªè€…-è·Ÿéšè€…ç¼–é˜Ÿæ§åˆ¶
ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒé¢†èˆªè€…å•ç‹¬é¿éšœå’Œå¯¼èˆª
ç¬¬äºŒé˜¶æ®µï¼šå›ºå®šé¢†èˆªè€…ï¼Œè®­ç»ƒè·Ÿéšè€…ç¼–é˜Ÿè·Ÿè¸ª
"""
import sys
import os
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))
import torch
from datetime import datetime
import json
import torch
import numpy as np
from agent.CTDE_PPO_agent import CTDE_PPO, device
from drone_envs.envs.drone_env_multi import DroneNavigationMulti
from drone_envs.config import multi_drone_env

# PPOé…ç½® - ğŸ”§ ä¼˜åŒ–è¶…å‚æ•°ï¼Œé™ä½æ¢ç´¢å™ªå£°å’Œæ›´æ–°é¢‘ç‡
ppo_config = {
    'lr_actor': 0.0003,      # é™ä½å­¦ä¹ ç‡ï¼Œæé«˜ç¨³å®šæ€§
    'lr_critic': 0.0006,     # Criticå­¦ä¹ ç‡ç•¥é«˜äºActor
    'gamma': 0.99,
    'K_epochs': 10,          # ğŸ”¥ ä»20é™åˆ°10ï¼Œæ ‡å‡†PPOé…ç½®
    'eps_clip': 0.2,
    'has_continuous_action_space': True,
    'action_std_init': 0.3   # ğŸ”¥ ä»0.03é™ä½åˆ°0.3ï¼Œå‡å°‘æ¢ç´¢å™ªå£°
}

def save_log_to_json(log, env_name, phase):
    filename = Path(f"agent/log/CTDE_PPO_{env_name}_{phase}_LOG.json")
    filename.parent.mkdir(parents=True, exist_ok=True)
    print(f"saving log to {filename}")
    with open(filename, 'w') as f:
        json.dump(log, f)

def train_leader_only(env, ppo_agent, max_episodes=150, max_steps=500):
    """ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒé¢†èˆªè€…å•ç‹¬é¿éšœå’Œå¯¼èˆªï¼ˆå¢å¼ºæ·±åº¦æ„ŸçŸ¥ç‰ˆæœ¬ï¼‰- é’ˆå¯¹å¤§ç¯å¢ƒä¼˜åŒ–"""
    print("=== ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒé¢†èˆªè€…å•ç‹¬é¿éšœå’Œå¯¼èˆª ===")

    for episode in range(max_episodes):
        state, _ = env.reset()
        episode_reward = 0
        leader_reward_total = 0
        obstacle_detections = 0  # éšœç¢ç‰©æ£€æµ‹æ¬¡æ•°

        for step in range(max_steps):
            # æå–é¢†èˆªè€…çš„è§‚æµ‹ï¼ˆæ ¹æ®å¹³é¢æ¨¡å¼è°ƒæ•´ç»´åº¦ï¼‰
            leader_obs_dim = 2 + 2 + 4 + 2 + env.depth_feature_dim if env.enforce_planar else 3 + 3 + 4 + 3 + env.depth_feature_dim
            leader_obs = state[:leader_obs_dim]  # é¢†èˆªè€…è§‚æµ‹æ˜¯å‰Nç»´
            # æå–å¢å¼ºæ·±åº¦ç‰¹å¾ï¼ˆåŒ…å«é¿éšœå†³ç­–ä¿¡æ¯ï¼‰
            # åœ¨æ–°çš„è§‚æµ‹æ ¼å¼ä¸­ï¼Œæ·±åº¦ç‰¹å¾åœ¨13-31ä½ç½®ï¼ˆ19ç»´ï¼‰
            depth_features = leader_obs[13:32] if env.use_leader_camera else None
            
            # ä»ç¯å¢ƒè·å–é¢å¤–çš„é¿éšœå†³ç­–ä¿¡æ¯ç”¨äºç›‘æ§
            if hasattr(env, 'depth_obstacle_processor') and env.use_leader_camera:
                try:
                    # ä½¿ç”¨å±è”½åçš„æ·±åº¦å›¾åƒè¿›è¡Œé¿éšœæ£€æµ‹ï¼Œé¿å…æ— äººæœºè‡ªèº«è¢«è¯¯è®¤ä¸ºéšœç¢ç‰©
                    depth_image = env._get_masked_leader_depth()
                    if depth_image is not None and depth_image.size > 0:
                        # é‡è¦ï¼šå…ˆé¢„å¤„ç†æ·±åº¦å›¾åƒå†å¤„ç†
                        raw_depth = depth_image if len(depth_image.shape) == 2 else depth_image[:, :, 0]
                        processed_depth = env.depth_obstacle_processor.preprocess_depth_image(raw_depth)
                        obstacle_detected, min_depth = env.depth_obstacle_processor.detect_obstacles(processed_depth)
                        if obstacle_detected:
                            obstacle_detections += 1
                except:
                    pass

            # ç¬¬ä¸€é˜¶æ®µï¼šåªæœ‰é¢†èˆªè€…åŠ¨ä½œï¼ˆ3ç»´ï¼‰ï¼Œè·Ÿéšè€…ç”±ç¯å¢ƒå†…éƒ¨å¤„ç†
            leader_action = ppo_agent.select_action([leader_obs], depth_features)[0]
            combined_actions = leader_action  # åªå‘é€é¢†èˆªè€…çš„åŠ¨ä½œ

            next_state, reward, terminated, truncated, info = env.step(combined_actions)

            # åªè®¡ç®—é¢†èˆªè€…çš„å¥–åŠ± - é€‚åº”æ–°çš„4å¥–åŠ±ç³»ç»Ÿ
            reward_info = info.get('reward_info', {})
            leader_reward = reward  # æ€»å¥–åŠ±å³é¢†èˆªè€…å¥–åŠ±
            leader_reward_total += leader_reward

            # å­˜å‚¨é¢†èˆªè€…çš„å¥–åŠ±ï¼ˆselect_action å·²ç»å­˜å‚¨äº†å…¶ä»–ç»éªŒï¼‰
            ppo_agent.buffers[0].rewards.append(leader_reward)
            ppo_agent.buffers[0].is_terminals.append(terminated or truncated)

            state = next_state
            episode_reward += reward

            if terminated or truncated:
                break

        # æ›´æ–°é¢†èˆªè€…ç­–ç•¥
        if len(ppo_agent.buffers[0].rewards) > 0:
            ppo_agent.update()

        # è¾“å‡ºå¢å¼ºçš„è®­ç»ƒä¿¡æ¯
        print(f"é¢†èˆªè€…è®­ç»ƒå›åˆ {episode + 1}/{max_episodes} | å¥–åŠ±: {episode_reward:.2f} | é¢†èˆªè€…å¥–åŠ±: {leader_reward_total:.2f} | éšœç¢ç‰©æ£€æµ‹: {obstacle_detections}")
        
        # æ¯10ä¸ªå›åˆè¾“å‡ºè¯¦ç»†é¿éšœç»Ÿè®¡
        if (episode + 1) % 10 == 0:
            print(f"  - éšœç¢ç‰©æ£€æµ‹æ¬¡æ•°: {obstacle_detections}")

    # ä¿å­˜é¢†èˆªè€…æ¨¡å‹
    leader_model_path = "agent/model/CTDE_leader_enhanced.pth"
    ppo_agent.save(leader_model_path)
    print(f"å¢å¼ºé¢†èˆªè€…æ¨¡å‹å·²ä¿å­˜: {leader_model_path}")

    return leader_model_path

def train_followers_only(env, ppo_agent, leader_model_path, max_episodes=150, max_steps=500):
    """ç¬¬äºŒé˜¶æ®µï¼šå›ºå®šé¢†èˆªè€…ï¼Œè®­ç»ƒè·Ÿéšè€…ç¼–é˜Ÿè·Ÿè¸ª - é’ˆå¯¹å¤§ç¯å¢ƒä¼˜åŒ–"""
    print("=== ç¬¬äºŒé˜¶æ®µï¼šå›ºå®šé¢†èˆªè€…ï¼Œè®­ç»ƒè·Ÿéšè€…ç¼–é˜Ÿè·Ÿè¸ªï¼ˆå¤§ç¯å¢ƒä¼˜åŒ–ï¼‰ ===")

    # åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µï¼šå¯ç”¨ç¼–é˜Ÿå¥–åŠ±
    env.training_stage = 2
    print("ç¯å¢ƒå·²åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µï¼šå¯ç”¨è·Ÿéšè€…ç¼–é˜Ÿå¥–åŠ±")

    # åŠ è½½é¢†èˆªè€…æ¨¡å‹
    ppo_agent.load(leader_model_path)
    print(f"åŠ è½½é¢†èˆªè€…æ¨¡å‹: {leader_model_path}")

    # å†»ç»“é¢†èˆªè€…ç½‘ç»œå‚æ•°
    for param in ppo_agent.leader_policy.parameters():
        param.requires_grad = False
    for param in ppo_agent.leader_policy_old.parameters():
        param.requires_grad = False

    print("é¢†èˆªè€…ç½‘ç»œå·²å†»ç»“ï¼Œå¼€å§‹è®­ç»ƒè·Ÿéšè€…...")

    for episode in range(max_episodes):
        state, _ = env.reset()
        episode_reward = 0
        follower_reward_total = 0

        for step in range(max_steps):
            # æå–é¢†èˆªè€…çš„è§‚æµ‹ï¼ˆæ ¹æ®å¹³é¢æ¨¡å¼è°ƒæ•´ç»´åº¦ï¼‰
            leader_obs_dim = 2 + 2 + 4 + 2 + env.depth_feature_dim if env.enforce_planar else 3 + 3 + 4 + 3 + env.depth_feature_dim
            leader_obs = state[:leader_obs_dim]  # é¢†èˆªè€…è§‚æµ‹æ˜¯å‰Nç»´
            # æå–é¢†èˆªè€…çš„æ·±åº¦ç‰¹å¾ï¼ˆé¢†èˆªè€…è§‚æµ‹ä¸­çš„æ·±åº¦éƒ¨åˆ†ï¼Œç´¢å¼•13:32ï¼‰
            depth_features = leader_obs[13:32] if env.use_leader_camera else None

            # é¢†èˆªè€…å’Œè·Ÿéšè€…éƒ½åŠ¨ä½œ
            actions = ppo_agent.select_action([leader_obs] + [state[i*leader_obs_dim:(i+1)*leader_obs_dim] for i in range(1, env.num_drones)], depth_features)
            combined_actions = np.concatenate(actions)

            next_state, reward, terminated, truncated, info = env.step(combined_actions)

            # åˆ†ç¦»å¥–åŠ± - é€‚åº”æ–°çš„4å¥–åŠ±ç³»ç»Ÿ
            reward_info = info.get('reward_info', {})
            leader_reward = reward  # æ€»å¥–åŠ±ä½œä¸ºé¢†èˆªè€…å¥–åŠ±
            follower_reward = 0.0   # è·Ÿéšè€…å¥–åŠ±è®¾ä¸º0
            follower_reward_total += follower_reward

            # å­˜å‚¨æ‰€æœ‰æ™ºèƒ½ä½“çš„å¥–åŠ±ï¼ˆselect_action å·²ç»å­˜å‚¨äº†å…¶ä»–ç»éªŒï¼‰
            for i in range(env.num_drones):
                if i == 0:  # é¢†èˆªè€…
                    ppo_agent.buffers[i].rewards.append(leader_reward)
                else:  # è·Ÿéšè€…
                    ppo_agent.buffers[i].rewards.append(follower_reward)
                ppo_agent.buffers[i].is_terminals.append(terminated or truncated)

            state = next_state
            episode_reward += reward

            if terminated or truncated:
                break

        # åªæ›´æ–°è·Ÿéšè€…ç­–ç•¥
        if any(len(buffer.rewards) > 0 for buffer in ppo_agent.buffers[1:]):
            ppo_agent.update()

        print(f"è·Ÿéšè€…è®­ç»ƒå›åˆ {episode + 1}/{max_episodes} | å¥–åŠ±: {episode_reward:.2f} | è·Ÿéšè€…å¥–åŠ±: {follower_reward_total:.2f}")

    # ä¿å­˜å®Œæ•´æ¨¡å‹
    final_model_path = "agent/model/CTDE_full_formation.pth"
    ppo_agent.save(final_model_path)
    print(f"å®Œæ•´ç¼–é˜Ÿæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

    return final_model_path

def joint_fine_tuning(env, ppo_agent, model_path, max_episodes=50, max_steps=300):
    """ç¬¬ä¸‰é˜¶æ®µï¼šè”åˆå¾®è°ƒæ‰€æœ‰æ™ºèƒ½ä½“"""
    print("=== ç¬¬ä¸‰é˜¶æ®µï¼šè”åˆå¾®è°ƒæ‰€æœ‰æ™ºèƒ½ä½“ ===")

    # åŠ è½½æ¨¡å‹
    ppo_agent.load(model_path)

    # è§£å†»é¢†èˆªè€…ç½‘ç»œ
    for param in ppo_agent.leader_policy.parameters():
        param.requires_grad = True
    for param in ppo_agent.leader_policy_old.parameters():
        param.requires_grad = True

    print("å¼€å§‹è”åˆå¾®è°ƒ...")

    for episode in range(max_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for step in range(max_steps):
            # æå–é¢†èˆªè€…çš„è§‚æµ‹ï¼ˆæ ¹æ®å¹³é¢æ¨¡å¼è°ƒæ•´ç»´åº¦ï¼‰
            leader_obs_dim = 2 + 2 + 4 + 2 + env.depth_feature_dim if env.enforce_planar else 3 + 3 + 4 + 3 + env.depth_feature_dim
            leader_obs = state[:leader_obs_dim]  # é¢†èˆªè€…è§‚æµ‹æ˜¯å‰Nç»´
            # æå–é¢†èˆªè€…çš„æ·±åº¦ç‰¹å¾ï¼ˆé¢†èˆªè€…è§‚æµ‹ä¸­çš„æ·±åº¦éƒ¨åˆ†ï¼Œç´¢å¼•13:32ï¼‰
            depth_features = leader_obs[13:32] if env.use_leader_camera else None

            # æ‰€æœ‰æ™ºèƒ½ä½“åŠ¨ä½œ
            actions = ppo_agent.select_action([leader_obs] + [state[i*leader_obs_dim:(i+1)*leader_obs_dim] for i in range(1, env.num_drones)], depth_features)
            combined_actions = np.concatenate(actions)

            next_state, reward, terminated, truncated, info = env.step(combined_actions)

            # ä½¿ç”¨å®Œæ•´å¥–åŠ± - é€‚åº”æ–°çš„4å¥–åŠ±ç³»ç»Ÿ
            reward_info = info.get('reward_info', {})
            leader_reward = reward  # æ€»å¥–åŠ±ä½œä¸ºé¢†èˆªè€…å¥–åŠ±
            follower_reward = 0.0   # è·Ÿéšè€…å¥–åŠ±è®¾ä¸º0

            # å­˜å‚¨ç»éªŒï¼ˆselect_action å·²ç»å­˜å‚¨äº†å…¶ä»–ç»éªŒï¼‰
            for i in range(env.num_drones):
                if i == 0:
                    ppo_agent.buffers[i].rewards.append(leader_reward)
                else:
                    ppo_agent.buffers[i].rewards.append(follower_reward)
                ppo_agent.buffers[i].is_terminals.append(terminated or truncated)

            state = next_state
            episode_reward += reward

            if terminated or truncated:
                break

        # æ›´æ–°æ‰€æœ‰ç­–ç•¥
        ppo_agent.update()

        print(f"è”åˆå¾®è°ƒå›åˆ {episode + 1}/{max_episodes} | å¥–åŠ±: {episode_reward:.2f}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = "agent/model/CTDE_final_tuned.pth"
    ppo_agent.save(final_model_path)
    print(f"æœ€ç»ˆå¾®è°ƒæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

    return final_model_path

def main():
    print("============================================================================================")
    print("å¼€å§‹åˆ†é˜¶æ®µCTDEè®­ç»ƒï¼šé¢†èˆªè€…-è·Ÿéšè€…æ¶æ„ï¼ˆé›†æˆ4å¥–åŠ±ç³»ç»Ÿï¼‰")
    print("============================================================================================")

    # åˆ›å»ºç¯å¢ƒ - ç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼šä»…è®­ç»ƒé¢†èˆªè€…å¯¼èˆª
    env = DroneNavigationMulti(
        num_drones=5,
        use_depth_camera=True,
        depth_camera_range=10.0,
        depth_resolution=16,  # åŸºç¡€æ·±åº¦ç‰¹å¾ç»´åº¦
        enable_formation_force=False,  # ç¬¬ä¸€é˜¶æ®µç¦ç”¨ç¼–é˜ŸåŠ›ï¼Œè®©è·Ÿéšè€…æ‚¬åœ
        training_stage=1,  # ç¬¬ä¸€é˜¶æ®µï¼šå–æ¶ˆè·Ÿéšè€…ç¼–é˜Ÿå¥–åŠ±
        max_steps=5000  # è®¾ç½®æœ€å¤§æ­¥æ•°ï¼Œé¿å…è¿‡æ—©æˆªæ–­
    )
    
    # æ‰“å°ç¯å¢ƒä¿¡æ¯
    print(f"ç¯å¢ƒé…ç½®:")
    print(f"  - æ— äººæœºæ•°é‡: {env.num_drones}")
    print(f"  - è§‚æµ‹ç©ºé—´: {env.observation_space.shape}")
    print(f"  - åŠ¨ä½œç©ºé—´: {env.action_space.shape}")
    print(f"  - æ·±åº¦ç‰¹å¾ç»´åº¦: {env.depth_feature_dim}")
    print(f"  - å¢å¼ºæ·±åº¦ç‰¹å¾: è¿ç»­æ·±åº¦é¿éšœ")

    # åˆ›å»ºCTDEä»£ç† - æ”¯æŒå¢å¼ºæ·±åº¦ç‰¹å¾ï¼Œæ ¹æ®å¹³é¢æ¨¡å¼è°ƒæ•´çŠ¶æ€ç»´åº¦
    # ä»é…ç½®æ–‡ä»¶è·å–æ·±åº¦ç‰¹å¾ç»´åº¦
    from drone_envs.config import multi_drone_env
    leader_visual_dim = multi_drone_env.get("depth_feature_dim", 130)
    
    # æ ¹æ®å¹³é¢æ¨¡å¼è®¡ç®—çŠ¶æ€ç»´åº¦
    # å¹³é¢æ¨¡å¼: ä½ç½®(2) + é€Ÿåº¦(2) + æœå‘(4) + ç›®æ ‡ç›¸å¯¹ä½ç½®(2) + æ·±åº¦ç‰¹å¾
    # 3Dæ¨¡å¼: ä½ç½®(3) + é€Ÿåº¦(3) + æœå‘(4) + ç›®æ ‡ç›¸å¯¹ä½ç½®(3) + æ·±åº¦ç‰¹å¾
    base_state_dim = 2 + 2 + 4 + 2 + env.depth_feature_dim if env.enforce_planar else 3 + 3 + 4 + 3 + env.depth_feature_dim
    
    ppo_agent = CTDE_PPO(
        leader_state_dim=base_state_dim,
        follower_state_dim=base_state_dim,
        leader_visual_dim=leader_visual_dim,  # CNNæ·±åº¦ç‰¹å¾ç»´åº¦ï¼ˆä»é…ç½®æ–‡ä»¶è·å–ï¼‰
        action_dim=2,  # æ¯ä¸ªæ— äººæœºçš„åŠ¨ä½œç»´åº¦ï¼š2 [thrust, torque] for body-frame control with camera
        num_drones=5,
        lr_actor=ppo_config['lr_actor'],
        lr_critic=ppo_config['lr_critic'],
        gamma=ppo_config['gamma'],
        K_epochs=ppo_config['K_epochs'],
        eps_clip=ppo_config['eps_clip'],
        has_continuous_action_space=ppo_config['has_continuous_action_space'],
        action_std_init=ppo_config['action_std_init']
    )

    print("CTDEä»£ç†å·²åˆ›å»ºï¼Œæ”¯æŒå¢å¼ºæ·±åº¦é¿éšœç‰¹å¾")
    print("============================================================================================")

    # ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒé¢†èˆªè€…
    leader_model_path = train_leader_only(env, ppo_agent, max_episodes=500, max_steps=5000)

    # ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒè·Ÿéšè€…
    formation_model_path = train_followers_only(env, ppo_agent, leader_model_path, max_episodes=500, max_steps=5000)

    # ç¬¬ä¸‰é˜¶æ®µï¼šè”åˆå¾®è°ƒ
    final_model_path = joint_fine_tuning(env, ppo_agent, formation_model_path, max_episodes=30, max_steps=300)

    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    log = {
        "training_phases": ["leader_enhanced", "followers_only", "joint_fine_tuning"],
        "models": {
            "leader_enhanced": leader_model_path,
            "formation": formation_model_path,
            "final": final_model_path
        },
        "enhancements": {
            "reward_system": "4-reward_minimal",
            "depth_obstacle_avoidance": True,
            "continuous_depth_rewards": True,
            "planar_action_mapping": True,
            "reward_components": ["success", "crash", "progress", "obstacle"]
        },
        "completion_time": str(datetime.now().replace(microsecond=0))
    }
    save_log_to_json(log, "DroneNavigationMultiFormation-v0", "staged_enhanced_training")

    env.close()

    print("============================================================================================")
    print("åˆ†é˜¶æ®µCTDEè®­ç»ƒå®Œæˆï¼ï¼ˆé›†æˆ4å¥–åŠ±ç³»ç»Ÿï¼‰")
    print(f"å¢å¼ºé¢†èˆªè€…æ¨¡å‹: {leader_model_path}")
    print(f"ç¼–é˜Ÿæ¨¡å‹: {formation_model_path}")
    print(f"æœ€ç»ˆæ¨¡å‹: {final_model_path}")
    print("å¢å¼ºåŠŸèƒ½: 4å¥–åŠ±ç³»ç»Ÿã€è¿ç»­æ·±åº¦é¿éšœã€å¹³é¢åŠ¨ä½œæ˜ å°„")
    print("============================================================================================")

if __name__ == '__main__':
    main()
