"""
ç¬¬ä¸€é˜¶æ®µä¸“ç”¨è®­ç»ƒè„šæœ¬ï¼šé¢†èˆªè€…é¿éšœå’Œå¯¼èˆªè®­ç»ƒ
åŒ…å«å®æ—¶å¥–åŠ±å›¾è¡¨ç»˜åˆ¶åŠŸèƒ½

è·¯å¾„é…ç½®è¯´æ˜:
- æ‰€æœ‰ä¿å­˜è·¯å¾„éƒ½é€šè¿‡ PathConfig ç±»é›†ä¸­ç®¡ç†
- åŸºç¡€ç›®å½•: agent/log/ (æ—¥å¿—), agent/model/ (æ¨¡å‹)
- ä¸»è¦è·¯å¾„:
  * TRAINING_PROGRESS_PLOT: è®­ç»ƒè¿›åº¦å›¾ (training_progress.png)
  * TRAINING_DATA_JSON: è®­ç»ƒæ•°æ® (training_data.json)
  * TRAJECTORIES_JSON: è½¨è¿¹æ•°æ® (trajectories.json)
  * FINAL_MODEL: æœ€ç»ˆæ¨¡å‹ (leader_phase1_final.pth)
  * FINAL_PROGRESS_PLOT: æœ€ç»ˆè¿›åº¦å›¾ (leader_phase1_final_progress.png)
  * FINAL_DATA_JSON: æœ€ç»ˆæ•°æ® (leader_phase1_final_data.json)
  * FINAL_TRAJECTORIES_JSON: æœ€ç»ˆè½¨è¿¹ (leader_phase1_final_trajectories.json)

è½¨è¿¹ä¿å­˜ç­–ç•¥:
- å®šæœŸä¿å­˜: æ¯ plot_interval ä¸ªå›åˆè‡ªåŠ¨ä¿å­˜æœ€è¿‘çš„è½¨è¿¹æ•°æ®
- å†…å­˜ç®¡ç†: ä¿å­˜åè‡ªåŠ¨æ¸…ç†æ—§è½¨è¿¹ï¼Œåªä¿ç•™æœ€è¿‘çš„ save_interval ä¸ªè½¨è¿¹
- æœ€ç»ˆä¿å­˜: è®­ç»ƒç»“æŸæ—¶ä¿å­˜å®Œæ•´çš„è½¨è¿¹æ•°æ®åˆ°æœ€ç»ˆæ–‡ä»¶
- é˜²æ­¢å†…å­˜æº¢å‡º: é¿å…ä¸€æ¬¡æ€§ä¿å­˜å¤§é‡è½¨è¿¹æ•°æ®å¯¼è‡´çš„é—®é¢˜
"""
import sys
import os
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import datetime
from collections import deque
import time
import pybullet as p

from agent.CTDE_PPO_agent import CTDE_PPO, device
from drone_envs.envs.drone_env_multi import DroneNavigationMulti

class PathConfig:
    """è·¯å¾„é…ç½®ç±» - é›†ä¸­ç®¡ç†æ‰€æœ‰ä¿å­˜è·¯å¾„"""
    
    # åŸºç¡€ç›®å½•
    BASE_DIR = Path(__file__).parent
    AGENT_DIR = BASE_DIR / "agent"
    LOG_DIR = AGENT_DIR / "log"
    MODEL_DIR = AGENT_DIR / "model"
    
    # è®­ç»ƒè¿›åº¦ç›¸å…³è·¯å¾„
    TRAINING_PROGRESS_PLOT = LOG_DIR / "training_progress.png"
    TRAINING_DATA_JSON = LOG_DIR / "training_data.json"
    TRAJECTORIES_JSON = LOG_DIR / "trajectories.json"
    
    # æœ€ç»ˆç»“æœè·¯å¾„
    FINAL_MODEL = MODEL_DIR / "leader_phase1_final.pth"
    FINAL_PROGRESS_PLOT = LOG_DIR / "leader_phase1_final_progress.png"
    FINAL_DATA_JSON = LOG_DIR / "leader_phase1_final_data.json"
    FINAL_TRAJECTORIES_JSON = LOG_DIR / "leader_phase1_final_trajectories.json"
    
    @classmethod
    def ensure_directories(cls):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        cls.LOG_DIR.mkdir(parents=True, exist_ok=True)
        cls.MODEL_DIR.mkdir(parents=True, exist_ok=True)
    
    @classmethod
    def get_episode_model_path(cls, episode_num):
        """è·å–æŒ‡å®šå›åˆçš„æ¨¡å‹ä¿å­˜è·¯å¾„"""
        return cls.MODEL_DIR / f"leader_phase1_episode_{episode_num}.pth"
    
    @classmethod
    def get_timestamped_path(cls, base_name, extension="json"):
        """è·å–å¸¦æ—¶é—´æˆ³çš„è·¯å¾„"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return cls.LOG_DIR / f"{base_name}_{timestamp}.{extension}"

class RewardTracker:
    """å¥–åŠ±è·Ÿè¸ªå’Œå¯è§†åŒ–ç±»"""
    def __init__(self, window_size=100):
        self.episode_rewards = []
        self.episode_lengths = []
        self.moving_avg_rewards = []
        self.success_rate = []
        self.collision_rate = []
        self.window_size = window_size
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # è®¾ç½®å›¾è¡¨å†…å­˜è­¦å‘Šé˜ˆå€¼ï¼Œé¿å…è­¦å‘Šå¹²æ‰°
        plt.rcParams['figure.max_open_warning'] = 50
        
    def add_episode(self, episode_reward, episode_length, success, collision):
        """æ·»åŠ æ–°å›åˆæ•°æ®"""
        self.episode_rewards.append(episode_reward)
        self.episode_lengths.append(episode_length)
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        if len(self.episode_rewards) >= self.window_size:
            moving_avg = np.mean(self.episode_rewards[-self.window_size:])
        else:
            moving_avg = np.mean(self.episode_rewards)
        self.moving_avg_rewards.append(moving_avg)
        
        # å­˜å‚¨æˆåŠŸæ ‡å¿—
        if not hasattr(self, 'success_flags'):
            self.success_flags = []
        self.success_flags.append(success)
        
        # è®¡ç®—æˆåŠŸç‡ - ä½¿ç”¨å­˜å‚¨çš„successæ ‡å¿—
        recent_episodes = min(len(self.success_flags), self.window_size)
        recent_successes = sum(self.success_flags[-recent_episodes:])
        self.success_rate.append(recent_successes / recent_episodes)
        
        # ç¢°æ’ç‡
        self.collision_rate.append(1.0 if collision else 0.0)
        
    def plot_training_progress(self, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾"""
        if save_path is None:
            save_path = PathConfig.TRAINING_PROGRESS_PLOT
        
        # å…³é—­ä¹‹å‰çš„å›¾è¡¨ï¼Œé¿å…å†…å­˜æ³„æ¼
        plt.close('all')
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        episodes = range(1, len(self.episode_rewards) + 1)
        
        # 1. å¥–åŠ±æ›²çº¿
        ax1.plot(episodes, self.episode_rewards, alpha=0.3, color='blue', label='åŸå§‹å¥–åŠ±')
        ax1.plot(episodes, self.moving_avg_rewards, color='red', linewidth=2, label=f'{self.window_size}å›åˆç§»åŠ¨å¹³å‡')
        ax1.set_xlabel('å›åˆæ•°')
        ax1.set_ylabel('å¥–åŠ±')
        ax1.set_title('é¢†èˆªè€…è®­ç»ƒå¥–åŠ±æ›²çº¿')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å›åˆé•¿åº¦
        ax2.plot(episodes, self.episode_lengths, color='green', alpha=0.7)
        ax2.set_xlabel('å›åˆæ•°')
        ax2.set_ylabel('å›åˆé•¿åº¦ï¼ˆæ­¥æ•°ï¼‰')
        ax2.set_title('å›åˆé•¿åº¦å˜åŒ–')
        ax2.grid(True, alpha=0.3)
        
        # 3. ç¢°æ’ç‡
        ax3.plot(episodes, self.collision_rate, color='red', alpha=0.7)
        ax3.set_xlabel('å›åˆæ•°')
        ax3.set_ylabel('ç¢°æ’ç‡')
        ax3.set_title('ç¢°æ’ç‡å˜åŒ–')
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1)
        
        # 4. æˆåŠŸç‡
        ax4.plot(episodes, self.success_rate, color='purple', alpha=0.7)
        ax4.set_xlabel('å›åˆæ•°')
        ax4.set_ylabel('æˆåŠŸç‡')
        ax4.set_title(f'å¯¼èˆªæˆåŠŸç‡ ({self.window_size}å›åˆæ»‘åŠ¨çª—å£)')
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"è®­ç»ƒè¿›åº¦å›¾å·²ä¿å­˜: {save_path}")
        
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œå…³é—­å›¾è¡¨ï¼Œè®©å›¾è¡¨å¯ä»¥æ˜¾ç¤º
        # ä¸‹æ¬¡è°ƒç”¨ plot_training_progress æ—¶ä¼šé€šè¿‡ plt.close('all') è‡ªåŠ¨å…³é—­
        return fig
    
    def save_data(self, save_path=None):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        if save_path is None:
            save_path = PathConfig.TRAINING_DATA_JSON
        # å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ä»¥æ”¯æŒJSONåºåˆ—åŒ–
        data = {
            'episode_rewards': [float(x) for x in self.episode_rewards],
            'episode_lengths': [int(x) for x in self.episode_lengths],
            'moving_avg_rewards': [float(x) for x in self.moving_avg_rewards],
            'success_rate': [float(x) for x in self.success_rate],
            'success_flags': [bool(x) for x in getattr(self, 'success_flags', [])],
            'collision_rate': [float(x) for x in self.collision_rate],
            'total_episodes': len(self.episode_rewards),
            'final_avg_reward': float(self.moving_avg_rewards[-1] if self.moving_avg_rewards else 0),
            'final_success_rate': float(self.success_rate[-1] if self.success_rate else 0),
            'timestamp': str(datetime.now())
        }
        
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜: {save_path}")

class TrajectoryTracker:
    """è½¨è¿¹è·Ÿè¸ªå’Œè®°å½•ç±» - æ”¯æŒå®šæœŸä¿å­˜ä»¥é¿å…å†…å­˜é—®é¢˜"""
    
    def __init__(self, save_interval=100, auto_save=True):
        self.trajectories = []  # å­˜å‚¨æ‰€æœ‰å›åˆçš„è½¨è¿¹æ•°æ®
        self.save_interval = save_interval  # æ¯å¤šå°‘ä¸ªå›åˆä¿å­˜ä¸€æ¬¡
        self.auto_save = auto_save  # æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¿å­˜
        self.last_save_episode = 0  # ä¸Šæ¬¡ä¿å­˜çš„å›åˆæ•°
        
    def start_episode(self, episode_id, goal_position, start_position, num_drones):
        """å¼€å§‹æ–°å›åˆçš„è½¨è¿¹è®°å½•"""
        episode_data = {
            'episode_id': episode_id,
            'goal_position': goal_position.tolist() if hasattr(goal_position, 'tolist') else goal_position,
            'start_position': start_position.tolist() if hasattr(start_position, 'tolist') else start_position,
            'num_drones': num_drones,
            'drone_trajectories': [[] for _ in range(num_drones)],  # æ¯ä¸ªæ— äººæœºçš„è½¨è¿¹ç‚¹
            'timestamps': [],  # æ—¶é—´æˆ³
            'rewards': [],     # æ¯æ­¥å¥–åŠ±
            'actions': [],     # æ¯æ­¥åŠ¨ä½œ
            'success': False,
            'collision': False,
            'termination_reason': 'unknown',
            'total_steps': 0,
            'total_reward': 0.0,
            'environment_info': {}  # ç¯å¢ƒç›¸å…³ä¿¡æ¯
        }
        return episode_data
    
    def record_step(self, episode_data, drone_positions, timestamp, reward, action):
        """è®°å½•æ¯ä¸€æ­¥çš„è½¨è¿¹ä¿¡æ¯ - åªè®°å½•äºŒç»´å¹³é¢åæ ‡"""
        episode_data['timestamps'].append(timestamp)
        episode_data['rewards'].append(float(reward))
        episode_data['actions'].append(action.tolist() if hasattr(action, 'tolist') else action)
        
        # åªè®°å½•äºŒç»´å¹³é¢åæ ‡ (x, y)ï¼Œå¿½ç•¥zåæ ‡
        for i, pos in enumerate(drone_positions):
            if i < len(episode_data['drone_trajectories']):
                # åªä¿å­˜x, yåæ ‡ï¼Œé€‚ç”¨äºå¹³é¢æ¨¡å¼
                plane_pos = [float(pos[0]), float(pos[1])]  # x, yåæ ‡
                episode_data['drone_trajectories'][i].append(plane_pos)
    
    def end_episode(self, episode_data, success, collision, termination_reason, total_reward, total_steps, environment_info=None):
        """ç»“æŸå›åˆè®°å½•"""
        episode_data['success'] = success
        episode_data['collision'] = collision
        episode_data['termination_reason'] = termination_reason
        episode_data['total_reward'] = float(total_reward)
        episode_data['total_steps'] = total_steps
        
        if environment_info:
            episode_data['environment_info'] = environment_info
            
        # ç¡®ä¿æ‰€æœ‰è½¨è¿¹é•¿åº¦ä¸€è‡´
        min_length = min(len(traj) for traj in episode_data['drone_trajectories']) if episode_data['drone_trajectories'] else 0
        for i in range(len(episode_data['drone_trajectories'])):
            episode_data['drone_trajectories'][i] = episode_data['drone_trajectories'][i][:min_length]
        
        # æˆªæ–­å…¶ä»–åˆ—è¡¨ä»¥ä¿æŒä¸€è‡´æ€§
        episode_data['timestamps'] = episode_data['timestamps'][:min_length]
        episode_data['rewards'] = episode_data['rewards'][:min_length]
        episode_data['actions'] = episode_data['actions'][:min_length]
        
        self.trajectories.append(episode_data)
        return episode_data
    
    def periodic_save(self, current_episode, save_path=None):
        """å®šæœŸä¿å­˜è½¨è¿¹æ•°æ® - åªä¿å­˜æœ€è¿‘çš„è½¨è¿¹ï¼Œé¿å…å†…å­˜ç´¯ç§¯"""
        if not self.auto_save:
            return
            
        if current_episode - self.last_save_episode >= self.save_interval:
            try:
                # åˆ›å»ºä¸´æ—¶ä¿å­˜è·¯å¾„
                if save_path is None:
                    base_path = PathConfig.TRAJECTORIES_JSON
                    temp_path = base_path.parent / f"{base_path.stem}_ep{current_episode}{base_path.suffix}"
                else:
                    temp_path = Path(save_path)
                
                # åªä¿å­˜æœ€è¿‘çš„è½¨è¿¹æ•°æ®ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
                recent_trajectories = self.trajectories[-self.save_interval:] if len(self.trajectories) > self.save_interval else self.trajectories
                
                self._save_trajectories_to_file(recent_trajectories, temp_path)
                print(f"å®šæœŸè½¨è¿¹ä¿å­˜: {temp_path} (æœ€è¿‘{len(recent_trajectories)}ä¸ªå›åˆ)")
                
                # æ¸…ç†å†…å­˜ï¼šåªä¿ç•™æœ€è¿‘çš„è½¨è¿¹æ•°æ®
                if len(self.trajectories) > self.save_interval:
                    # ä¿ç•™æœ€è¿‘çš„save_intervalä¸ªè½¨è¿¹ï¼Œç”¨äºä¸‹æ¬¡ä¿å­˜
                    self.trajectories = self.trajectories[-self.save_interval:]
                    print(f"å†…å­˜æ¸…ç†ï¼šä¿ç•™æœ€è¿‘{len(self.trajectories)}ä¸ªè½¨è¿¹")
                
                self.last_save_episode = current_episode
                
            except Exception as e:
                print(f"å®šæœŸè½¨è¿¹ä¿å­˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    
    def save_trajectories(self, save_path=None):
        """ä¿å­˜æ‰€æœ‰è½¨è¿¹æ•°æ®åˆ°æ–‡ä»¶"""
        if save_path is None:
            save_path = PathConfig.TRAJECTORIES_JSON
        
        return self._save_trajectories_to_file(self.trajectories, save_path)
    
    def _save_trajectories_to_file(self, trajectories_data, save_path):
        """å†…éƒ¨æ–¹æ³•ï¼šå°†è½¨è¿¹æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶"""
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        # è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†numpyç±»å‹
        def numpy_encoder(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (list, tuple)):
                return [numpy_encoder(item) for item in obj]
            elif isinstance(obj, dict):
                return {key: numpy_encoder(value) for key, value in obj.items()}
            else:
                return obj
        
        # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼ï¼Œå¹¶å¤„ç†numpyç±»å‹
        serializable_data = {
            'total_episodes': len(trajectories_data),
            'trajectories': numpy_encoder(trajectories_data),
            'summary': {
                'successful_episodes': sum(1 for t in trajectories_data if t['success']),
                'collision_episodes': sum(1 for t in trajectories_data if t['collision']),
                'average_reward': float(np.mean([t['total_reward'] for t in trajectories_data])) if trajectories_data else 0.0,
                'average_steps': float(np.mean([t['total_steps'] for t in trajectories_data])) if trajectories_data else 0.0,
                'timestamp': str(datetime.now())
            }
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, indent=2, ensure_ascii=False)
        
        print(f"è½¨è¿¹æ•°æ®å·²ä¿å­˜: {save_path} (å…±{len(trajectories_data)}ä¸ªå›åˆ)")
        
        return save_path

def train_leader_phase1(max_episodes=20000, max_steps=3000, plot_interval=1000):
    """ç¬¬ä¸€é˜¶æ®µï¼šä¸“é—¨è®­ç»ƒé¢†èˆªè€…é¿éšœå’Œå¯¼èˆª
    
    Args:
        max_episodes: æœ€å¤§è®­ç»ƒå›åˆæ•°ï¼Œ20000å›åˆç”¨äºå……åˆ†å­¦ä¹ 
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼Œ3000æ­¥ï¼Œç»™æ— äººæœºå……è¶³æ—¶é—´åˆ°è¾¾ç›®æ ‡
        plot_interval: ç»˜å›¾é—´éš”ï¼Œ1000å›åˆä¿å­˜ä¸€æ¬¡
    """
    print("="*80)
    print("å¼€å§‹ç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼šé¢†èˆªè€…é¿éšœå’Œå¯¼èˆª")
    print("="*80)
    
    # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
    PathConfig.ensure_directories()
    
    # åˆ›å»ºç¯å¢ƒ
    env = DroneNavigationMulti(
        num_drones=1,  # ä¿®æ”¹ä¸ºä»…1æ¶é¢†èˆªè€…
        use_depth_camera=True,
        depth_camera_range=10.0,
        depth_resolution=16,
        enable_formation_force=False,  # å…³é—­ç¼–é˜ŸåŠ›
        training_stage=1,  # ç¬¬ä¸€é˜¶æ®µ
        max_steps=max_steps  # ä¼ é€’æœ€å¤§æ­¥æ•°å‚æ•°
    )
    
    print(f"ç¯å¢ƒé…ç½®:")
    print(f"  - æ— äººæœºæ•°é‡: {env.num_drones}")
    print(f"  - è§‚æµ‹ç©ºé—´: {env.observation_space.shape}")
    print(f"  - æ·±åº¦ç‰¹å¾ç»´åº¦: {env.depth_feature_dim}")
    print(f"  - è®­ç»ƒé˜¶æ®µ: {env.training_stage}")
    print(f"  - ç¼–é˜ŸåŠ›çŠ¶æ€: {'ç¦ç”¨' if not env.enable_formation_force else 'å¯ç”¨'}")
    print(f"  - å¹³é¢æ¨¡å¼: {'å¯ç”¨' if env.enforce_planar else 'ç¦ç”¨'}")
    
    # åˆ›å»ºCTDEä»£ç† - æ ¹æ®å¹³é¢æ¨¡å¼è°ƒæ•´çŠ¶æ€ç»´åº¦
    # å¹³é¢æ¨¡å¼ï¼šä½ç½®(2) + é€Ÿåº¦(2) + æœå‘(4) + ç›®æ ‡ç›¸å¯¹ä½ç½®(2) + æ·±åº¦ç‰¹å¾(130) = 140ç»´
    leader_state_dim = 2 + 2 + 4 + 2 + env.depth_feature_dim if env.enforce_planar else 3 + 3 + 4 + 3 + env.depth_feature_dim
    # ä½ç½®(2/3) + é€Ÿåº¦(2/3) + æœå‘(4) + ç›®æ ‡ç›¸å¯¹ä½ç½®(2/3) + æ·±åº¦ç‰¹å¾(130)
    
    ppo_agent = CTDE_PPO(
        leader_state_dim=leader_state_dim,
        follower_state_dim=leader_state_dim,  # è·Ÿéšè€…çŠ¶æ€ç»´åº¦ä¸é¢†èˆªè€…ç›¸åŒ
        leader_visual_dim=env.depth_feature_dim,
        action_dim=2,  # å‰è¿›/åé€€åŠ›å’Œè½¬å‘æ‰­çŸ©
        num_drones=1,  # ç¬¬ä¸€é˜¶æ®µåªè®­ç»ƒé¢†èˆªè€…
        lr_actor=0.0003,   # ğŸ”§ ä»0.0005é™ä½åˆ°0.0003ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œé˜²æ­¢è¦†ç›–æˆåŠŸç­–ç•¥
        lr_critic=0.001,   # Criticå­¦ä¹ ç‡
        gamma=0.99,        # æŠ˜æ‰£å› å­
        K_epochs=40,       # ğŸ¯ PPOæ›´æ–°è½®æ¬¡ï¼Œä»20æ¢å¤åˆ°40ä»¥å……åˆ†å­¦ä¹ 
        eps_clip=0.2,      # PPOè£å‰ªå‚æ•°
        has_continuous_action_space=True,
        action_std_init=0.3  # ğŸ¯ 20000å›åˆä¼˜åŒ–: ä»0.1å¢åŠ åˆ°0.3ï¼Œä¿æŒé•¿æœŸæ¢ç´¢èƒ½åŠ›
    )
    
    print("CTDEä»£ç†å·²åˆ›å»º")
    print("="*80)
    
    # åˆå§‹åŒ–å¥–åŠ±è·Ÿè¸ªå™¨å’Œè½¨è¿¹è·Ÿè¸ªå™¨
    reward_tracker = RewardTracker(window_size=50)
    trajectory_tracker = TrajectoryTracker(save_interval=plot_interval, auto_save=True)  # å®šæœŸä¿å­˜é—´éš”ä¸ç»˜å›¾é—´éš”ç›¸åŒ
    
    # è®­ç»ƒå¾ªç¯
    start_time = time.time()
    
    for episode in range(max_episodes):
        # ç¡®ä¿bufferåœ¨episodeå¼€å§‹æ—¶æ˜¯ç©ºçš„
        for buffer in ppo_agent.buffers:
            buffer.clear()
            
        state, _ = env.reset()
        
        # åˆå§‹åŒ–è½¨è¿¹è®°å½•
        leader_start_pos = np.array(env.start_position)
        goal_pos = np.array(env.goal)
        episode_trajectory = trajectory_tracker.start_episode(
            episode_id=episode + 1,
            goal_position=goal_pos,
            start_position=leader_start_pos,
            num_drones=env.num_drones
        )
        
        episode_reward = 0
        leader_reward_total = 0
        obstacle_detections = 0
        collision_occurred = False
        collision_type = ""  # ç¢°æ’ç±»å‹
        
        for step in range(max_steps):
            # æå–é¢†èˆªè€…è§‚æµ‹ - æ ¹æ®å¹³é¢æ¨¡å¼è°ƒæ•´æ·±åº¦ç‰¹å¾ä½ç½®
            leader_obs_dim = 2 + 2 + 4 + 2 + env.depth_feature_dim if env.enforce_planar else 3 + 3 + 4 + 3 + env.depth_feature_dim
            leader_obs = state[:leader_obs_dim]
            # åœ¨å¹³é¢æ¨¡å¼ä¸‹ï¼Œæ·±åº¦ç‰¹å¾ä»ç´¢å¼•10å¼€å§‹ï¼›åœ¨3Dæ¨¡å¼ä¸‹ä»ç´¢å¼•13å¼€å§‹
            depth_start_idx = 2 + 2 + 4 + 2 if env.enforce_planar else 3 + 3 + 4 + 3
            depth_features = leader_obs[depth_start_idx:depth_start_idx + env.depth_feature_dim] if env.use_leader_camera else None
            
            # ç›‘æ§é¿éšœä¿¡æ¯
            if hasattr(env, 'depth_obstacle_processor') and env.use_leader_camera:
                try:
                    # ä½¿ç”¨å±è”½åçš„æ·±åº¦å›¾åƒè¿›è¡Œé¿éšœæ£€æµ‹ï¼Œé¿å…æ— äººæœºè‡ªèº«è¢«è¯¯è®¤ä¸ºéšœç¢ç‰©
                    depth_image = env._get_masked_leader_depth()
                    if depth_image is not None and depth_image.size > 0:
                        raw_depth = depth_image if len(depth_image.shape) == 2 else depth_image[:, :, 0]
                        processed_depth = env.depth_obstacle_processor.preprocess_depth_image(raw_depth)
                        obstacle_detected, min_depth = env.depth_obstacle_processor.detect_obstacles(processed_depth)
                        
                        if obstacle_detected:
                            obstacle_detections += 1
                        
                        # ç§»é™¤æ·±åº¦ç¢°æ’æ£€æµ‹ï¼Œé¿å…ä¸ç¯å¢ƒç¢°æ’æ£€æµ‹å†²çª
                        # ç¢°æ’æ£€æµ‹ç”±ç¯å¢ƒç»Ÿä¸€å¤„ç†ï¼Œè¿™é‡Œåªç”¨äºç»Ÿè®¡é¿éšœä¿¡æ¯
                            
                except Exception as e:
                    pass
            
            # ç¬¬ä¸€é˜¶æ®µï¼šåªæ§åˆ¶é¢†èˆªè€…
            leader_action = ppo_agent.select_action([leader_obs], depth_features)[0]
            
            # ç¯å¢ƒæ­¥è¿› - ç¬¬ä¸€é˜¶æ®µç›´æ¥ä½¿ç”¨é¢†èˆªè€…åŠ¨ä½œ
            next_state, reward, terminated, truncated, info = env.step(leader_action)
            
            # è®°å½•è½¨è¿¹ä¿¡æ¯
            current_time = time.time()
            # è·å–å½“å‰æ‰€æœ‰æ— äººæœºçš„ä½ç½®
            drone_positions = []
            for i in range(env.num_drones):
                pos, _ = p.getBasePositionAndOrientation(env.drones[i].drone, env.client)
                drone_positions.append(np.array(pos))
            
            trajectory_tracker.record_step(
                episode_trajectory, 
                drone_positions, 
                current_time, 
                reward, 
                leader_action
            )
            
            # è®°å½•å¥–åŠ±å’Œç¢°æ’ä¿¡æ¯ - ä½¿ç”¨ç¯å¢ƒè¿”å›çš„å¥–åŠ±ä¿¡æ¯
            reward_info = info.get('reward_info', {})
            episode_reward += reward  # ä½¿ç”¨ç¯å¢ƒè¿”å›çš„æ€»å¥–åŠ±
            
            # ğŸ¯ å¥–åŠ±ç›‘æ§ï¼šä¿®å¤åsuccess=3000æ˜¯æ­£å¸¸å€¼
            # å•æ­¥å¥–åŠ±èŒƒå›´: æˆåŠŸæ­¥ ~3010 (3000+å¯†é›†), æ™®é€šæ­¥ ~10, å¤±è´¥æ­¥ ~-100
            if abs(reward) > 3500:  # ğŸ”§ é˜ˆå€¼ä»1000æå‡åˆ°3500ï¼ˆsuccess=3000 + ä½™é‡ï¼‰
                # åªåœ¨å¼‚å¸¸é«˜çš„å¥–åŠ±æ—¶è­¦å‘Šï¼ˆç†è®ºæœ€å¤§å€¼åº”è¯¥ä¸è¶…è¿‡3100ï¼‰
                print(f"âš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸å¥–åŠ±å€¼ {reward:.2f} åœ¨æ­¥æ•° {step + 1}")
                print(f"  å¥–åŠ±è¯¦æƒ…: {reward_info}")
            elif reward > 3000:  # æˆåŠŸå¥–åŠ±ï¼Œè®°å½•ä½†ä¸è­¦å‘Š
                print(f"ğŸ‰ æˆåŠŸï¼å¥–åŠ± {reward:.2f} åœ¨æ­¥æ•° {step + 1}")
                # ç§»é™¤æˆªæ–­ï¼šç¯å¢ƒä¸­çš„reward_calculatorå·²ç»æ­£ç¡®å¤„ç†å¥–åŠ±èŒƒå›´
            
            # ä»ç¯å¢ƒè·å–ç¢°æ’ä¿¡æ¯ - åªåœ¨çœŸæ­£ç»ˆæ­¢ï¼ˆä¸æ˜¯æˆªæ–­ï¼‰æ—¶è®°å½•ç¢°æ’
            crash_reward = reward_info.get('crash', 0)
            if crash_reward < 0 and terminated:  # åªæœ‰çœŸæ­£ç¢°æ’ç»ˆæ­¢æ—¶æ‰è®°å½•ç¢°æ’
                collision_occurred = True
                # ä»æ·±åº¦ä¿¡æ¯ä¸­è·å–ç¢°æ’ç±»å‹ï¼ˆå¦‚æœå¯è·å–ï¼‰
                if hasattr(env, '_get_depth_info'):
                    try:
                        depth_info = env._get_depth_info()
                        collision_type = depth_info.get('collision_type', 'unknown')
                        if collision_type == 'physical_contact':
                            collision_type = f"ç‰©ç†ç¢°æ’({depth_info.get('contact_points', 0)}ç‚¹)"
                        elif collision_type == 'boundary':
                            collision_type = "è¾¹ç•Œç¢°æ’"
                        else:
                            collision_type = "æ·±åº¦ç¢°æ’"
                    except:
                        collision_type = "ç¢°æ’"
            
            # å­˜å‚¨ç»éªŒ
            ppo_agent.buffers[0].rewards.append(reward)
            ppo_agent.buffers[0].is_terminals.append(terminated or truncated)
            
            state = next_state
            
            if terminated or truncated:
                break
        
        # æ›´æ–°ç­–ç•¥
        if len(ppo_agent.buffers[0].rewards) > 0:
            try:
                ppo_agent.update()
            except Exception as e:
                print(f"ç­–ç•¥æ›´æ–°å¤±è´¥: {e}")
                # å³ä½¿æ›´æ–°å¤±è´¥ä¹Ÿè¦æ¸…ç©ºbufferï¼Œé¿å…å¥–åŠ±ç´¯ç§¯
                for buffer in ppo_agent.buffers:
                    buffer.clear()
                continue
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        success = info.get('success', False)  # ä»ç¯å¢ƒè·å–çœŸæ­£çš„æˆåŠŸæ ‡å¿—
        
        # æ ¹æ®ç»ˆæ­¢ç±»å‹è®¾ç½®æ˜¾ç¤ºä¿¡æ¯
        if terminated and not success:
            # çœŸæ­£ç¢°æ’ç»ˆæ­¢
            termination_type = collision_type if collision_occurred else "ç»ˆæ­¢"
        elif truncated:
            # è¾¾åˆ°æœ€å¤§æ­¥æ•°æˆªæ–­
            termination_type = "è¶…æ—¶"
        elif success:
            # æˆåŠŸåˆ°è¾¾
            termination_type = "æˆåŠŸ"
        else:
            termination_type = "æœªçŸ¥"
        
        # è®°å½•æ•°æ®
        reward_tracker.add_episode(episode_reward, step + 1, success, collision_occurred)
        
        # ç»“æŸè½¨è¿¹è®°å½•
        trajectory_tracker.end_episode(
            episode_trajectory, 
            success, 
            collision_occurred, 
            termination_type, 
            episode_reward, 
            step + 1, 
            {'reward_info': info.get('reward_info', {})}
        )
        
        # è¾“å‡ºè¿›åº¦
        elapsed_time = time.time() - start_time
        eta = (elapsed_time / (episode + 1)) * (max_episodes - episode - 1)

        # è®¡ç®—å½“å‰ç»Ÿè®¡ä¿¡æ¯
        current_avg_reward = np.mean(reward_tracker.episode_rewards[-10:]) if len(reward_tracker.episode_rewards) >= 10 else episode_reward
        current_success_rate = reward_tracker.success_rate[-1] if reward_tracker.success_rate else 0
        current_collision_rate = np.mean(reward_tracker.collision_rate[-10:]) if len(reward_tracker.collision_rate) >= 10 else (1.0 if collision_occurred else 0.0)

        print(f"å›åˆ {episode + 1:3d}/{max_episodes:3d} | "
              f"å¥–åŠ±: {episode_reward:8.2f} | "
              f"å¹³å‡: {current_avg_reward:6.2f} | "
              f"æ­¥æ•°: {step + 1:4d} | "
              f"æˆåŠŸ: {'âœ“' if success else 'âœ—'} | "
              f"æˆåŠŸç‡: {current_success_rate:.1%} | "
              f"ç»ˆæ­¢: {termination_type} | "
              f"ç¢°æ’ç‡: {current_collision_rate:.1%} | "
              f"ETA: {eta/60:.1f}åˆ†é’Ÿ")
        
        # å®šæœŸç»˜åˆ¶å’Œä¿å­˜
        if (episode + 1) % plot_interval == 0:
            reward_tracker.plot_training_progress()
            reward_tracker.save_data()
            
            # å®šæœŸä¿å­˜è½¨è¿¹æ•°æ®
            trajectory_tracker.periodic_save(episode + 1)
            
            # ä¿å­˜å½“å‰æ¨¡å‹
            model_path = PathConfig.get_episode_model_path(episode + 1)
            ppo_agent.save(model_path)
            print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
            
            # ğŸ”¥ æ˜¾ç¤ºç»éªŒå›æ”¾ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯
            replay_stats = ppo_agent.get_replay_buffer_stats()
            print(f"ğŸ“¦ ç»éªŒå›æ”¾ç¼“å†²åŒº: "
                  f"å®¹é‡ {replay_stats['size']}/1000 | "  # ğŸ”¥ æ›´æ–°æ˜¾ç¤ºä¸º1000
                  f"æˆåŠŸç»éªŒ {replay_stats['success_count']} ({replay_stats['success_rate']:.1%}) | "
                  f"ç´¯è®¡æ·»åŠ  {replay_stats['total_added']}")
            
            # ğŸ¯ ä¿®å¤æ¢ç´¢ç­–ç•¥: å»¶é•¿æ¢ç´¢æœŸï¼Œé˜²æ­¢è¿‡æ—©æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
            # é˜¶æ®µ1 (0-8000): é«˜æ¢ç´¢ 0.3 â†’ 0.25 (æ¸©å’Œè¡°å‡)
            # é˜¶æ®µ2 (8000-16000): ä¸­æ¢ç´¢ 0.25 â†’ 0.15 (é€‚åº¦è¡°å‡)
            # é˜¶æ®µ3 (16000-20000): ä½æ¢ç´¢ 0.15 â†’ 0.08 (ä¿ç•™æ¢ç´¢)
            if episode + 1 <= 8000:
                # å‰8000å›åˆï¼šæ¯3000å›åˆè¡°å‡ä¸€æ¬¡ï¼Œä¿æŒé«˜æ¢ç´¢
                if (episode + 1) % 3000 == 0:
                    current_std = ppo_agent.action_std
                    ppo_agent.decay_action_std(action_std_decay_rate=0.99, min_action_std=0.25)
                    print(f"ğŸ” æ¢ç´¢è¡°å‡: {current_std:.4f} â†’ {ppo_agent.action_std:.4f} (é˜¶æ®µ1: é«˜æ¢ç´¢æœŸ)")
            elif episode + 1 <= 16000:
                # ä¸­æœŸ8000-16000ï¼šæ¯3000å›åˆè¡°å‡ä¸€æ¬¡
                if (episode + 1) % 3000 == 0:
                    current_std = ppo_agent.action_std
                    ppo_agent.decay_action_std(action_std_decay_rate=0.97, min_action_std=0.15)
                    print(f"ğŸ” æ¢ç´¢è¡°å‡: {current_std:.4f} â†’ {ppo_agent.action_std:.4f} (é˜¶æ®µ2: ä¸­æ¢ç´¢æœŸ)")
            else:
                # åæœŸ16000-20000ï¼šæ¯2000å›åˆè¡°å‡ä¸€æ¬¡ï¼Œä¿ç•™è¶³å¤Ÿæ¢ç´¢
                if (episode + 1) % 2000 == 0:
                    current_std = ppo_agent.action_std
                    ppo_agent.decay_action_std(action_std_decay_rate=0.96, min_action_std=0.08)
                    print(f"ğŸ” æ¢ç´¢è¡°å‡: {current_std:.4f} â†’ {ppo_agent.action_std:.4f} (é˜¶æ®µ3: ç²¾è°ƒæœŸ)")
            
            print("-" * 80)
    
    # è®­ç»ƒå®Œæˆ
    print("="*80)
    print("ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå®Œæˆï¼")
    
    # æœ€ç»ˆç»Ÿè®¡
    final_avg_reward = np.mean(reward_tracker.episode_rewards[-50:]) if len(reward_tracker.episode_rewards) >= 50 else np.mean(reward_tracker.episode_rewards)
    final_success_rate = reward_tracker.success_rate[-1] if reward_tracker.success_rate else 0
    
    print(f"æœ€ç»ˆç»Ÿè®¡:")
    print(f"  - æ€»å›åˆæ•°: {len(reward_tracker.episode_rewards)}")
    print(f"  - æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_reward:.2f}")
    print(f"  - æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.2%}")
    print(f"  - è®­ç»ƒæ—¶é•¿: {(time.time() - start_time)/60:.1f}åˆ†é’Ÿ")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œæ•°æ®
    final_model_path = PathConfig.FINAL_MODEL
    ppo_agent.save(final_model_path)
    
    reward_tracker.plot_training_progress(PathConfig.FINAL_PROGRESS_PLOT)
    reward_tracker.save_data(PathConfig.FINAL_DATA_JSON)
    
    # ä¿å­˜å®Œæ•´çš„è½¨è¿¹æ•°æ®ï¼ˆåˆå¹¶æ‰€æœ‰å®šæœŸä¿å­˜çš„æ•°æ®ï¼‰
    try:
        trajectory_tracker.save_trajectories(PathConfig.FINAL_TRAJECTORIES_JSON)
        print(f"å®Œæ•´è½¨è¿¹æ•°æ®å·²ä¿å­˜: {PathConfig.FINAL_TRAJECTORIES_JSON}")
    except Exception as e:
        print(f"å®Œæ•´è½¨è¿¹ä¿å­˜å¤±è´¥: {e}")
        # å¦‚æœå®Œæ•´ä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜å½“å‰å†…å­˜ä¸­çš„æ•°æ®
        try:
            backup_path = PathConfig.FINAL_TRAJECTORIES_JSON.parent / f"{PathConfig.FINAL_TRAJECTORIES_JSON.stem}_backup{PathConfig.FINAL_TRAJECTORIES_JSON.suffix}"
            trajectory_tracker._save_trajectories_to_file(trajectory_tracker.trajectories, backup_path)
            print(f"å¤‡ä»½è½¨è¿¹æ•°æ®å·²ä¿å­˜: {backup_path}")
        except Exception as e2:
            print(f"å¤‡ä»½è½¨è¿¹ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")
    
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    print("="*80)
    
    env.close()
    return final_model_path, reward_tracker, trajectory_tracker

if __name__ == '__main__':
    # è¿è¡Œç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
    model_path, reward_tracker, trajectory_tracker = train_leader_phase1(
        max_episodes=20000,  # ä»500å¢åŠ åˆ°2000å›åˆ
        max_steps=3000,   # æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼Œç»™æ— äººæœºå……è¶³æ—¶é—´åˆ°è¾¾ç›®æ ‡
        plot_interval=1000   # æ¯100ä¸ªå›åˆç»˜åˆ¶ä¸€æ¬¡å›¾
    )
    
    print(f"è®­ç»ƒå®Œæˆï¼æ£€æŸ¥ {PathConfig.LOG_DIR} ç›®å½•æŸ¥çœ‹ç»“æœå›¾è¡¨å’Œæ•°æ®ã€‚")
