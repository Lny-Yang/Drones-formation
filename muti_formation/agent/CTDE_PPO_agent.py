from datetime import datetime
import torch
import torch.nn as nn
from torch.distributions import MultivariateNormal
from torch.distributions import Categorical
import numpy as np
import gym
from collections import deque
import random

# å…¼å®¹ä¸¤ç§å¯¼å…¥æ–¹å¼
try:
    from ..drone_envs.config import multi_drone_env  # ç›¸å¯¹å¯¼å…¥ï¼ˆä»æ ¹ç›®å½•è¿è¡Œæ—¶ï¼‰
except ImportError:
    from drone_envs.config import multi_drone_env  # ç›´æ¥å¯¼å…¥ï¼ˆä»muti_formationç›®å½•è¿è¡Œæ—¶ï¼‰

# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ CUDA è®¾å¤‡ï¼Œå¦‚æœæœ‰åˆ™ä½¿ç”¨ GPU è¿›è¡Œè®¡ç®—ï¼ŒåŒæ—¶æ¸…ç©º CUDA ç¼“å­˜ï¼›å¦åˆ™ä½¿ç”¨ CPUã€‚
device = torch.device('cpu')
if(torch.cuda.is_available()):
    device = torch.device('cuda:0')
    torch.cuda.empty_cache()
    print("Device set to : " + str(torch.cuda.get_device_name(device)))
else:
    print("Device set to : cpu")
print("============================================================================================")

################################## CTDE PPO Policy ##################################

# ç»éªŒå›æ”¾ç¼“å†²åŒºç±»ï¼Œç”¨äºå­˜å‚¨æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„æ•°æ®
class RolloutBuffer:
    def __init__(self):
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œç”¨äºå­˜å‚¨æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­äº¤äº’äº§ç”Ÿçš„æ•°æ®ã€‚
        self.actions = []  # å­˜å‚¨æ™ºèƒ½ä½“æ‰§è¡Œçš„åŠ¨ä½œ
        self.states = []  # å­˜å‚¨æ™ºèƒ½ä½“æ‰€å¤„çš„çŠ¶æ€
        self.logprobs = []  # å­˜å‚¨åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        self.rewards = []  # å­˜å‚¨è·å¾—çš„å¥–åŠ±
        self.state_values = []  # å­˜å‚¨çŠ¶æ€å€¼
        self.is_terminals = []  # å­˜å‚¨å›åˆæ˜¯å¦ç»“æŸçš„æ ‡å¿—

    def clear(self):
        # æ¸…ç©ºç¼“å†²åŒºä¸­çš„æ‰€æœ‰æ•°æ®
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.state_values[:]
        del self.is_terminals[:]


class PrioritizedReplayBuffer:
    """ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒº - ä¸“é—¨å­˜å‚¨é«˜ä»·å€¼ç»éªŒ"""
    def __init__(self, max_size=1000, success_priority=10.0):
        """
        Args:
            max_size: ç¼“å†²åŒºæœ€å¤§å®¹é‡
            success_priority: æˆåŠŸç»éªŒçš„ä¼˜å…ˆçº§æƒé‡
        """
        self.max_size = max_size
        self.success_priority = success_priority
        
        # ä½¿ç”¨dequeå®ç°å›ºå®šå¤§å°çš„FIFOç¼“å†²åŒº
        self.buffer = deque(maxlen=max_size)
        self.priorities = deque(maxlen=max_size)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_added = 0
        self.success_count = 0
    
    def add_episode(self, states, actions, logprobs, rewards, state_values, is_terminals, episode_return, is_success):
        """
        æ·»åŠ ä¸€æ•´ä¸ªepisodeçš„ç»éªŒ
        
        Args:
            states, actions, logprobs, rewards, state_values, is_terminals: episodeæ•°æ®
            episode_return: episodeæ€»å›æŠ¥
            is_success: æ˜¯å¦æˆåŠŸåˆ°è¾¾ç›®æ ‡
        """
        # è®¡ç®—ä¼˜å…ˆçº§ï¼šæˆåŠŸepisodeè·å¾—é«˜ä¼˜å…ˆçº§ï¼Œå¦åˆ™åŸºäºå›æŠ¥
        if is_success:
            priority = self.success_priority * (1.0 + episode_return / 1000.0)
            self.success_count += 1
        else:
            # éæˆåŠŸepisodeï¼Œä¼˜å…ˆçº§åŸºäºå½’ä¸€åŒ–å›æŠ¥
            priority = max(0.1, episode_return / 1000.0)  # æœ€å°ä¼˜å…ˆçº§0.1
        
        # å­˜å‚¨episodeæ•°æ®
        episode_data = {
            'states': states.copy(),
            'actions': actions.copy(),
            'logprobs': logprobs.copy(),
            'rewards': rewards.copy(),
            'state_values': state_values.copy(),
            'is_terminals': is_terminals.copy(),
            'episode_return': episode_return,
            'is_success': is_success,
            'length': len(states)
        }
        
        self.buffer.append(episode_data)
        self.priorities.append(priority)
        self.total_added += 1
    
    def sample(self, num_episodes=5):
        """
        æ ¹æ®ä¼˜å…ˆçº§é‡‡æ ·episodes
        
        Args:
            num_episodes: é‡‡æ ·çš„episodeæ•°é‡
        
        Returns:
            é‡‡æ ·çš„episodesæ•°æ®
        """
        if len(self.buffer) == 0:
            return None
        
        # è®¡ç®—é‡‡æ ·æ•°é‡ï¼ˆä¸è¶…è¿‡ç¼“å†²åŒºå¤§å°ï¼‰
        num_episodes = min(num_episodes, len(self.buffer))
        
        # å½’ä¸€åŒ–ä¼˜å…ˆçº§ä¸ºæ¦‚ç‡
        priorities = np.array(self.priorities)
        probabilities = priorities / priorities.sum()
        
        # æ ¹æ®ä¼˜å…ˆçº§é‡‡æ ·ï¼ˆæ— æ”¾å›ï¼‰
        sampled_indices = np.random.choice(
            len(self.buffer), 
            size=num_episodes, 
            replace=False,
            p=probabilities
        )
        
        # æ”¶é›†é‡‡æ ·çš„episodes
        sampled_states = []
        sampled_actions = []
        sampled_logprobs = []
        sampled_rewards = []
        sampled_state_values = []
        sampled_is_terminals = []
        
        for idx in sampled_indices:
            episode = self.buffer[idx]
            sampled_states.extend(episode['states'])
            sampled_actions.extend(episode['actions'])
            sampled_logprobs.extend(episode['logprobs'])
            sampled_rewards.extend(episode['rewards'])
            sampled_state_values.extend(episode['state_values'])
            sampled_is_terminals.extend(episode['is_terminals'])
        
        return {
            'states': sampled_states,
            'actions': sampled_actions,
            'logprobs': sampled_logprobs,
            'rewards': sampled_rewards,
            'state_values': sampled_state_values,
            'is_terminals': sampled_is_terminals,
            'num_episodes': num_episodes,
            'total_steps': len(sampled_states)
        }
    
    def __len__(self):
        return len(self.buffer)
    
    def get_stats(self):
        """è·å–ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯"""
        if len(self.buffer) == 0:
            return {
                'size': 0,
                'success_count': 0,
                'success_rate': 0.0,
                'avg_priority': 0.0,
                'total_added': self.total_added
            }
        
        success_in_buffer = sum(1 for ep in self.buffer if ep['is_success'])
        
        return {
            'size': len(self.buffer),
            'success_count': success_in_buffer,
            'success_rate': success_in_buffer / len(self.buffer),
            'avg_priority': np.mean(self.priorities),
            'total_added': self.total_added
        }

class VisualFeatureExtractor(nn.Module):
    """æ”¹è¿›ç‰ˆè§†è§‰ç‰¹å¾æå–å™¨ï¼Œç”¨äºå¤„ç†CNNæ·±åº¦ç‰¹å¾ï¼ˆ128ç»´ + 2é¢å¤–ç‰¹å¾ = 130ç»´ï¼‰"""
    def __init__(self, input_channels=128, enhanced_channels=2, feature_dim=64):
        super(VisualFeatureExtractor, self).__init__()
        self.input_channels = input_channels  # CNNç‰¹å¾ï¼š128ç»´
        self.enhanced_channels = enhanced_channels  # é¢å¤–ç‰¹å¾ï¼š2ç»´
        self.total_channels = input_channels + enhanced_channels  # æ€»å…±130ç»´

        # CNNç‰¹å¾å¤„ç†å™¨ï¼ˆå¤„ç†128ç»´CNNç‰¹å¾ï¼‰
        self.cnn_feature_processor = nn.Sequential(
            nn.Linear(input_channels, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, feature_dim),
            nn.ReLU(),
        )

        # å¢å¼ºç‰¹å¾å¤„ç†å™¨ï¼ˆå¤„ç†éšœç¢ç‰©æ£€æµ‹ + æœ€å°æ·±åº¦ï¼‰
        self.enhanced_feature_processor = nn.Sequential(
            nn.Linear(enhanced_channels, 32),
            nn.ReLU(),
            nn.Linear(32, feature_dim//2),
            nn.ReLU(),
        )

        # ç‰¹å¾èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(feature_dim + feature_dim//2, feature_dim),
            nn.ReLU(),
        )

        # é¿éšœå†³ç­–å¢å¼ºå™¨
        self.avoidance_enhancer = nn.Sequential(
            nn.Linear(enhanced_channels, 16),
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.Sigmoid()  # è¾“å‡ºé¿éšœæƒé‡
        )

        self.feature_dim = feature_dim

    def forward(self, depth_features):
        """
        å¤„ç†CNNæ·±åº¦ç‰¹å¾è¾“å…¥
        è¾“å…¥: depth_features (batch_size, 130) - 128ç»´CNNç‰¹å¾ + 2ç»´å¢å¼ºç‰¹å¾
        è¾“å‡º: æå–çš„è§†è§‰ç‰¹å¾ (batch_size, feature_dim)
        """
        # depth_features: [batch_size, total_channels] æˆ– [total_channels]
        # total_channels = input_channels(128) + enhanced_channels(2) = 130
        if len(depth_features.shape) == 1:
            depth_features = depth_features.unsqueeze(0)

        batch_size = depth_features.size(0)

        # åˆ†ç¦»CNNç‰¹å¾å’Œå¢å¼ºç‰¹å¾
        cnn_features = depth_features[:, :self.input_channels]  # å‰128ç»´ï¼šCNNç‰¹å¾
        enhanced_features = depth_features[:, self.input_channels:self.total_channels]  # å2ç»´ï¼šå¢å¼ºç‰¹å¾

        # å¤„ç†CNNç‰¹å¾
        cnn_processed = self.cnn_feature_processor(cnn_features)

        # å¤„ç†å¢å¼ºç‰¹å¾
        enhanced_processed = self.enhanced_feature_processor(enhanced_features)

        # ç”Ÿæˆé¿éšœæƒé‡
        avoidance_weights = self.avoidance_enhancer(enhanced_features)

        # ç‰¹å¾èåˆ
        combined_features = torch.cat([cnn_processed, enhanced_processed], dim=-1)
        fused_features = self.fusion_layer(combined_features)

        # åº”ç”¨é¿éšœæƒé‡å¢å¼º
        final_features = fused_features * avoidance_weights.mean(dim=1, keepdim=True)

        return final_features

class LeaderActorCritic(nn.Module):
    """é¢†èˆªè€…ä¸“ç”¨ç½‘ç»œï¼Œé›†æˆå¢å¼ºè§†è§‰è¾“å…¥ï¼ˆåŒ…å«é¿éšœå†³ç­–ä¿¡æ¯ï¼‰"""
    def __init__(self, state_dim, visual_dim, action_dim, has_continuous_action_space, action_std_init):
        super(LeaderActorCritic, self).__init__()
        self.has_continuous_action_space = has_continuous_action_space
        self.action_dim = action_dim
        self.visual_dim = visual_dim  # ä¿å­˜è§†è§‰ç»´åº¦

        if has_continuous_action_space:
            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)

        # å¢å¼ºè§†è§‰ç‰¹å¾æå–å™¨ï¼ˆæ”¯æŒ130ç»´è¾“å…¥ï¼š128 CNNç‰¹å¾ + 2å¢å¼ºç‰¹å¾ï¼‰
        self.visual_extractor = VisualFeatureExtractor(
            input_channels=128,  # CNNæ·±åº¦ç‰¹å¾ï¼š128ç»´
            enhanced_channels=2,  # å¢å¼ºç‰¹å¾ï¼šéšœç¢æ£€æµ‹ + æœ€å°æ·±åº¦
            feature_dim=64
        )

        # çŠ¶æ€ç¼–ç å™¨ï¼ˆå¤„ç†éè§†è§‰çŠ¶æ€ï¼‰
        # åŠ¨æ€è®¡ç®—éè§†è§‰çŠ¶æ€ç»´åº¦ï¼šæ€»çŠ¶æ€ç»´åº¦å‡å»æ·±åº¦ç‰¹å¾ç»´åº¦
        # æ·±åº¦ç‰¹å¾å›ºå®šä¸º130ç»´ï¼ˆ128 CNN + 2å¢å¼ºï¼‰
        self.depth_feature_dim = 130
        self.non_visual_dim = max(1, state_dim - self.depth_feature_dim)
        
        self.state_encoder = nn.Sequential(
            nn.Linear(self.non_visual_dim, 64),  # åŠ¨æ€éè§†è§‰çŠ¶æ€ç»´åº¦
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
        )

        # èåˆç½‘ç»œï¼ˆè§†è§‰ + çŠ¶æ€ï¼‰
        self.fusion_net = nn.Sequential(
            nn.Linear(64 + 64, 128),  # çŠ¶æ€ç‰¹å¾64 + è§†è§‰ç‰¹å¾64
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
        )

        # æ¼”å‘˜ç½‘ç»œ
        if has_continuous_action_space:
            self.actor = nn.Sequential(
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, action_dim)
                # ç§»é™¤Tanhæ¿€æ´»å‡½æ•°ï¼Œè®©ç½‘ç»œç›´æ¥è¾“å‡ºåŠ¨ä½œå€¼
                # åŠ¨ä½œä¼šåœ¨ç¯å¢ƒä¸­è¢«clipåˆ°configå®šä¹‰çš„èŒƒå›´
            )
            # ğŸ”¥ åˆå§‹åŒ–è¾“å‡ºå±‚çš„biasä¸º0ï¼Œè®©åˆå§‹åŠ¨ä½œæ¥è¿‘0
            nn.init.zeros_(self.actor[-1].bias)
            nn.init.orthogonal_(self.actor[-1].weight, gain=0.01)  # å°çš„åˆå§‹æƒé‡
        else:
            self.actor = nn.Sequential(
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Linear(64, action_dim),
                nn.Softmax(dim=-1)
            )

        # è¯„è®ºå®¶ç½‘ç»œï¼ˆå…¨å±€çŠ¶æ€è¯„ä¼°ï¼‰
        self.critic = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def set_action_std(self, new_action_std):
        if self.has_continuous_action_space:
            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)

    def forward(self, state, depth_features):
        # ç¡®ä¿è¾“å…¥æ˜¯æ‰¹æ¬¡æ ¼å¼
        if len(state.shape) == 1:
            state = state.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        if depth_features is None or depth_features.numel() == 0:
            # å¦‚æœæ·±åº¦ç‰¹å¾ä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤ç‰¹å¾
            batch_size = state.shape[0]
            depth_features = torch.ones(batch_size, 130, device=state.device)  # 128 CNN + 2å¢å¼º
        elif len(depth_features.shape) == 1:
            depth_features = depth_features.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        
        # ç¡®ä¿æ·±åº¦ç‰¹å¾ç»´åº¦æ­£ç¡®
        if depth_features.shape[-1] < 130:
            # å¦‚æœç‰¹å¾ä¸è¶³130ç»´ï¼Œç”¨1.0å¡«å……åˆ°130ç»´
            batch_size = depth_features.shape[0]
            current_dim = depth_features.shape[-1]
            padding = torch.ones(batch_size, 130 - current_dim, device=depth_features.device)
            depth_features = torch.cat([depth_features, padding], dim=-1)
        elif depth_features.shape[-1] > 130:
            # å¦‚æœç‰¹å¾è¶…è¿‡130ç»´ï¼Œæˆªå–å‰130ç»´
            depth_features = depth_features[:, :130]

        # æå–è§†è§‰ç‰¹å¾
        visual_features = self.visual_extractor(depth_features)

        # ç¼–ç çŠ¶æ€ç‰¹å¾ï¼ˆæ’é™¤æ·±åº¦ç‰¹å¾ï¼‰
        # éè§†è§‰çŠ¶æ€æ˜¯å‰non_visual_dimç»´
        non_visual_features = state[:, :self.non_visual_dim]  # pos + vel + orientation + target
        state_features = self.state_encoder(non_visual_features)

        # èåˆè§†è§‰å’ŒçŠ¶æ€ç‰¹å¾
        fused_features = torch.cat([state_features, visual_features], dim=-1)
        fused_output = self.fusion_net(fused_features)

        return fused_output

    def act(self, state, depth_features):
        fused_output = self.forward(state, depth_features)

        if self.has_continuous_action_space:
            action_mean = self.actor(fused_output)
            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)
            dist = MultivariateNormal(action_mean, cov_mat)
        else:
            action_probs = self.actor(fused_output)
            dist = Categorical(action_probs)

        action = dist.sample()
        action_logprob = dist.log_prob(action)
        state_val = self.critic(fused_output)

        return action.detach(), action_logprob.detach(), state_val.detach()

    def evaluate(self, state, depth_features, action):
        fused_output = self.forward(state, depth_features)

        if self.has_continuous_action_space:
            action_mean = self.actor(fused_output)
            action_var = self.action_var.expand_as(action_mean)
            cov_mat = torch.diag_embed(action_var).to(device)
            dist = MultivariateNormal(action_mean, cov_mat)

            if self.action_dim == 1:
                action = action.reshape(-1, self.action_dim)
        else:
            action_probs = self.actor(fused_output)
            dist = Categorical(action_probs)

        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_values = self.critic(fused_output)

        return action_logprobs, state_values, dist_entropy

class FollowerActorCritic(nn.Module):
    """è·Ÿéšè€…ä¸“ç”¨ç½‘ç»œï¼Œç®€åŒ–ç‰ˆï¼Œæ”¯æŒåŠ¨æ€çŠ¶æ€ç»´åº¦"""
    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):
        super(FollowerActorCritic, self).__init__()
        self.has_continuous_action_space = has_continuous_action_space
        self.action_dim = action_dim
        self.state_dim = state_dim  # ä¿å­˜çŠ¶æ€ç»´åº¦

        if has_continuous_action_space:
            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)

        # è·Ÿéšè€…ç½‘ç»œï¼ˆç®€åŒ–ï¼Œæ— è§†è§‰è¾“å…¥ï¼‰
        self.network = nn.Sequential(
            nn.Linear(state_dim, 64),  # ä½¿ç”¨åŠ¨æ€çŠ¶æ€ç»´åº¦
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
        )

        # æ¼”å‘˜ç½‘ç»œ
        if has_continuous_action_space:
            self.actor = nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, action_dim)
                # ç§»é™¤Tanhæ¿€æ´»å‡½æ•°
            )
            # ğŸ”¥ åˆå§‹åŒ–è¾“å‡ºå±‚çš„biasä¸º0ï¼Œè®©åˆå§‹åŠ¨ä½œæ¥è¿‘0
            nn.init.zeros_(self.actor[-1].bias)
            nn.init.orthogonal_(self.actor[-1].weight, gain=0.01)  # å°çš„åˆå§‹æƒé‡
        else:
            self.actor = nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, action_dim),
                nn.Softmax(dim=-1)
            )

        # è¯„è®ºå®¶ç½‘ç»œ
        self.critic = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def set_action_std(self, new_action_std):
        if self.has_continuous_action_space:
            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)

    def act(self, state):
        features = self.network(state)

        if self.has_continuous_action_space:
            action_mean = self.actor(features)
            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)
            dist = MultivariateNormal(action_mean, cov_mat)
        else:
            action_probs = self.actor(features)
            dist = Categorical(action_probs)

        action = dist.sample()
        action_logprob = dist.log_prob(action)
        state_val = self.critic(features)

        return action.detach(), action_logprob.detach(), state_val.detach()

    def evaluate(self, state, action):
        features = self.network(state)

        if self.has_continuous_action_space:
            action_mean = self.actor(features)
            action_var = self.action_var.expand_as(action_mean)
            cov_mat = torch.diag_embed(action_var).to(device)
            dist = MultivariateNormal(action_mean, cov_mat)

            if self.action_dim == 1:
                action = action.reshape(-1, self.action_dim)
        else:
            action_probs = self.actor(features)
            dist = Categorical(action_probs)

        action_logprobs = dist.log_prob(action)
        dist_entropy = dist.entropy()
        state_values = self.critic(features)

        return action_logprobs, state_values, dist_entropy

class CTDE_PPO:
    """CTDEæ¶æ„çš„PPOç®—æ³•"""
    def __init__(self,
                 leader_state_dim,
                 follower_state_dim,
                 leader_visual_dim,
                 action_dim,
                 num_drones=5,
                 lr_actor=0.0003,
                 lr_critic=0.001,
                 gamma=0.99,
                 K_epochs=40,
                 eps_clip=0.2,
                 has_continuous_action_space=True,
                 action_std_init=0.6):

        self.has_continuous_action_space = has_continuous_action_space
        self.num_drones = num_drones
        self.leader_idx = 0
        self.follower_indices = list(range(1, num_drones))
        self.action_std_init = action_std_init  # æ·»åŠ è¿™ä¸ªå±æ€§

        if has_continuous_action_space:
            self.action_std = action_std_init

        self.gamma = gamma
        self.eps_clip = eps_clip
        self.K_epochs = K_epochs

        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ï¼‰
        self.buffers = [RolloutBuffer() for _ in range(num_drones)]
        
        # ğŸ”¥ æ–°å¢ï¼šä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒº - é˜²æ­¢ç¾éš¾æ€§é—å¿˜
        # ğŸ¯ ä¿®å¤å¥–åŠ±æ¬ºéª—åçš„ä¼˜åŒ–: è¿›ä¸€æ­¥æ‰©å®¹ä»¥åº”å¯¹20000å›åˆé•¿æœŸè®­ç»ƒ
        # å®¹é‡è®¡ç®—: 20000å›åˆ Ã— 90%æˆåŠŸç‡ Ã— 2%ä¿ç•™ = 360ä¸ªæˆåŠŸç»éªŒ
        # å®é™…è®¾ç½®: 1000å®¹é‡å¯å®¹çº³æ›´å¤šå†å²ï¼Œæ›´å¼ºé˜²é—å¿˜èƒ½åŠ›
        self.replay_buffer = PrioritizedReplayBuffer(
            max_size=1000,  # ï¿½ ä»500æ‰©å¤§åˆ°1000ï¼Œå¼ºåŒ–å†å²ç»éªŒä¿ç•™
            success_priority=80.0  # ï¿½ ä»50.0æå‡åˆ°80.0ï¼Œæœ€å¤§åŒ–æˆåŠŸç»éªŒæƒé‡
        )
        self.use_replay = True  # æ˜¯å¦ä½¿ç”¨ç»éªŒå›æ”¾
        self.replay_ratio = 0.5  # ä¿æŒ0.5ï¼Œå¹³è¡¡å†å²å’Œæ–°ç»éªŒï¼ˆé¿å…è¿‡æ‹Ÿåˆå†å²ï¼‰

        # åˆ›å»ºé¢†èˆªè€…å’Œè·Ÿéšè€…ç½‘ç»œ
        self.leader_policy = LeaderActorCritic(
            leader_state_dim, leader_visual_dim, action_dim,
            has_continuous_action_space, action_std_init
        ).to(device)

        self.follower_policies = [
            FollowerActorCritic(follower_state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)
            for _ in range(num_drones - 1)
        ]

        # å…¨å±€è¯„è®ºå®¶ï¼ˆCTDEçš„æ ¸å¿ƒï¼‰
        # ç¬¬ä¸€é˜¶æ®µåªæœ‰ä¸€ä¸ªé¢†èˆªè€…ï¼Œä½¿ç”¨é¢†èˆªè€…è‡ªå·±çš„è¯„è®ºå®¶
        # ç¬¬äºŒé˜¶æ®µæœ‰å¤šä¸ªæ™ºèƒ½ä½“ï¼Œä½¿ç”¨å…¨å±€è¯„è®ºå®¶
        if self.num_drones == 1:
            # ç¬¬ä¸€é˜¶æ®µï¼šé¢†èˆªè€…è¯„è®ºå®¶å·²åœ¨LeaderActorCriticä¸­å®šä¹‰
            self.global_critic = None
        else:
            # ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€è¯„è®ºå®¶
            self.global_critic = nn.Sequential(
                nn.Linear(leader_state_dim + follower_state_dim * (num_drones - 1), 256),
                nn.ReLU(),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Linear(128, 1)
            ).to(device)

        # ä¼˜åŒ–å™¨
        self.leader_optimizer = torch.optim.Adam([
            {'params': self.leader_policy.parameters(), 'lr': lr_actor}
        ])

        self.follower_optimizers = [
            torch.optim.Adam([
                {'params': policy.parameters(), 'lr': lr_actor}
            ]) for policy in self.follower_policies
        ]

        # åªæœ‰åœ¨å…¨å±€è¯„è®ºå®¶å­˜åœ¨æ—¶æ‰åˆ›å»ºå…¶ä¼˜åŒ–å™¨
        if self.global_critic is not None:
            self.critic_optimizer = torch.optim.Adam([
                {'params': self.global_critic.parameters(), 'lr': lr_critic}
            ])
        else:
            self.critic_optimizer = None

        # æ—§ç½‘ç»œ
        self.leader_policy_old = LeaderActorCritic(
            leader_state_dim, leader_visual_dim, action_dim,
            has_continuous_action_space, action_std_init
        ).to(device)
        self.leader_policy_old.load_state_dict(self.leader_policy.state_dict())

        self.follower_policies_old = []
        for i in range(num_drones - 1):
            old_policy = FollowerActorCritic(
                follower_state_dim, action_dim, has_continuous_action_space, action_std_init
            ).to(device)
            old_policy.load_state_dict(self.follower_policies[i].state_dict())
            self.follower_policies_old.append(old_policy)

        self.MseLoss = nn.MSELoss()

    def select_action(self, states, depth_features=None, leader_only=False):
        """ä¸ºæ‰€æœ‰æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ"""
        actions = []

        # å¤„ç†å•ä¸ªçŠ¶æ€è¾“å…¥çš„æƒ…å†µï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if not isinstance(states, list):
            states = [states]

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ™ºèƒ½ä½“çŠ¶æ€
        if len(states) < self.num_drones and not leader_only:
            # å¦‚æœçŠ¶æ€ä¸è¶³ï¼Œå¤åˆ¶æœ€åä¸€ä¸ªçŠ¶æ€
            while len(states) < self.num_drones:
                states.append(states[-1])

        # é¢†èˆªè€…åŠ¨ä½œé€‰æ‹©
        leader_state = states[0]  # é¢†èˆªè€…æ€»æ˜¯ç¬¬ä¸€ä¸ª

        # ä»çŠ¶æ€ä¸­æå–æ·±åº¦ç‰¹å¾ï¼ˆå¦‚æœæ²¡æœ‰æä¾›depth_featuresï¼‰
        # æ·±åº¦ç‰¹å¾æ˜¯çŠ¶æ€çš„æœ€å130ç»´
        if depth_features is None and len(leader_state) >= self.leader_policy.depth_feature_dim:
            depth_start_idx = len(leader_state) - self.leader_policy.depth_feature_dim
            depth_features = leader_state[depth_start_idx:]  # æå–æœ€å130ç»´ä½œä¸ºæ·±åº¦ç‰¹å¾

        if depth_features is not None:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(leader_state).to(device)
                depth_tensor = torch.FloatTensor(depth_features).to(device)
                action, action_logprob, state_val = self.leader_policy_old.act(state_tensor, depth_tensor)

            self.buffers[self.leader_idx].states.append(state_tensor)
            self.buffers[self.leader_idx].actions.append(action)
            self.buffers[self.leader_idx].logprobs.append(action_logprob)
            self.buffers[self.leader_idx].state_values.append(state_val)

            # Clip action to config range
            action_np = action.detach().cpu().numpy().flatten()
            action_np[0] = np.clip(action_np[0], multi_drone_env['thrust_lower_bound'], multi_drone_env['thrust_upper_bound'])
            action_np[1] = np.clip(action_np[1], multi_drone_env['torque_lower_bound'], multi_drone_env['torque_upper_bound'])
            actions.append(action_np)
        else:
            # å¦‚æœæ²¡æœ‰æ·±åº¦ç‰¹å¾,ä½¿ç”¨ç®€åŒ–ç‰ˆï¼ˆå°†é¢†èˆªè€…å½“ä½œè·Ÿéšè€…å¤„ç†ï¼‰
            with torch.no_grad():
                state_tensor = torch.FloatTensor(leader_state).to(device)
                action, action_logprob, state_val = self.follower_policies_old[0].act(state_tensor)

            self.buffers[self.leader_idx].states.append(state_tensor)
            self.buffers[self.leader_idx].actions.append(action)
            self.buffers[self.leader_idx].logprobs.append(action_logprob)
            self.buffers[self.leader_idx].state_values.append(state_val)

            # Clip action to config range
            action_np = action.detach().cpu().numpy().flatten()
            action_np[0] = np.clip(action_np[0], multi_drone_env['thrust_lower_bound'], multi_drone_env['thrust_upper_bound'])
            action_np[1] = np.clip(action_np[1], multi_drone_env['torque_lower_bound'], multi_drone_env['torque_upper_bound'])
            actions.append(action_np)

        # å¦‚æœåªéœ€è¦é¢†èˆªè€…åŠ¨ä½œï¼Œè¿”å›
        if leader_only:
            return actions

        # è·Ÿéšè€…åŠ¨ä½œé€‰æ‹©
        for i, follower_idx in enumerate(self.follower_indices):
            if follower_idx < len(states):
                with torch.no_grad():
                    state_tensor = torch.FloatTensor(states[follower_idx]).to(device)
                    # è·Ÿéšè€…è§‚æµ‹å·²ç»æ˜¯å®Œæ•´çš„26ç»´ï¼ˆæ·±åº¦ç‰¹å¾ç”¨0å¡«å……ï¼‰
                    action, action_logprob, state_val = self.follower_policies_old[i].act(state_tensor)

                self.buffers[follower_idx].states.append(state_tensor)
                self.buffers[follower_idx].actions.append(action)
                self.buffers[follower_idx].logprobs.append(action_logprob)
                self.buffers[follower_idx].state_values.append(state_val)

                # Clip action to config range
                action_np = action.detach().cpu().numpy().flatten()
                action_np[0] = np.clip(action_np[0], multi_drone_env['thrust_lower_bound'], multi_drone_env['thrust_upper_bound'])
                action_np[1] = np.clip(action_np[1], multi_drone_env['torque_lower_bound'], multi_drone_env['torque_upper_bound'])
                actions.append(action_np)
            else:
                # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„çŠ¶æ€ï¼Œä½¿ç”¨é›¶åŠ¨ä½œ
                actions.append(np.zeros(self.action_dim))

        return actions

    def update(self):
        """CTDEæ›´æ–°ç­–ç•¥ - é›†æˆä¼˜å…ˆç»éªŒå›æ”¾"""
        
        # ğŸ”¥ æ­¥éª¤1: ä¿å­˜å½“å‰episodeåˆ°ç»éªŒå›æ”¾ç¼“å†²åŒº
        if self.use_replay and len(self.buffers[0].rewards) > 0:
            # è®¡ç®—episodeæ€»å›æŠ¥
            episode_return = sum(self.buffers[0].rewards)
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸï¼ˆä»æœ€åä¸€ä¸ªå¥–åŠ±åˆ¤æ–­ï¼ŒæˆåŠŸå¥–åŠ±é€šå¸¸å¾ˆå¤§ï¼‰
            is_success = any(r > 1500 for r in self.buffers[0].rewards)  # æˆåŠŸå¥–åŠ±ä¸€èˆ¬>2000
            
            # æ·»åŠ åˆ°å›æ”¾ç¼“å†²åŒº
            self.replay_buffer.add_episode(
                states=self.buffers[0].states.copy(),
                actions=self.buffers[0].actions.copy(),
                logprobs=self.buffers[0].logprobs.copy(),
                rewards=self.buffers[0].rewards.copy(),
                state_values=self.buffers[0].state_values.copy(),
                is_terminals=self.buffers[0].is_terminals.copy(),
                episode_return=episode_return,
                is_success=is_success
            )
        
        # ğŸ”¥ æ­¥éª¤2: ä»ç»éªŒå›æ”¾ç¼“å†²åŒºé‡‡æ ·å¹¶æ··åˆåˆ°å½“å‰buffer
        if self.use_replay and len(self.replay_buffer) > 10:  # è‡³å°‘æœ‰10ä¸ªepisodesæ‰å¼€å§‹å›æ”¾
            # é‡‡æ ·å†å²ç»éªŒ
            replay_data = self.replay_buffer.sample(num_episodes=min(5, len(self.replay_buffer) // 10))
            
            if replay_data is not None:
                # å°†å›æ”¾æ•°æ®æ··åˆåˆ°å½“å‰bufferï¼ˆåªæ··åˆé¢†èˆªè€…æ•°æ®ï¼‰
                # è®¡ç®—æ··åˆæ¯”ä¾‹
                current_size = len(self.buffers[0].rewards)
                replay_size = len(replay_data['rewards'])
                
                # æŒ‰replay_ratioæ¯”ä¾‹æ··åˆ
                target_replay_size = int(current_size * self.replay_ratio / (1 - self.replay_ratio))
                if replay_size > target_replay_size:
                    # éšæœºé‡‡æ ·ä¸€éƒ¨åˆ†å›æ”¾æ•°æ®
                    indices = random.sample(range(replay_size), target_replay_size)
                    replay_data = {
                        'states': [replay_data['states'][i] for i in indices],
                        'actions': [replay_data['actions'][i] for i in indices],
                        'logprobs': [replay_data['logprobs'][i] for i in indices],
                        'rewards': [replay_data['rewards'][i] for i in indices],
                        'state_values': [replay_data['state_values'][i] for i in indices],
                        'is_terminals': [replay_data['is_terminals'][i] for i in indices],
                    }
                
                # æ··åˆæ•°æ®åˆ°buffer
                self.buffers[0].states.extend(replay_data['states'])
                self.buffers[0].actions.extend(replay_data['actions'])
                self.buffers[0].logprobs.extend(replay_data['logprobs'])
                self.buffers[0].rewards.extend(replay_data['rewards'])
                self.buffers[0].state_values.extend(replay_data['state_values'])
                self.buffers[0].is_terminals.extend(replay_data['is_terminals'])
        
        # æ”¶é›†æ‰€æœ‰æ™ºèƒ½ä½“çš„å¥–åŠ±
        all_rewards = []
        all_original_rewards = []  # ä¿å­˜åŸå§‹å›æŠ¥ç”¨äºä¼˜åŠ¿å‡½æ•°è®¡ç®—
        all_states = []
        all_actions = []
        all_logprobs = []
        all_state_values = []
        all_is_terminals = []

        for i in range(self.num_drones):
            # è·³è¿‡æ²¡æœ‰å¥–åŠ±æ•°æ®çš„æ™ºèƒ½ä½“
            if len(self.buffers[i].rewards) == 0:
                continue
                
            # Monte Carlo ä¼°è®¡å›æŠ¥
            rewards = []
            discounted_reward = 0
            for reward, is_terminal in zip(reversed(self.buffers[i].rewards),
                                         reversed(self.buffers[i].is_terminals)):
                if is_terminal:
                    discounted_reward = 0
                discounted_reward = reward + (self.gamma * discounted_reward)
                rewards.insert(0, discounted_reward)

            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
            # ä¿å­˜åŸå§‹å›æŠ¥ç”¨äºä¼˜åŠ¿å‡½æ•°è®¡ç®—
            original_rewards = rewards.clone()
            
            # å¥–åŠ±æ ‡å‡†åŒ–ï¼šä½¿ç”¨æ›´ç¨³å®šçš„æ–¹æ³•ï¼Œé¿å…æ•°å€¼é—®é¢˜
            if len(rewards) > 1:  # è‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬æ‰èƒ½è®¡ç®—æ–¹å·®
                rewards_mean = rewards.mean()
                rewards_std = rewards.std()
                # åªåœ¨å¥–åŠ±å˜åŒ–æ˜¾è‘—æ—¶è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆé¿å…è¿‡åº¦æ ‡å‡†åŒ–ï¼‰
                if rewards_std > max(0.1, abs(rewards_mean) * 0.05):  # æ ‡å‡†å·®å¤§äºå¹³å‡å€¼çš„5%æˆ–0.1
                    # ä½¿ç”¨æ¸©å’Œçš„æ ‡å‡†åŒ–ï¼Œä¿ç•™å¥–åŠ±çš„ç›¸å¯¹å¼ºåº¦
                    rewards = (rewards - rewards_mean) / (rewards_std + 1e-8)
                    # é™åˆ¶æ ‡å‡†åŒ–åçš„å¥–åŠ±èŒƒå›´ï¼Œé¿å…æç«¯å€¼
                    rewards = torch.clamp(rewards, -5.0, 5.0)

            all_rewards.append(rewards)
            all_original_rewards.append(original_rewards)  # ä¿å­˜åŸå§‹å›æŠ¥
            all_states.append(torch.squeeze(torch.stack(self.buffers[i].states, dim=0)).detach().to(device))
            all_actions.append(torch.squeeze(torch.stack(self.buffers[i].actions, dim=0)).detach().to(device))
            all_logprobs.append(torch.squeeze(torch.stack(self.buffers[i].logprobs, dim=0)).detach().to(device))
            all_state_values.append(torch.squeeze(torch.stack(self.buffers[i].state_values, dim=0)).detach().to(device))
            all_is_terminals.append(self.buffers[i].is_terminals)

        # å¦‚æœæ²¡æœ‰æ´»è·ƒçš„æ™ºèƒ½ä½“ï¼Œè¿”å›
        if not all_rewards:
            return

        # å…¨å±€çŠ¶æ€ç”¨äºè¯„è®ºå®¶
        if len(all_states) == 1:
            # ç¬¬ä¸€é˜¶æ®µï¼šåªæœ‰é¢†èˆªè€…ï¼Œä½¿ç”¨é¢†èˆªè€…è‡ªå·±çš„è¯„è®ºå®¶
            global_input = all_states[0]
            use_global_critic = False
            # ä»çŠ¶æ€ä¸­æå–æ·±åº¦ç‰¹å¾ï¼ˆæœ€å130ç»´ï¼‰
            if all_states[0].dim() > 1:  # æ‰¹æ¬¡æ•°æ®
                depth_start_idx = all_states[0].shape[-1] - self.leader_policy.depth_feature_dim
                depth_features_batch = all_states[0][:, depth_start_idx:].detach().to(device)
            else:  # å•ä¸ªæ•°æ®
                depth_start_idx = len(all_states[0]) - self.leader_policy.depth_feature_dim
                depth_features_batch = all_states[0][depth_start_idx:].detach().to(device).unsqueeze(0)
        else:
            # å…¶ä»–é˜¶æ®µï¼šæ‰€æœ‰æ™ºèƒ½ä½“ï¼Œä½¿ç”¨å…¨å±€è¯„è®ºå®¶
            # å°†æ‰€æœ‰æ™ºèƒ½ä½“çš„çŠ¶æ€æ²¿ç€ç‰¹å¾ç»´åº¦è¿æ¥
            global_input = torch.cat(all_states, dim=-1)  # [batch_size, total_state_dim]
            use_global_critic = True
            depth_features_batch = None

        # æ›´æ–°è¯„è®ºå®¶
        for _ in range(self.K_epochs):
            if use_global_critic:
                # å…¨å±€è¯„è®ºå®¶ä½¿ç”¨æ‰€æœ‰æ™ºèƒ½ä½“çš„è”åˆçŠ¶æ€ï¼Œè¾“å‡ºå…¨å±€ä»·å€¼
                global_values = self.global_critic(global_input)  # [batch_size, 1]
                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…ç›¸åŒçš„å…¨å±€ä»·å€¼ï¼ˆCTDEçš„æ ¸å¿ƒæ€æƒ³ï¼‰
                critic_loss = sum(self.MseLoss(global_values.squeeze(), r) for r in all_rewards)
            else:
                # ç¬¬ä¸€é˜¶æ®µä½¿ç”¨é¢†èˆªè€…è¯„è®ºå®¶
                fused_features = self.leader_policy(all_states[0], depth_features_batch)
                critic_values = self.leader_policy.critic(fused_features)
                critic_loss = self.MseLoss(critic_values.squeeze(), all_rewards[0])

            if use_global_critic:
                self.critic_optimizer.zero_grad()
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.global_critic.parameters(), max_norm=0.5)  # æ¢¯åº¦è£å‰ª
                self.critic_optimizer.step()
            else:
                # ç¬¬ä¸€é˜¶æ®µæ›´æ–°é¢†èˆªè€…è¯„è®ºå®¶ï¼ˆé€šè¿‡leader_optimizerï¼Œå› ä¸ºè¯„è®ºå®¶æ˜¯leader_policyçš„ä¸€éƒ¨åˆ†ï¼‰
                pass  # è¯„è®ºå®¶å·²ç»é€šè¿‡leader_optimizeræ›´æ–°äº†

        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆä½¿ç”¨åŸå§‹å›æŠ¥ï¼Œä¸ä½¿ç”¨æ ‡å‡†åŒ–åçš„ï¼‰
        if use_global_critic:
            # å…¨å±€è¯„è®ºå®¶ä¸ºæ‰€æœ‰æ™ºèƒ½ä½“æä¾›ç›¸åŒçš„å…¨å±€ä»·å€¼ä¼°è®¡
            global_values = self.global_critic(global_input).detach()  # [batch_size, 1]
            # æ¯ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨ç›¸åŒçš„å…¨å±€ä»·å€¼è®¡ç®—ä¼˜åŠ¿ï¼ˆCTDEçš„æ ¸å¿ƒï¼‰
            advantages = [original_r - global_values.squeeze() for original_r in all_original_rewards]
        else:
            fused_features = self.leader_policy(all_states[0], depth_features_batch)
            critic_values = self.leader_policy.critic(fused_features).detach()
            advantages = [all_original_rewards[0] - critic_values.squeeze()]

        # æ›´æ–°é¢†èˆªè€…ç­–ç•¥ï¼ˆå¦‚æœæœ‰é¢†èˆªè€…æ•°æ®ï¼‰
        if len(self.buffers[self.leader_idx].rewards) > 0:
            leader_state_idx = 0  # é¢†èˆªè€…æ˜¯ç¬¬ä¸€ä¸ª
            for _ in range(self.K_epochs):
                # ä»çŠ¶æ€ä¸­æå–æ·±åº¦ç‰¹å¾ï¼ˆæœ€å130ç»´ï¼‰
                if all_states[leader_state_idx].dim() > 1:  # æ‰¹æ¬¡æ•°æ®
                    depth_start_idx = all_states[leader_state_idx].shape[-1] - self.leader_policy.depth_feature_dim
                    depth_features = all_states[leader_state_idx][:, depth_start_idx:].detach().to(device)
                else:  # å•ä¸ªæ•°æ®
                    depth_start_idx = len(all_states[leader_state_idx]) - self.leader_policy.depth_feature_dim
                    depth_features = all_states[leader_state_idx][depth_start_idx:].detach().to(device).unsqueeze(0)

                logprobs, state_values, dist_entropy = self.leader_policy.evaluate(
                    all_states[leader_state_idx], depth_features, all_actions[leader_state_idx])

                ratios = torch.exp(logprobs - all_logprobs[leader_state_idx].detach())
                surr1 = ratios * advantages[leader_state_idx]
                surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages[leader_state_idx]

                loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values.squeeze(), all_rewards[leader_state_idx]) - 0.05 * dist_entropy

                self.leader_optimizer.zero_grad()
                loss.mean().backward()
                torch.nn.utils.clip_grad_norm_(self.leader_policy.parameters(), max_norm=0.5)  # æ¢¯åº¦è£å‰ª
                self.leader_optimizer.step()

        # æ›´æ–°è·Ÿéšè€…ç­–ç•¥ï¼ˆå¦‚æœæœ‰è·Ÿéšè€…æ•°æ®ï¼‰
        for follower_idx in range(len(self.follower_indices)):
            actual_idx = self.follower_indices[follower_idx]
            if len(self.buffers[actual_idx].rewards) > 0:
                state_idx = [i for i, idx in enumerate(range(self.num_drones)) if len(self.buffers[idx].rewards) > 0].index(actual_idx)
                for _ in range(self.K_epochs):
                    logprobs, state_values, dist_entropy = self.follower_policies[follower_idx].evaluate(
                        all_states[state_idx], all_actions[state_idx])

                    ratios = torch.exp(logprobs - all_logprobs[state_idx].detach())
                    surr1 = ratios * advantages[state_idx]
                    surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages[state_idx]

                    loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values.squeeze(), all_rewards[state_idx]) - 0.05 * dist_entropy

                    self.follower_optimizers[follower_idx].zero_grad()
                    loss.mean().backward()
                    torch.nn.utils.clip_grad_norm_(self.follower_policies[follower_idx].parameters(), max_norm=0.5)  # æ¢¯åº¦è£å‰ª
                    self.follower_optimizers[follower_idx].step()

        # æ›´æ–°æ—§ç½‘ç»œ
        if len(self.buffers[self.leader_idx].rewards) > 0:
            self.leader_policy_old.load_state_dict(self.leader_policy.state_dict())
        for i in range(len(self.follower_policies)):
            actual_idx = self.follower_indices[i]
            if len(self.buffers[actual_idx].rewards) > 0:
                self.follower_policies_old[i].load_state_dict(self.follower_policies[i].state_dict())

        # æ¸…ç©ºç¼“å†²åŒº
        for buffer in self.buffers:
            buffer.clear()

    def decay_action_std(self, action_std_decay_rate, min_action_std):
        """åŠ¨æ€è¡°å‡åŠ¨ä½œæ ‡å‡†å·®ä»¥æé«˜é‡‡æ ·æ•ˆç‡"""
        if self.has_continuous_action_space:
            self.action_std = max(min_action_std, self.action_std * action_std_decay_rate)
            self.leader_policy.set_action_std(self.action_std)
            for policy in self.follower_policies:
                policy.set_action_std(self.action_std)
            print(f"åŠ¨ä½œæ ‡å‡†å·®æ›´æ–°ä¸º: {self.action_std}")
    
    def set_action_std(self, new_std):
        """è®¾ç½®æ–°çš„åŠ¨ä½œæ ‡å‡†å·®"""
        if self.has_continuous_action_space:
            self.action_std = new_std
            self.leader_policy.set_action_std(self.action_std)
            for policy in self.follower_policies:
                policy.set_action_std(self.action_std)
    
    def get_replay_buffer_stats(self):
        """è·å–ç»éªŒå›æ”¾ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯"""
        return self.replay_buffer.get_stats()

    def save(self, checkpoint_path):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'leader_policy': self.leader_policy_old.state_dict(),
            'follower_policies': [policy.state_dict() for policy in self.follower_policies_old],
        }
        # åªæœ‰åœ¨å…¨å±€è¯„è®ºå®¶å­˜åœ¨æ—¶æ‰ä¿å­˜
        if self.global_critic is not None:
            checkpoint['global_critic'] = self.global_critic.state_dict()
        
        torch.save(checkpoint, checkpoint_path)

    def validate_algorithm(self):
        """éªŒè¯PPOç®—æ³•å®ç°çš„æ­£ç¡®æ€§"""
        print("=== PPOç®—æ³•éªŒè¯ ===")

        # éªŒè¯ç½‘ç»œç»“æ„
        print("âœ“ é¢†èˆªè€…ç½‘ç»œå‚æ•°:", sum(p.numel() for p in self.leader_policy.parameters()))
        print("âœ“ è·Ÿéšè€…ç½‘ç»œæ•°é‡:", len(self.follower_policies))
        if self.global_critic is not None:
            print("âœ“ å…¨å±€è¯„è®ºå®¶å‚æ•°:", sum(p.numel() for p in self.global_critic.parameters()))

        # éªŒè¯åŠ¨ä½œé€‰æ‹©
        test_state = np.random.randn(156)  # å‡è®¾çŠ¶æ€ç»´åº¦ä¸º156ï¼ˆ26 + 130ï¼‰
        test_depth = np.random.randn(130)  # æ·±åº¦ç‰¹å¾

        try:
            actions = self.select_action([test_state], test_depth, leader_only=True)
            print(f"âœ“ åŠ¨ä½œé€‰æ‹©æˆåŠŸ: {len(actions)} ä¸ªåŠ¨ä½œ")
        except Exception as e:
            print(f"âœ— åŠ¨ä½œé€‰æ‹©å¤±è´¥: {e}")
            return False

        # éªŒè¯ç¼“å†²åŒº
        for i, buffer in enumerate(self.buffers):
            if len(buffer.states) > 0:
                print(f"âœ“ æ™ºèƒ½ä½“{i}ç¼“å†²åŒºæœ‰æ•°æ®: {len(buffer.states)} æ¡")

        # éªŒè¯ä¼˜åŒ–å™¨
        print("âœ“ é¢†èˆªè€…ä¼˜åŒ–å™¨:", type(self.leader_optimizer).__name__)
        print("âœ“ è·Ÿéšè€…ä¼˜åŒ–å™¨æ•°é‡:", len(self.follower_optimizers))

        print("âœ“ PPOç®—æ³•éªŒè¯å®Œæˆ")
        return True
