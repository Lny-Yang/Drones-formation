"""
ç¬¬ä¸€é˜¶æ®µè®­ç»ƒè„šæœ¬ - é¢†èˆªè€…å•æœºå¯¼èˆªè®­ç»ƒ

è®­ç»ƒç›®æ ‡:
- è®­ç»ƒé¢†èˆªè€…æ— äººæœºè¿›è¡Œå•æœºå¯¼èˆª
- å­¦ä¹ é¿éšœå’Œç›®æ ‡åˆ°è¾¾èƒ½åŠ›
- ä½¿ç”¨PPOç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
"""

import sys
import os
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import datetime
from collections import deque
import time
import pybullet as p

# å¯¼å…¥stable-baselines3
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
import torch
import random
from drone_envs.envs.drone_env_multi import DroneNavigationMulti

# å¯¼å…¥è¿›åº¦æ¡
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    print("âš ï¸  tqdm æœªå®‰è£…ï¼Œæ— æ³•æ˜¾ç¤ºè¿›åº¦æ¡: pip install tqdm")
    TQDM_AVAILABLE = False

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)


class PathConfig:
    """è·¯å¾„é…ç½®ç±» - é›†ä¸­ç®¡ç†æ‰€æœ‰ä¿å­˜è·¯å¾„ (SB3ä¸“ç”¨)"""
    
    # åŸºç¡€ç›®å½• - æ”¾åœ¨agentç›®å½•ä¸‹
    BASE_DIR = Path(__file__).parent
    AGENT_DIR = BASE_DIR / "agent"
    LOG_SB3_DIR = AGENT_DIR / "log_SB3"
    MODEL_SB3_DIR = AGENT_DIR / "model_SB3"
    
    # ç®€åŒ–å¼•ç”¨
    LOG_DIR = LOG_SB3_DIR
    MODEL_DIR = MODEL_SB3_DIR
    
    # è®­ç»ƒè¿›åº¦ç›¸å…³è·¯å¾„
    TRAINING_PROGRESS_PLOT = LOG_DIR / "training_progress.png"
    TRAINING_DATA_JSON = LOG_DIR / "training_data.json"
    TRAJECTORIES_JSON = LOG_DIR / "trajectories.json"
    
    # æœ€ç»ˆç»“æœè·¯å¾„
    FINAL_MODEL = MODEL_DIR / "leader_phase1_final"
    FINAL_PROGRESS_PLOT = LOG_DIR / "leader_phase1_final_progress.png"
    FINAL_DATA_JSON = LOG_DIR / "leader_phase1_final_data.json"
    FINAL_TRAJECTORIES_JSON = LOG_DIR / "leader_phase1_final_trajectories.json"
    
    @classmethod
    def ensure_directories(cls):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        cls.AGENT_DIR.mkdir(parents=True, exist_ok=True)
        cls.LOG_SB3_DIR.mkdir(parents=True, exist_ok=True)
        cls.MODEL_SB3_DIR.mkdir(parents=True, exist_ok=True)
    
    @classmethod
    def get_episode_model_path(cls, episode_num):
        """è·å–æŒ‡å®šå›åˆçš„æ¨¡å‹ä¿å­˜è·¯å¾„"""
        return cls.MODEL_DIR / f"leader_phase1_episode_{episode_num}"
    
    @classmethod
    def get_timestamped_path(cls, base_name, extension="json"):
        """è·å–å¸¦æ—¶é—´æˆ³çš„è·¯å¾„"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return cls.LOG_DIR / f"{base_name}_{timestamp}.{extension}"

class RewardTracker:
    """å¥–åŠ±è·Ÿè¸ªå’Œå¯è§†åŒ–ç±»"""
    def __init__(self, window_size=100):
        self.episode_rewards = []
        self.episode_lengths = []
        self.moving_avg_rewards = []
        self.success_rate = []
        self.collision_rate = []
        self.moving_avg_collision = []  # æ–°å¢æ»‘åŠ¨å¹³å‡ç¢°æ’ç‡
        self.window_size = window_size
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # è®¾ç½®å›¾è¡¨å†…å­˜è­¦å‘Šé˜ˆå€¼ï¼Œé¿å…è­¦å‘Šå¹²æ‰°
        plt.rcParams['figure.max_open_warning'] = 50
        
    def add_episode(self, episode_reward, episode_length, success, collision):
        """æ·»åŠ æ–°å›åˆæ•°æ®"""
        self.episode_rewards.append(episode_reward)
        self.episode_lengths.append(episode_length)
        # è®¡ç®—å¥–åŠ±æ»‘åŠ¨å¹³å‡
        if len(self.episode_rewards) >= self.window_size:
            moving_avg = np.mean(self.episode_rewards[-self.window_size:])
        else:
            moving_avg = np.mean(self.episode_rewards)
        self.moving_avg_rewards.append(moving_avg)
        # å­˜å‚¨æˆåŠŸæ ‡å¿—
        if not hasattr(self, 'success_flags'):
            self.success_flags = []
        self.success_flags.append(success)
        # è®¡ç®—æˆåŠŸç‡ - ä½¿ç”¨å­˜å‚¨çš„successæ ‡å¿—
        recent_episodes = min(len(self.success_flags), self.window_size)
        recent_successes = sum(self.success_flags[-recent_episodes:])
        self.success_rate.append(recent_successes / recent_episodes)
        # ç¢°æ’ç‡ï¼ˆä»…ç”¨äºæ»‘åŠ¨å¹³å‡ï¼‰
        self.collision_rate.append(1.0 if collision else 0.0)
        # è®¡ç®—æ»‘åŠ¨å¹³å‡ç¢°æ’ç‡
        if len(self.collision_rate) >= self.window_size:
            avg_collision = np.mean(self.collision_rate[-self.window_size:])
        else:
            avg_collision = np.mean(self.collision_rate)
        self.moving_avg_collision.append(avg_collision)
        
    def plot_training_progress(self, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾"""
        if save_path is None:
            save_path = PathConfig.TRAINING_PROGRESS_PLOT
        # å…³é—­ä¹‹å‰çš„å›¾è¡¨ï¼Œé¿å…å†…å­˜æ³„æ¼
        plt.close('all')
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        episodes = range(1, len(self.episode_rewards) + 1)
        # 1. å¥–åŠ±æ›²çº¿
        ax1.plot(episodes, self.episode_rewards, alpha=0.3, color='blue', label='åŸå§‹å¥–åŠ±')
        ax1.plot(episodes, self.moving_avg_rewards, color='red', linewidth=2, label=f'{self.window_size}å›åˆç§»åŠ¨å¹³å‡')
        ax1.set_xlabel('å›åˆæ•°')
        ax1.set_ylabel('å¥–åŠ±')
        ax1.set_title('ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå¥–åŠ±æ›²çº¿')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        # 2. å›åˆé•¿åº¦
        ax2.plot(episodes, self.episode_lengths, color='green', alpha=0.7)
        ax2.set_xlabel('å›åˆæ•°')
        ax2.set_ylabel('å›åˆé•¿åº¦ï¼ˆæ­¥æ•°ï¼‰')
        ax2.set_title('å›åˆé•¿åº¦å˜åŒ–')
        ax2.grid(True, alpha=0.3)
        # 3. ç¢°æ’ç‡ï¼ˆä»…æ»‘åŠ¨å¹³å‡ï¼‰
        ax3.plot(episodes, self.moving_avg_collision, color='orange', linewidth=2, label=f'{self.window_size}å›åˆæ»‘åŠ¨å¹³å‡')
        ax3.set_xlabel('å›åˆæ•°')
        ax3.set_ylabel('ç¢°æ’ç‡')
        ax3.set_title(f'ç¢°æ’ç‡å˜åŒ–ï¼ˆ{self.window_size}å›åˆæ»‘åŠ¨å¹³å‡ï¼‰')
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1)
        ax3.legend()
        # 4. æˆåŠŸç‡
        ax4.plot(episodes, self.success_rate, color='purple', alpha=0.7)
        ax4.set_xlabel('å›åˆæ•°')
        ax4.set_ylabel('æˆåŠŸç‡')
        ax4.set_title(f'å¯¼èˆªæˆåŠŸç‡ ({self.window_size}å›åˆæ»‘åŠ¨çª—å£)')
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… è®­ç»ƒè¿›åº¦å›¾å·²ä¿å­˜: {save_path}")
        return fig
    
    def save_data(self, save_path=None):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        if save_path is None:
            save_path = PathConfig.TRAINING_DATA_JSON
        # å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ä»¥æ”¯æŒJSONåºåˆ—åŒ–
        data = {
            'episode_rewards': [float(x) for x in self.episode_rewards],
            'episode_lengths': [int(x) for x in self.episode_lengths],
            'moving_avg_rewards': [float(x) for x in self.moving_avg_rewards],
            'success_rate': [float(x) for x in self.success_rate],
            'success_flags': [bool(x) for x in getattr(self, 'success_flags', [])],
            'collision_rate': [float(x) for x in self.collision_rate],
            'moving_avg_collision': [float(x) for x in self.moving_avg_collision],  # ğŸ”¥ æ·»åŠ è¿™ä¸ªå­—æ®µ
            'total_episodes': len(self.episode_rewards),
            'final_avg_reward': float(self.moving_avg_rewards[-1] if self.moving_avg_rewards else 0),
            'final_success_rate': float(self.success_rate[-1] if self.success_rate else 0),
            'algorithm': 'Stable-Baselines3 PPO',
            'timestamp': str(datetime.now())
        }
        
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: {save_path}")


class TrainingCallback(BaseCallback):
            
    """è‡ªå®šä¹‰å›è°ƒå‡½æ•° - ç”¨äºè·Ÿè¸ªè®­ç»ƒè¿›åº¦å’Œå®šæœŸä¿å­˜"""
    
    def __init__(self, reward_tracker, max_episodes, plot_interval=500, save_interval=500, verbose=1, initial_episode=0):
        super(TrainingCallback, self).__init__(verbose)
        self.reward_tracker = reward_tracker
        self.max_episodes = max_episodes  # ğŸ”¥ æ·»åŠ æœ€å¤§å›åˆæ•°é™åˆ¶
        self.plot_interval = plot_interval
        self.save_interval = save_interval
        self.episode_count = initial_episode  # ğŸ”¥ æ”¯æŒä»æŒ‡å®šå›åˆæ•°å¼€å§‹
        self.episode_reward = 0
        self.episode_length = 0
        self.start_time = time.time()
        
        # åˆå§‹åŒ–è¿›åº¦æ¡
        if TQDM_AVAILABLE:
            self.pbar = tqdm(total=max_episodes, initial=initial_episode, desc="è®­ç»ƒè¿›åº¦", unit="å›åˆ", 
                            bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}')
        else:
            self.pbar = None
        
        
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        try:
            # ğŸ”¥ æ£€æµ‹NaNå€¼ï¼ˆå…¼å®¹numpyå’Œtensorç±»å‹ï¼‰
            reward = self.locals['rewards'][0]
            if isinstance(reward, np.ndarray):
                reward = reward.item()
            if np.isnan(reward) or np.isinf(reward):
                print(f"âš ï¸  è­¦å‘Š: åœ¨æ­¥éª¤ {self.num_timesteps} æ£€æµ‹åˆ°å¼‚å¸¸å¥–åŠ±å€¼!")
                return False
            
            # ç´¯ç§¯å¥–åŠ±
            self.episode_reward += self.locals['rewards'][0]
            self.episode_length += 1
            
            # æ£€æŸ¥æ˜¯å¦å›åˆç»“æŸ
            if self.locals['dones'][0]:
                self.episode_count += 1
                # ä½¿ç”¨max_episodesä½œä¸ºæ€»å›åˆæ•°ï¼Œæå‰å®šä¹‰ï¼Œé¿å…UnboundLocalError
                total_episodes = self.max_episodes
                # è·å–infoä¿¡æ¯
                info = self.locals['infos'][0]
                success = info.get('success', False)
                # ä»reward_infoä¸­åˆ¤æ–­æ˜¯å¦ç¢°æ’
                reward_info = info.get('reward_info', {})
                crash_reward = reward_info.get('crash', 0)
                collision = crash_reward < 0  # å¦‚æœæœ‰ç¢°æ’æƒ©ç½šï¼Œè¯´æ˜å‘ç”Ÿäº†ç¢°æ’
                # è®°å½•åˆ°tracker
                self.reward_tracker.add_episode(
                    self.episode_reward,
                    self.episode_length,
                    success,
                    collision
                )
                # ç›´æ¥ä½¿ç”¨RewardTrackerå·²è®¡ç®—å¥½çš„æ»‘åŠ¨å¹³å‡/çª—å£ç»Ÿè®¡
                current_avg_reward = self.reward_tracker.moving_avg_rewards[-1] if self.reward_tracker.moving_avg_rewards else self.episode_reward
                current_success_rate = self.reward_tracker.success_rate[-1] if self.reward_tracker.success_rate else 0
                current_collision_rate = self.reward_tracker.moving_avg_collision[-1] if self.reward_tracker.moving_avg_collision else (1.0 if collision else 0.0)
                # è®¡ç®—ETA
                elapsed_time = time.time() - self.start_time

                # å†™å…¥TensorBoardæ—¥å¿—ï¼ˆå¥–åŠ±å’ŒæˆåŠŸç‡ï¼‰
                try:
                    if hasattr(self.model, 'logger'):
                        self.model.logger.record('custom/episode_reward', self.episode_reward)
                        self.model.logger.record('custom/success_rate', current_success_rate)
                except Exception as e:
                    print(f"logger.record å¤±è´¥: {e}")
                # å†™å…¥æ»‘åŠ¨å¹³å‡ç¢°æ’ç‡åˆ°TensorBoard
                try:
                    if hasattr(self.model, 'logger') and hasattr(self.reward_tracker, 'moving_avg_collision'):
                        self.model.logger.record('custom/avg_collision_rate', self.reward_tracker.moving_avg_collision[-1])
                except Exception as e:
                    print(f"logger.record collision å¤±è´¥: {e}")

                eta = (elapsed_time / self.episode_count) * (total_episodes - self.episode_count) if self.episode_count > 0 else 0
                # è·å–ç»ˆæ­¢ç±»å‹
                if success:
                    termination_type = "æˆåŠŸ"
                elif collision:
                    # å°è¯•ä»reward_infoè·å–æ›´è¯¦ç»†çš„ç¢°æ’ç±»å‹
                    contact_points = reward_info.get('contact_points', 0)
                    if contact_points > 0:
                        termination_type = f"ç‰©ç†ç¢°æ’({contact_points}ç‚¹)"
                    else:
                        termination_type = "ç¢°æ’"
                else:
                    termination_type = "è¶…æ—¶"
                
                # æ›´æ–°è¿›åº¦æ¡
                if self.pbar:
                    self.pbar.update(1)
                    self.pbar.set_postfix({
                        'å¥–åŠ±': f"{self.episode_reward:7.2f}",
                        'å¹³å‡': f"{current_avg_reward:6.2f}",
                        'æˆåŠŸç‡': f"{current_success_rate:5.1%}",
                        'ç¢°æ’ç‡': f"{current_collision_rate:5.1%}",
                        'ETA': f"{eta/60:5.1f}åˆ†"
                    })
                
                # æ¯100å›åˆæ‰“å°ä¸€æ¬¡è¯¦ç»†è¿›åº¦ä¿¡æ¯ï¼ˆå‡å°‘è¾“å‡ºï¼‰
                if self.episode_count % 100 == 0:
                    print(f"å›åˆ {self.episode_count:4d}/{total_episodes} | "
                        f"å¥–åŠ±: {self.episode_reward:8.2f} | "
                        f"å¹³å‡: {current_avg_reward:6.2f} | "
                        f"æˆåŠŸç‡: {current_success_rate:.1%} | "
                        f"ç¢°æ’ç‡: {current_collision_rate:.1%} | "
                        f"ETA: {eta/60:.1f}åˆ†é’Ÿ")
                
                
                # å®šæœŸç»˜åˆ¶å’Œä¿å­˜
                if self.episode_count % self.plot_interval == 0:
                    self.reward_tracker.plot_training_progress()
                    self.reward_tracker.save_data()
                    
                
                # å®šæœŸä¿å­˜æ¨¡å‹
                if self.episode_count % self.save_interval == 0:
                    # ğŸ”¥ ä¿å­˜å‰æ£€æŸ¥å¹¶è£å‰ªlog_stdï¼ˆé˜²æ­¢ä¿å­˜çˆ†ç‚¸çš„å€¼ï¼‰
                    log_std_val = self.model.policy.log_std.data
                    log_std_mean = log_std_val.mean().item()
                    log_std_max = log_std_val.max().item()
                    
                    # # ğŸ¯ ä¸¥æ ¼çš„é˜ˆå€¼ï¼š[-0.8, 0.0]ï¼ˆstdèŒƒå›´: 0.45~1.0ï¼‰
                    # # ç†ç”±ï¼šä»å¤´è®­ç»ƒæ—¶ï¼Œåº”è¯¥ä¿æŒè¾ƒä½çš„æ¢ç´¢å™ªå£°ï¼Œé¿å…log_stdå¤±æ§
                    # if log_std_max > 0.0 or log_std_mean > -0.4:
                    #     print(f"âš ï¸  æ£€æµ‹åˆ°log_stdå¢é•¿: å‡å€¼={log_std_mean:.4f}, æœ€å¤§={log_std_max:.4f}")
                    #     print(f"    å¯¹åº”std: å‡å€¼={np.exp(log_std_mean):.2f}, æœ€å¤§={np.exp(log_std_max):.2f}")
                    #     print(f"    â†’ è£å‰ªåˆ° [-0.8, 0.0] (stdèŒƒå›´: 0.45~1.0)")
                    #     self.model.policy.log_std.data.clamp_(-0.8, 0.0)
                    #     new_mean = self.model.policy.log_std.data.mean().item()
                    #     print(f"    âœ… è£å‰ªåå‡å€¼={new_mean:.4f} (stdâ‰ˆ{np.exp(new_mean):.2f})")
                    
                    model_path = PathConfig.get_episode_model_path(self.episode_count)
                    self.model.save(model_path)
                    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
                    print("-" * 80)
                
                # é‡ç½®å›åˆç»Ÿè®¡
                self.episode_reward = 0
                self.episode_length = 0
                
                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å›åˆæ•°ï¼Œå¦‚æœæ˜¯åˆ™åœæ­¢è®­ç»ƒ
                if self.episode_count >= self.max_episodes:
                    print("="*80)
                    print(f"âœ… å·²å®Œæˆ {self.max_episodes} å›åˆè®­ç»ƒï¼Œåœæ­¢è®­ç»ƒ")
                    print("="*80)
                    return False  # è¿”å›Falseåœæ­¢è®­ç»ƒ
            
            return True
        except Exception as e:
            print(f"_on_step å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return False

def make_env(max_steps=1000):  # ğŸ”¥ å¢åŠ åˆ°1000æ­¥ï¼Œæä¾›æ›´å¤šæ¢ç´¢æ—¶é—´
    """åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°
    
    Args:
        max_steps: æ¯ä¸ªå›åˆçš„æœ€å¤§æ­¥æ•° (é»˜è®¤1000)
    """

    def _init():
        env = DroneNavigationMulti(
            num_drones=1,
            use_depth_camera=True,
            depth_camera_range=10.0,
            depth_resolution=16,
            enable_formation_force=False,
            training_stage=1,
            max_steps=max_steps
        )
        return env
    return _init


def train_leader_phase1_sb3(max_episodes=20000, total_timesteps=None, plot_interval=500, 
                           continue_training=False, load_model_path=None):
    """ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ - é¢†èˆªè€…å•æœºå¯¼èˆªè®­ç»ƒï¼ˆä½¿ç”¨PPOç®—æ³•ï¼‰
    
    Args:
        max_episodes: æœ€å¤§è®­ç»ƒå›åˆæ•° (é»˜è®¤20000)
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™æ ¹æ®max_episodesä¼°ç®—ï¼‰
        plot_interval: ç»˜å›¾å’Œä¿å­˜é—´éš”
        continue_training: æ˜¯å¦ç»§ç»­è®­ç»ƒï¼ˆåŠ è½½ä¹‹å‰çš„æ¨¡å‹ï¼‰
        load_model_path: è¦åŠ è½½çš„æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneä¸”continue_training=Trueï¼Œåˆ™åŠ è½½æœ€æ–°æ¨¡å‹ï¼‰
    """
    print("="*80)
    if continue_training:
        print("ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ - é¢†èˆªè€…å¯¼èˆªè®­ç»ƒï¼ˆç»§ç»­è®­ç»ƒï¼‰")
    else:
        print("ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ - é¢†èˆªè€…å¯¼èˆªè®­ç»ƒï¼ˆä»å¤´å¼€å§‹ï¼‰")
    print("="*80)
    
    # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
    PathConfig.ensure_directories()
    
    # åˆ›å»ºå•ç¯å¢ƒ
    print("æ­£åœ¨åˆ›å»ºç¯å¢ƒ...")
    env = DummyVecEnv([make_env(max_steps=1000)])  # ğŸ”¥ 1000æ­¥/å›åˆï¼Œæä¾›æ›´å¤šæ¢ç´¢æ—¶é—´
    # è·å–ç¯å¢ƒé…ç½®ä¿¡æ¯
    test_env = env.envs[0]
    print(f"ç¯å¢ƒé…ç½®:")
    print(f"  - æ— äººæœºæ•°é‡: {test_env.num_drones}")
    print(f"  - è§‚æµ‹ç©ºé—´: {test_env.observation_space.shape}")
    print(f"  - åŠ¨ä½œç©ºé—´: {test_env.action_space.shape}")
    print(f"  - æ·±åº¦ç‰¹å¾ç»´åº¦: {test_env.depth_feature_dim}")
    print(f"  - è®­ç»ƒé˜¶æ®µ: {test_env.training_stage}")
    print(f"  - ç¼–é˜ŸåŠ›çŠ¶æ€: {'ç¦ç”¨' if not test_env.enable_formation_force else 'å¯ç”¨'}")
    print(f"  - å¹³é¢æ¨¡å¼: {'å¯ç”¨' if test_env.enforce_planar else 'ç¦ç”¨'}")
    print(f"  - æœ€å¤§æ­¥æ•°: {test_env.max_steps}")
    # è®¾ç½®ç¯å¢ƒçš„æ€»å›åˆæ•°ï¼ˆç”¨äºETAè®¡ç®—ï¼‰
    test_env.max_episodes = max_episodes
    
    # ä¼°ç®—æ€»æ­¥æ•°ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if total_timesteps is None:
        # ğŸ”¥ ä½¿ç”¨æ›´ä¿å®ˆçš„ä¼°è®¡ï¼Œç¡®ä¿ä¸ä¼šå› ä¸ºæ­¥æ•°é™åˆ¶è€Œè¿‡æ—©åœæ­¢
        # å‡è®¾å¹³å‡æ¯å›åˆ600æ­¥ï¼ˆmax_steps=1000ï¼Œè€ƒè™‘åˆ°æ¢ç´¢å’Œå¤±è´¥çš„æƒ…å†µï¼‰
        avg_steps_per_episode = 600
        total_timesteps = max_episodes * avg_steps_per_episode
        print(f"  - ä¼°ç®—æ€»æ­¥æ•°: {total_timesteps:,} ({max_episodes}å›åˆ Ã— {avg_steps_per_episode}æ­¥)")
        print(f"  âš ï¸  æ³¨æ„: å®é™…è®­ç»ƒå°†åœ¨è¾¾åˆ° {max_episodes} å›åˆæ—¶åœæ­¢ï¼ˆç”±å›è°ƒå‡½æ•°æ§åˆ¶ï¼‰")
    
    print("="*80)
    
    # ğŸ”¥ MLPç­–ç•¥å¼ºåˆ¶ä½¿ç”¨CPUï¼ˆæ¯”GPUå¿«3-5å€ï¼ï¼‰
    # å‚è€ƒï¼šhttps://github.com/DLR-RM/stable-baselines3/issues/1245
    device_name = 'cpu'
    print(f"âœ… ä½¿ç”¨CPUè®­ç»ƒ (MLPç­–ç•¥åœ¨CPUä¸Šæ¯”GPUæ›´å¿«3-5å€)")
    if torch.cuda.is_available():
        print(f"   æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)} (ä½†MLPä¸é€‚åˆGPUåŠ é€Ÿ)")
    
    # ğŸ”„ æ£€æŸ¥æ˜¯å¦ç»§ç»­è®­ç»ƒ
    if continue_training:
        # ç¡®å®šè¦åŠ è½½çš„æ¨¡å‹è·¯å¾„
        if load_model_path is None:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
            if PathConfig.FINAL_MODEL.with_suffix('.zip').exists():
                load_model_path = PathConfig.FINAL_MODEL
                print(f"ğŸ”„ æ‰¾åˆ°æœ€ç»ˆæ¨¡å‹ï¼ŒåŠ è½½: {load_model_path}")
            else:
                # æŸ¥æ‰¾æœ€æ–°çš„episodeæ¨¡å‹
                model_files = list(PathConfig.MODEL_DIR.glob("leader_phase1_episode_*.zip"))
                if model_files:
                    # æŒ‰episodeæ•°æ’åºï¼Œå–æœ€å¤§çš„
                    model_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
                    load_model_path = model_files[-1].with_suffix('')
                    print(f"ï¿½ æ‰¾åˆ°æœ€æ–°episodeæ¨¡å‹ï¼ŒåŠ è½½: {load_model_path}")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å·²ä¿å­˜æ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
                    continue_training = False
        else:
            print(f"ğŸ”„ åŠ è½½æŒ‡å®šæ¨¡å‹: {load_model_path}")
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºå®Œæ•´çš„è·¯å¾„
            if isinstance(load_model_path, str):
                load_model_path = PathConfig.get_episode_model_path(int(load_model_path.split('_')[-1]))
        
        if continue_training:
            try:
                print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
                model = PPO.load(load_model_path, env=env, device=device_name)
                print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                
                # å°è¯•åŠ è½½è®­ç»ƒå†å²æ•°æ®
                if PathConfig.TRAINING_DATA_JSON.exists():
                    print("\næ­£åœ¨åŠ è½½è®­ç»ƒå†å²...")
                    with open(PathConfig.TRAINING_DATA_JSON, 'r', encoding='utf-8') as f:
                        history_data = json.load(f)
                    print(f"âœ… å·²åŠ è½½ {history_data['total_episodes']} å›åˆçš„å†å²æ•°æ®")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå†å²æ•°æ®ï¼Œå°†é‡æ–°ç»Ÿè®¡")
                    history_data = None
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
                print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
                continue_training = False
                history_data = None
    
    # ğŸ†• åˆ›å»ºæˆ–å·²åŠ è½½æ¨¡å‹
    if not continue_training:
        print("åˆ›å»ºæ–°çš„PPOæ¨¡å‹...")
        
        model = PPO(
            policy="MlpPolicy",
            env=env,
            learning_rate=3e-4,        # å­¦ä¹ ç‡ï¼ˆæ ‡å‡†å€¼ï¼‰
            n_steps=2048,               # ğŸ”¥ é™ä½åˆ°512ï¼ˆä»2048ï¼‰ï¼Œæ›´é¢‘ç¹æ›´æ–°ï¼ŒåŠ å¿«è¿­ä»£é€Ÿåº¦
            batch_size=64,             # Mini-batchå¤§å°
            n_epochs=10,                # ğŸ”¥ å‡å°‘åˆ°4ï¼ˆä»10ï¼‰ï¼ŒåŠ å¿«å•æ¬¡æ›´æ–°é€Ÿåº¦
            gamma=0.99,                # æŠ˜æ‰£å› å­
            gae_lambda=0.95,           # GAEå‚æ•°
            clip_range=0.2,            # PPOè£å‰ªèŒƒå›´
            clip_range_vf=0.2,         # Value functionè£å‰ªï¼Œç¨³å®šä»·å€¼ä¼°è®¡
            ent_coef=0.01,             # ğŸ”¥ é™ä½ç†µç³»æ•°ï¼ˆ0.03â†’0.01ï¼‰ï¼Œå‡å°‘æ¢ç´¢æ¨åŠ¨åŠ›
            vf_coef=0.5,               # Value functionæŸå¤±ç³»æ•°
            max_grad_norm=0.5,         # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            use_sde=False,             # ä¸ä½¿ç”¨çŠ¶æ€ä¾èµ–æ¢ç´¢
            sde_sample_freq=-1,
            target_kl=None,            # ä¸ä½¿ç”¨KLæ•£åº¦early stoppingï¼Œä¾èµ–clipæœºåˆ¶
            tensorboard_log=str(PathConfig.LOG_DIR / "tensorboard"),     
            policy_kwargs=dict(
                # ğŸ¯ ç½‘ç»œç»“æ„è®¾è®¡ï¼ˆé’ˆå¯¹140ç»´è§‚æµ‹ç©ºé—´ â†’ 2ç»´åŠ¨ä½œï¼‰:
                # è¾“å…¥140ç»´ â†’ Actor[256â†’128] â†’ åŠ¨ä½œ2ç»´
                # è¾“å…¥140ç»´ â†’ Critic[256â†’128] â†’ ä»·å€¼1ç»´
                net_arch=[dict(pi=[256, 128], vf=[256, 128])],
                activation_fn=torch.nn.Tanh,
                ortho_init=True,
                log_std_init=-0.8,  # ğŸ”¥ åˆå§‹åŒ–log_stdä¸º-0.8 (stdâ‰ˆ0.45)ï¼Œæ›´ä¿å®ˆçš„æ¢ç´¢
            ),
            verbose=1,
            seed=SEED,                 # ğŸ”¥ è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
            device=device_name,
        )
    
    print("æ¨¡å‹é…ç½®:")
    # ğŸ”¥ å¤„ç†scheduleå‡½æ•°æ˜¾ç¤º
    lr_val = model.learning_rate(1.0) if callable(model.learning_rate) else model.learning_rate
    clip_val = model.clip_range(1.0) if callable(model.clip_range) else model.clip_range
    clip_vf_val = model.clip_range_vf(1.0) if callable(model.clip_range_vf) else model.clip_range_vf
    
    print(f"  - å­¦ä¹ ç‡: {lr_val}")
    print(f"  - Batchå¤§å°: {model.batch_size}")
    print(f"  - è®­ç»ƒè½®æ•°: {model.n_epochs}")
    print(f"  - N_steps: {model.n_steps}")
    print(f"  - Gamma: {model.gamma}")
    print(f"  - GAE Lambda: {model.gae_lambda}")
    print(f"  - ClipèŒƒå›´: {clip_val} (PPOæ ¸å¿ƒæœºåˆ¶)")
    print(f"  - ClipèŒƒå›´(VF): {clip_vf_val} (ç¨³å®šä»·å€¼ä¼°è®¡)")
    print(f"  - ç†µç³»æ•°: {model.ent_coef}")
    print(f"  - VFç³»æ•°: {model.vf_coef}")
    print(f"  - æ¢¯åº¦è£å‰ª: {model.max_grad_norm} (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)")
    print(f"  - Target KL: {model.target_kl} (æ ‡å‡†PPO)")
    
    # ğŸ”¥ æ˜¾ç¤ºå½“å‰log_stdå€¼
    current_log_std = model.policy.log_std.data.cpu().numpy()
    print(f"  - Log_std: å‡å€¼={current_log_std.mean():.4f}, "
          f"èŒƒå›´=[{current_log_std.min():.4f}, {current_log_std.max():.4f}], "
          f"å¯¹åº”stdâ‰ˆ{np.exp(current_log_std.mean()):.4f}")
    print(f"  - è®¾å¤‡: {model.device}")
    print("="*80)
    
    # åˆ›å»ºå¥–åŠ±è·Ÿè¸ªå™¨
    reward_tracker = RewardTracker(window_size=500)  # ğŸ”¥ ä½¿ç”¨500å›åˆçª—å£ï¼Œå‡å°‘ç»Ÿè®¡å™ªå£°
    
    # ğŸ”„ å¦‚æœç»§ç»­è®­ç»ƒï¼Œæ¢å¤å†å²æ•°æ®
    # ğŸ”¥ å…ˆç¡®å®šè¦æ¢å¤åˆ°å“ªä¸ªepisodeï¼ˆä¼˜å…ˆä½¿ç”¨æ¨¡å‹æ–‡ä»¶åçš„episodeæ•°ï¼‰
    target_episode = 0
    if continue_training and 'load_model_path' in locals() and load_model_path:
        if isinstance(load_model_path, (str, Path)):
            model_name = Path(load_model_path).name
            if 'episode_' in model_name:
                try:
                    target_episode = int(model_name.split('episode_')[-1])
                    print(f"ğŸ¯ ç›®æ ‡æ¢å¤åˆ°episode: {target_episode}")
                except:
                    pass
    
    if continue_training and 'history_data' in locals() and history_data:
        print("æ­£åœ¨æ¢å¤è®­ç»ƒå†å²...")
        
        # ğŸ”¥ åŠ è½½æ‰€æœ‰å†å²æ•°æ®
        all_rewards = history_data.get('episode_rewards', [])
        all_lengths = history_data.get('episode_lengths', [])
        all_moving_avg = history_data.get('moving_avg_rewards', [])
        all_success_rate = history_data.get('success_rate', [])
        all_success_flags = history_data.get('success_flags', [])
        all_collision_rate = history_data.get('collision_rate', [])
        all_moving_avg_collision = history_data.get('moving_avg_collision', [])
        
        # ğŸ”¥ å¦‚æœç›®æ ‡episode < å†å²æ•°æ®æ€»æ•°ï¼Œæˆªæ–­åˆ°ç›®æ ‡episode
        if target_episode > 0 and target_episode < len(all_rewards):
            print(f"  âš ï¸  å†å²æ•°æ®æœ‰ {len(all_rewards)} å›åˆï¼Œä½†æ¨¡å‹æ˜¯episode_{target_episode}")
            print(f"  ğŸ”ª æˆªæ–­å†å²æ•°æ®åˆ°å‰ {target_episode} å›åˆï¼ˆä¸¢å¼ƒåç»­æ•°æ®ï¼‰")
            
            reward_tracker.episode_rewards = all_rewards[:target_episode]
            reward_tracker.episode_lengths = all_lengths[:target_episode]
            reward_tracker.moving_avg_rewards = all_moving_avg[:target_episode]
            reward_tracker.success_rate = all_success_rate[:target_episode]
            reward_tracker.success_flags = all_success_flags[:target_episode]
            reward_tracker.collision_rate = all_collision_rate[:target_episode]
            
            # å¯¹äºmoving_avg_collisionï¼Œå¦‚æœé•¿åº¦ä¸åŒ¹é…åˆ™é‡æ–°è®¡ç®—
            if len(all_moving_avg_collision) >= target_episode:
                reward_tracker.moving_avg_collision = all_moving_avg_collision[:target_episode]
            else:
                print("  âš ï¸  moving_avg_collisioné•¿åº¦ä¸è¶³ï¼Œé‡æ–°è®¡ç®—...")
                reward_tracker.moving_avg_collision = []
                for i in range(target_episode):
                    if i >= reward_tracker.window_size:
                        avg_collision = np.mean(reward_tracker.collision_rate[i-reward_tracker.window_size+1:i+1])
                    else:
                        avg_collision = np.mean(reward_tracker.collision_rate[:i+1])
                    reward_tracker.moving_avg_collision.append(avg_collision)
        else:
            # æ­£å¸¸æ¢å¤æ‰€æœ‰æ•°æ®
            reward_tracker.episode_rewards = all_rewards
            reward_tracker.episode_lengths = all_lengths
            reward_tracker.moving_avg_rewards = all_moving_avg
            reward_tracker.success_rate = all_success_rate
            reward_tracker.success_flags = all_success_flags
            reward_tracker.collision_rate = all_collision_rate
            
            # é‡æ–°è®¡ç®—moving_avg_collisionï¼ˆå¦‚æœå†å²æ•°æ®ä¸­æ²¡æœ‰ï¼‰
            if all_moving_avg_collision:
                reward_tracker.moving_avg_collision = all_moving_avg_collision
            else:
                print("  âš ï¸  å†å²æ•°æ®ç¼ºå°‘moving_avg_collisionï¼Œé‡æ–°è®¡ç®—...")
                reward_tracker.moving_avg_collision = []
                for i in range(len(reward_tracker.collision_rate)):
                    if i >= reward_tracker.window_size:
                        avg_collision = np.mean(reward_tracker.collision_rate[i-reward_tracker.window_size+1:i+1])
                    else:
                        avg_collision = np.mean(reward_tracker.collision_rate[:i+1])
                    reward_tracker.moving_avg_collision.append(avg_collision)
        
        print(f"âœ… å·²æ¢å¤ {len(reward_tracker.episode_rewards)} å›åˆçš„è®­ç»ƒå†å²")
        if reward_tracker.success_rate:
            print(f"   æ¢å¤ç‚¹æˆåŠŸç‡: {reward_tracker.success_rate[-1]:.1%}")
        if reward_tracker.moving_avg_rewards:
            print(f"   æ¢å¤ç‚¹å¹³å‡å¥–åŠ±: {reward_tracker.moving_avg_rewards[-1]:.2f}")
        if reward_tracker.moving_avg_collision:
            print(f"   æ¢å¤ç‚¹ç¢°æ’ç‡: {reward_tracker.moving_avg_collision[-1]:.1%}")
    
    # åˆ›å»ºå›è°ƒå‡½æ•°
    initial_episode = 0
    if continue_training and 'history_data' in locals() and history_data:
        # ğŸ”¥ ä»æ¨¡å‹æ–‡ä»¶åæå–episodeæ•°ï¼Œè€Œä¸æ˜¯ä»å†å²æ•°æ®
        if isinstance(load_model_path, (str, Path)):
            model_name = Path(load_model_path).name
            if 'episode_' in model_name:
                try:
                    # æå–episodeæ•°å­—ï¼ˆä¾‹å¦‚ï¼šleader_phase1_episode_99000 â†’ 99000ï¼‰
                    episode_num = int(model_name.split('episode_')[-1])
                    initial_episode = episode_num
                    print(f"ğŸ”„ ä»æ¨¡å‹æ–‡ä»¶episodeæ•°ç»§ç»­è®­ç»ƒ: {initial_episode}")
                except:
                    # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨å†å²æ•°æ®
                    initial_episode = history_data.get('total_episodes', 0)
                    print(f"ğŸ”„ ä»å†å²æ•°æ®ç»§ç»­è®­ç»ƒ: {initial_episode}")
            else:
                initial_episode = history_data.get('total_episodes', 0)
                print(f"ğŸ”„ ä»å†å²æ•°æ®ç»§ç»­è®­ç»ƒ: {initial_episode}")
        else:
            initial_episode = history_data.get('total_episodes', 0)
            print(f"ğŸ”„ ä»å†å²æ•°æ®ç»§ç»­è®­ç»ƒ: {initial_episode}")
    
    callback = TrainingCallback(
        reward_tracker=reward_tracker,
        max_episodes=max_episodes,  # ğŸ”¥ ä¼ å…¥æœ€å¤§å›åˆæ•°
        plot_interval=plot_interval,
        save_interval=plot_interval,
        initial_episode=initial_episode  # ğŸ”¥ ä¼ å…¥åˆå§‹å›åˆæ•°
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            log_interval=5000,  # æ¯5000ä¸ªepisodeæ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼Œå‡å°‘print
            progress_bar=False  # æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ˜¾ç¤º
        )
    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼")
    
    # è®­ç»ƒå®Œæˆ
    training_time = time.time() - start_time
    print("="*80)
    print("è®­ç»ƒå®Œæˆï¼")
    
    # æœ€ç»ˆç»Ÿè®¡
    final_avg_reward = np.mean(reward_tracker.episode_rewards[-50:]) if len(reward_tracker.episode_rewards) >= 50 else np.mean(reward_tracker.episode_rewards)
    final_success_rate = reward_tracker.success_rate[-1] if reward_tracker.success_rate else 0
    
    print(f"æœ€ç»ˆç»Ÿè®¡:")
    print(f"  - æ€»å›åˆæ•°: {len(reward_tracker.episode_rewards)}")
    print(f"  - æ€»æ­¥æ•°: {callback.num_timesteps}")
    print(f"  - æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_reward:.2f}")
    print(f"  - æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.2%}")
    print(f"  - è®­ç»ƒæ—¶é•¿: {training_time/60:.1f}åˆ†é’Ÿ")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œæ•°æ®
    final_model_path = PathConfig.FINAL_MODEL
    model.save(final_model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    reward_tracker.plot_training_progress(PathConfig.FINAL_PROGRESS_PLOT)
    reward_tracker.save_data(PathConfig.FINAL_DATA_JSON)
    
    
    print("="*80)
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜:")
    print(f"  ğŸ“Š æ—¥å¿—å’Œå›¾è¡¨: agent/log_SB3/")
    print(f"  ğŸ“ æ¨¡å‹æ–‡ä»¶: agent/model_SB3/")
    print("="*80)
    
    # å…³é—­è¿›åº¦æ¡
    if hasattr(callback, 'pbar') and callback.pbar:
        callback.pbar.close()
    
    env.close()
    return final_model_path, reward_tracker, model


if __name__ == '__main__':
    # ğŸ”¥ ä»å¤´å¼€å§‹è®­ç»ƒ - ä¼˜åŒ–é…ç½®
    # å…³é”®æ”¹è¿›ï¼š
    # 1. ent_coef: 0.03 â†’ 0.01 (é™ä½ç†µæ¨åŠ¨ï¼Œå‡ç¼“log_stdå¢é•¿)
    # 2. log_std_init: -1.0 â†’ -0.8 (æ›´ä¿å®ˆçš„åˆå§‹æ¢ç´¢ï¼Œstdâ‰ˆ0.45)
    # 3. n_steps: 1024 â†’ 2048 (æ”¶é›†æ›´å¤šç»éªŒå†æ›´æ–°ï¼Œæé«˜ç¨³å®šæ€§)
    # 4. log_stdè£å‰ª: [-0.5,0.3] â†’ [-0.8,0.0] (æ›´ä¸¥æ ¼çš„æ§åˆ¶)
    # 5. seed=SEED (ä¿è¯å¯å¤ç°)
    
    model_path, reward_tracker, model = train_leader_phase1_sb3(
        max_episodes=100000,       # è®­ç»ƒå›åˆæ•°
        total_timesteps=None,      # è‡ªåŠ¨æ ¹æ®å›åˆæ•°ä¼°ç®—
        plot_interval=1000,        # æ¯1000å›åˆä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        continue_training=True,    # ğŸ”¥ ä»å¤´å¼€å§‹è®­ç»ƒ
        load_model_path='leader_phase1_episode_14000'      # ä»14000å›åˆç»§ç»­è®­ç»ƒ
    )
    
    print(f"\nè®­ç»ƒå®Œæˆï¼æ£€æŸ¥ {PathConfig.LOG_DIR} ç›®å½•æŸ¥çœ‹ç»“æœå›¾è¡¨å’Œæ•°æ®ã€‚")

